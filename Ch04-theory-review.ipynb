{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For many situations, the response variable is *qualitative*. Often, qualitative variables are referred to as *categorical*. The process for predicting a qualitative response is called **classification**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\underline{\\text{Overview of Classification}}:$\n",
    "\n",
    "Like regression, in the classification setting we have a set of training observations $(x_1, y_1),\\dots, (x_n, y_n)$ that we can use to build a classifier. In this chapter, we learn how to build a model to predict *default* $Y$ for any given value of *balance* $X_1$ and *income* $X_2$.\n",
    "\n",
    "\n",
    "**Why not linear regression?**\n",
    "\n",
    "1. A regression method cannot accomodate a qualitative response with more than two classes. \n",
    "\n",
    "*Example* If we encode three diagnoses: $\\text{stroke}, \\text{drug overdose}, \\text{epileptic seizure}$ response variable as $Y$\n",
    "\n",
    "$$\n",
    "Y =     \n",
    "\\begin{cases}\n",
    "1 & \\text{if stroke;} \\\\\\\\\n",
    "2 & \\text{if drug overdose;} \\\\\\\\\n",
    "3 & \\text{if epileptic seizure;} \\\\\\\\\n",
    "\\end{cases}\n",
    "\\tag{1}\n",
    "$$\n",
    "\n",
    "we can use least squares to fit a linear model. However, this implies a false ordering and equal spacing between categories, which may not reflect reality. Linear regression isn’t appropriate when the response is qualitative with no natural numeric structure.\n",
    "\n",
    "2. A regression method will not provide meaningful estimates of $Pr(Y|X)$, even with just two classes. \n",
    "\n",
    "*Example* For a binary response, e.g.,\n",
    "\n",
    "$$\n",
    "Y =\n",
    "\\begin{cases}\n",
    "0 & \\text{if stroke} \\\\\n",
    "1 & \\text{if drug overdose}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "we can fit a linear regression and predict drug overdose if $\\hat{Y} > 0.5$, stroke otherwise. This gives crude probability estimates, though some may fall outside $[0, 1]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\underline{\\text{Logistic Regression:}}$\n",
    "\n",
    "Logistic regression models the **probability** that $Y$ belongs to a paticular category. For the **Default** data, logistic regression models the probability of default given a **balance**\n",
    "\n",
    "$$Pr(\\text{default=Yes|balance}).$$\n",
    "\n",
    "which can be abbreviated as $p(\\text{balance})$, which will range between 0 and 1. For example, any individual for whom $p(\\text{balance}) > 0.5$ might predict a $\\text{default=Yes}$. A company could have a more conservative approach and use a lower threshold like $p(\\text{balance}) > 0.1$.\n",
    "\n",
    "**The Logistic Model**\n",
    "\n",
    "Using linear regression to model $p(X) = \\Pr(Y = 1 \\mid X)$ with\n",
    "\n",
    "$$\n",
    "p(X) = \\beta_0 + \\beta_1 X\n",
    "$$\n",
    "\n",
    "can lead to invalid probability estimates—values below 0 or above 1—especially when $X$ has a wide range. To fix this, we use a function that maps any input to $[0, 1]$, such as the logistic function:\n",
    "\n",
    "$$\n",
    "p(X) = \\frac{e^{\\beta_0 + \\beta_1 X}}{1 + e^{\\beta_0 + \\beta_1 X}}.\n",
    "$$\n",
    "\n",
    "This is the basis of logistic regression. To fit the model, we use *maximum likelihood*, ensuring predicted probabilities stay within $[0, 1]$. The logistic function\n",
    "\n",
    "$$\n",
    "p(X) = \\frac{e^{\\beta_0 + \\beta_1 X}}{1 + e^{\\beta_0 + \\beta_1 X}}\n",
    "$$\n",
    "\n",
    "yields an S-shaped curve—low $X$ leads to probabilities near 0, high $X$ near 1—unlike linear regression, which can produce invalid probabilities.\n",
    "\n",
    "We can rewrite the model as\n",
    "\n",
    "$$\n",
    "\\frac{p(X)}{1 - p(X)} = e^{\\beta_0 + \\beta_1 X}\n",
    "$$\n",
    "\n",
    "where the left-hand side is the **odds**, and its log gives the **logit**:\n",
    "\n",
    "$$\n",
    "\\log\\left( \\frac{p(X)}{1 - p(X)} \\right) = \\beta_0 + \\beta_1 X\n",
    "$$\n",
    "\n",
    "Here, $\\beta_1$ reflects how the **log-odds** change with $X$, and multiplying the odds by $e^{\\beta_1}$ gives the effect of a one-unit increase in $X$. Unlike linear regression, the effect of $X$ on $p(X)$ depends on it's current value,  making the probability response nonlinear. If $\\beta_1 > 0$, increasing $X$ increases $p(X)$; if $\\beta_1 < 0$, increasing $X$ decreases $p(X)$; it's monotone function.\n",
    "\n",
    "**Estimating the Regression Coefficients**\n",
    "\n",
    "In logistic regression, the coefficients $\\beta_0$ and $\\beta_1$ are unknown and estimated from training data. Instead of least squares (used in linear regression), we use **maximum likelihood**, which has better statistical properties.\n",
    "\n",
    "The idea is to choose $\\hat{\\beta}_0$, $\\hat{\\beta}_1$ so that the predicted probability $\\hat{p}(x_i)$ is close to 1 for individuals who defaulted and close to 0 for those who didn’t. This is done by maximizing the **likelihood function**.\n",
    "\n",
    "Assuming a **Bernoulli model** for each output $y_i \\in \\{0, 1\\}$ given an input $x_i$, with:\n",
    "\n",
    "$$\n",
    "P(y_i = 1 \\mid x_i) = p(x_i), \\quad P(y_i = 0 \\mid x_i) = 1 - p(x_i)\n",
    "$$\n",
    "\n",
    "Given a dataset of $n$ i.i.d. observations $\\{(x_i, y_i)\\}_{i=1}^n$, the **likelihood function** is\n",
    "\n",
    "$$\n",
    "\\ell(\\beta) = \\prod_{i=1}^{n} p(x_i)^{y_i} (1 - p(x_i))^{1 - y_i}\n",
    "$$\n",
    "\n",
    "and if $y_i = 1$, the term becomes $p(x_i). If $y_i = 0$, the term becomes $1 - p(x_i)$. The version of this in ISLP is written as\n",
    "\n",
    "$$\n",
    "\\ell(\\beta_0, \\beta_1) = \\prod_{i: y_i = 1} p(x_i) \\cdot \\prod_{i': y_{i'} = 0} (1 - p(x_{i'}))\n",
    "$$\n",
    "\n",
    "which just partitions the products such that the **first product** is over all indices $i$ such that $y_i = 1$ and the **second product** is over all indices $i'$ such that $y_{i'} = 0$.\n",
    "\n",
    "The assumption that is made is that the observations are independent and the joint probability of all observaed labels given the inputs is the product of the individual conditional probabilities. is maximized such that you observed a sequence of $y_i=0's$ and $y_i=1's$. \n",
    "\n",
    "\n",
    "For example, if $\\hat{\\beta}_1 = 0.0055$, then a one-unit increase in balance raises the **log-odds** of default by 0.0055. The **z-statistic** tests if this effect is statistically significant, similar to the t-statistic in linear regression. A small **p-value** indicates we reject the null $H_0: \\beta_1 = 0$, meaning balance is associated with default risk.\n",
    "\n",
    "The intercept $\\hat{\\beta}_0$ is usually not of direct interest—it adjusts the overall probability to match the proportion of defaults in the data.\n",
    "\n",
    "**Making Predictions**\n",
    "\n",
    "Once the coefficients are estimated, we can compute predicted probabilities using the logistic model:\n",
    "\n",
    "$$\n",
    "\\hat{p}(X) = \\frac{e^{\\hat{\\beta}_0 + \\hat{\\beta}_1 X}}{1 + e^{\\hat{\\beta}_0 + \\hat{\\beta}_1 X}}\n",
    "$$\n",
    "\n",
    "From **Table 4.1**, for `balance` as the predictor:\n",
    "\n",
    "* $\\hat{\\beta}_0 = -10.6513$\n",
    "* $\\hat{\\beta}_1 = 0.0055$\n",
    "\n",
    "Then:\n",
    "\n",
    "* For a balance of \\$1,000:\n",
    "\n",
    "$$\n",
    "\\hat{p}(X = 1000) = \\frac{e^{-10.6513 + 0.0055 \\cdot 1000}}{1 + e^{-10.6513 + 0.0055 \\cdot 1000}} = \\frac{e^{-5.1513}}{1 + e^{-5.1513}} \\approx 0.00576\n",
    "$$\n",
    "\n",
    "* For a balance of \\$2,000:\n",
    "\n",
    "$$\n",
    "\\hat{p}(X = 2000) = \\frac{e^{-10.6513 + 0.0055 \\cdot 2000}}{1 + e^{-10.6513 + 0.0055 \\cdot 2000}} = \\frac{e^{0.3487}}{1 + e^{0.3487}} \\approx 0.586\n",
    "$$\n",
    "\n",
    "**Table 4.1: Logistic Regression Predicting Default from Balance**\n",
    "\n",
    "| Coefficient | Std. Error | z-Statistic | p-Value |         |\n",
    "| ----------- | ---------- | ----------- | ------- | ------- |\n",
    "| Intercept   | -10.6513   | 0.3612      | -29.5   | <0.0001 |\n",
    "| Balance     | 0.0055     | 0.0002      | 24.9    | <0.0001 |\n",
    "\n",
    "Logistic regression also handles qualitative predictors using **dummy variables**. For example, for the qualitative variable `student` (1 = student, 0 = non-student), the estimated coefficients from **Table 4.2** are:\n",
    "\n",
    "* $\\hat{\\beta}_0 = -3.5041$\n",
    "* $\\hat{\\beta}_1 = 0.4049$\n",
    "\n",
    "Then:\n",
    "\n",
    "* For a student:\n",
    "\n",
    "$$\n",
    "\\hat{p}(\\text{default} = \\text{Yes} \\mid \\text{student} = 1) = \\frac{e^{-3.5041 + 0.4049 \\cdot 1}}{1 + e^{-3.5041 + 0.4049 \\cdot 1}} = \\frac{e^{-3.0992}}{1 + e^{-3.0992}} \\approx 0.0431\n",
    "$$\n",
    "\n",
    "* For a non-student:\n",
    "\n",
    "$$\n",
    "\\hat{p}(\\text{default} = \\text{Yes} \\mid \\text{student} = 0) = \\frac{e^{-3.5041 + 0.4049 \\cdot 0}}{1 + e^{-3.5041 + 0.4049 \\cdot 0}} = \\frac{e^{-3.5041}}{1 + e^{-3.5041}} \\approx 0.0292\n",
    "$$\n",
    "\n",
    "The positive coefficient indicates that students are predicted to have a higher probability of default than non-students.\n",
    "\n",
    "**Table 4.2: Logistic Regression Predicting Default from Student Status**\n",
    "\n",
    "| Coefficient    | Std. Error | z-Statistic | p-Value |         |\n",
    "| -------------- | ---------- | ----------- | ------- | ------- |\n",
    "| Intercept      | -3.5041    | 0.0707      | -49.55  | <0.0001 |\n",
    "| Student \\[Yes] | 0.4049     | 0.1150      | 3.52    | 0.0004  |\n",
    "\n",
    "**Multiple Regression**\n",
    "\n",
    "To predict a binary response using multiple predictors, we generalize the logistic regression model as:\n",
    "\n",
    "$$\n",
    "\\log \\left( \\frac{p(X)}{1 - p(X)} \\right) = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p\n",
    "$$\n",
    "\n",
    "or equivalently,\n",
    "\n",
    "$$\n",
    "\\hat{p}(X) = \\frac{e^{\\hat{\\beta}_0 + \\hat{\\beta}_1 X_1 + \\cdots + \\hat{\\beta}_p X_p}}{1 + e^{\\hat{\\beta}_0 + \\hat{\\beta}_1 X_1 + \\cdots + \\hat{\\beta}_p X_p}}\n",
    "$$\n",
    "\n",
    "**Table 4.3: Logistic Regression Predicting Default from Multiple Predictors**\n",
    "\n",
    "| Coefficient    | Std. Error | z-Statistic | p-Value |         |\n",
    "| -------------- | ---------- | ----------- | ------- | ------- |\n",
    "| Intercept      | -10.8690   | 0.4923      | -22.08  | <0.0001 |\n",
    "| Balance        | 0.0057     | 0.0002      | 24.74   | <0.0001 |\n",
    "| Income         | 0.0030     | 0.0082      | 0.37    | 0.7115  |\n",
    "| Student \\[Yes] | -0.6468    | 0.2362      | -2.74   | 0.0062  |\n",
    "\n",
    "As before, we use **maximum likelihood** to estimate the coefficients. From the table:\n",
    "\n",
    "* Balance is positively and significantly associated with default.\n",
    "* Income is not statistically significant.\n",
    "* Student status has a **negative coefficient**, in contrast to the positive coefficient in the earlier single-variable model (Table 4.2).\n",
    "\n",
    "This reversal occurs due to **confounding variable**: student status and balance are correlated—students tend to carry higher balances, which are strongly associated with higher default rates. So although students have higher **overall** default rates, for the **same balance and income**, students are actually **less likely** to default than non-students.\n",
    "\n",
    "**Predictions from the Multiple Logistic Regression Model**\n",
    "\n",
    "Using the estimated coefficients from Table 4.3, we can compute predicted probabilities:\n",
    "\n",
    "* For a **student** with balance = \\$1,500 and income = \\$40,000:\n",
    "\n",
    "$$\n",
    "\\hat{p}(\\text{default} = \\text{Yes} \\mid \\text{student} = 1, \\text{balance} = 1500, \\text{income} = 40) =\n",
    "\\frac{e^{-10.869 + 0.0057 \\cdot 1500 + 0.0030 \\cdot 40 - 0.6468 \\cdot 1}}{1 + e^{-10.869 + 0.0057 \\cdot 1500 + 0.0030 \\cdot 40 - 0.6468 \\cdot 1}} \\approx 0.058\n",
    "$$\n",
    "\n",
    "* For a **non-student** with the same balance and income:\n",
    "\n",
    "$$\n",
    "\\hat{p}(\\text{default} = \\text{Yes} \\mid \\text{student} = 0, \\text{balance} = 1500, \\text{income} = 40) =\n",
    "\\frac{e^{-10.869 + 0.0057 \\cdot 1500 + 0.0030 \\cdot 40 - 0.6468 \\cdot 0}}{1 + e^{-10.869 + 0.0057 \\cdot 1500 + 0.0030 \\cdot 40 - 0.6468 \\cdot 0}} \\approx 0.105\n",
    "$$\n",
    "\n",
    "Note: Income is measured in **thousands of dollars**, so we use 40 instead of 40,000 in the calculation.\n",
    "\n",
    "**Discussion of confounding**\n",
    "\n",
    "The **overall default rate** refers to the empirical proportion of individuals in the dataset who defaulted, regardless of predictor values.\n",
    "\n",
    "Mathematically, if you have $n$ observations and $y_i \\in \\{0, 1\\}$ indicates whether person $i$ defaulted:\n",
    "\n",
    "$$\n",
    "\\text{Overall default rate} = \\frac{1}{n} \\sum_{i=1}^n y_i\n",
    "$$\n",
    "\n",
    "This is **not** a model-based prediction—it's simply the average value of the binary response across the data. For example, if 3.3% of people in the data defaulted, the overall default rate is 0.033.\n",
    "\n",
    "In plots like **Figure 4.3**, the horizontal dashed lines show this empirical rate **within subgroups**, e.g., students vs. non-students, averaged across all values of balance and income.\n",
    "\n",
    "How can students be less likely to default (in the model), yet have a higher overall default rate?\n",
    "\n",
    "This is the classic signature of **confounding**.\n",
    "\n",
    "* In the **single-variable logistic regression** (Table 4.2), students appear **more likely** to default. That’s because:\n",
    "\n",
    "  * Students tend to have **higher balances** (right panel of Figure 4.3).\n",
    "  * Higher balances are strongly associated with higher default risk.\n",
    "  * So, without adjusting for balance, it *looks* like students are riskier.\n",
    "\n",
    "* In the **multiple logistic regression** (Table 4.3), after controlling for **balance** and **income**, the **coefficient for student becomes negative**:\n",
    "\n",
    "  * This means that **for two people with the same balance and income**, the student is actually **less likely** to default.\n",
    "  * So, once we hold balance constant, the true effect of student status is revealed.\n",
    "\n",
    "* The multiple logistic regression says:\n",
    "\n",
    "  $$\n",
    "  \\log \\left( \\frac{\\Pr(\\text{default} = 1 \\mid X)}{1 - \\Pr(\\text{default} = 1 \\mid X)} \\right) = \\beta_0 + \\beta_1 \\cdot \\text{balance} + \\beta_2 \\cdot \\text{income} + \\beta_3 \\cdot \\text{student}\n",
    "  $$\n",
    "\n",
    "  where $\\hat{\\beta}_3 = -0.6468$ implies students are less likely to default **when balance and income are the same**.\n",
    "\n",
    "The higher **overall** student default rate is due to **students having higher average balances**, not because being a student inherently increases default risk.\n",
    "\n",
    "**Student status is a confounding variable**. It’s associated with higher balance, which increases default risk. Once you control for balance, being a student is actually associated with **lower** default probability.\n",
    "\n",
    "\n",
    "**Multinomial Logistic Regression (for $K > 2$ classes)**\n",
    "\n",
    "Logistic regression can be extended to handle **multi-class classification**—this is called **multinomial logistic regression**.\n",
    "\n",
    "Suppose $Y \\in \\{1, 2, ..., K\\}$ is a response with **K classes**, and we choose one class (say class $K$) as the **baseline**.\n",
    "\n",
    "For $k = 1, \\dots, K-1$:\n",
    "\n",
    "$$\n",
    "\\Pr(Y = k \\mid X = x) = \\frac{e^{\\beta_{k0} + \\beta_{k1}x_1 + \\cdots + \\beta_{kp}x_p}}{1 + \\sum_{l=1}^{K-1} e^{\\beta_{l0} + \\beta_{l1}x_1 + \\cdots + \\beta_{lp}x_p}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Pr(Y = K \\mid X = x) = \\frac{1}{1 + \\sum_{l=1}^{K-1} e^{\\beta_{l0} + \\beta_{l1}x_1 + \\cdots + \\beta_{lp}x_p}}\n",
    "$$\n",
    "\n",
    "And the **log-odds** for class $k$ vs. the baseline class $K$ is:\n",
    "\n",
    "$$\n",
    "\\log \\left( \\frac{\\Pr(Y = k \\mid X = x)}{\\Pr(Y = K \\mid X = x)} \\right) = \\beta_{k0} + \\beta_{k1}x_1 + \\cdots + \\beta_{kp}x_p\n",
    "$$\n",
    "\n",
    "This generalizes binary logistic regression: the log-odds between any class and the baseline is linear in the predictors.\n",
    "\n",
    "* The choice of **baseline class** doesn’t affect predicted probabilities or log-odds between any pair of classes, but it **does affect coefficient interpretation**.\n",
    "* For example, if **epileptic seizure** is the baseline, then $\\beta_{\\text{stroke}, j}$ is the change in log-odds of stroke vs. epileptic seizure for a one-unit increase in $x_j$.\n",
    "* The **odds ratio** increases by $e^{\\beta_{\\text{stroke}, j}}$ per unit increase in $x_j$.\n",
    "\n",
    "**Softmax Coding**\n",
    "\n",
    "An alternative formulation used widely in **machine learning** is **softmax coding**, which treats all classes symmetrically:\n",
    "\n",
    "$$\n",
    "\\Pr(Y = k \\mid X = x) = \\frac{e^{\\beta_{k0} + \\beta_{k1}x_1 + \\cdots + \\beta_{kp}x_p}}{\\sum_{l=1}^K e^{\\beta_{l0} + \\beta_{l1}x_1 + \\cdots + \\beta_{lp}x_p}} \\quad \\text{for } k = 1, \\dots, K\n",
    "$$\n",
    "\n",
    "In this version\n",
    "\n",
    "* Coefficients are estimated for **all K classes**.\n",
    "* The **log-odds between any two classes** $k$ and $k'$ is:\n",
    "\n",
    "$$\n",
    "\\log \\left( \\frac{\\Pr(Y = k \\mid X = x)}{\\Pr(Y = k' \\mid X = x)} \\right)\n",
    "= (\\beta_{k0} - \\beta_{k'0}) + (\\beta_{k1} - \\beta_{k'1})x_1 + \\cdots + (\\beta_{kp} - \\beta_{k'p})x_p\n",
    "$$\n",
    "\n",
    "Softmax coding produces the **same predicted probabilities and log-odds** as the baseline coding—it’s just a different parameterization.\n",
    "\n",
    "**Summary Table**\n",
    "\n",
    "| Concept                  | Description                                                                                        |\n",
    "| ------------------------ | -------------------------------------------------------------------------------------------------- |\n",
    "| Modeling approach        | **Discriminative**: directly models $\\Pr(Y = k \\mid X = x)$                                        |\n",
    "| Functional form (binary) | $\\log \\frac{p(x)}{1 - p(x)} = \\beta_0 + \\beta_1 x$                                                 |\n",
    "| Output                   | Probability estimates for each class                                                               |\n",
    "| Probability function     | $p(x) = \\frac{e^{\\beta_0 + \\beta_1 x}}{1 + e^{\\beta_0 + \\beta_1 x}}$                               |\n",
    "| Fitting method           | Maximum likelihood estimation                                                                      |\n",
    "| Link function            | **Logit** (log-odds)                                                                               |\n",
    "| Assumptions              | Linear relationship between predictors and log-odds                                                |\n",
    "| Decision rule            | Predict class 1 if $p(x) > t$, with threshold $t$ (e.g., 0.5)                                      |\n",
    "| Interpretability         | Coefficients represent **change in log-odds** per unit change in predictor                         |\n",
    "| Extensions               | - Multinomial logistic regression (for $K > 2$)                                                    |\n",
    "|                          | - Softmax regression (symmetric multiclass generalization)                                         |\n",
    "| Strengths                | - Probabilistic output                                                                             |\n",
    "|                          | - Handles binary and multiclass tasks                                                              |\n",
    "|                          | - Can include interactions and non-linearities via features                                        |\n",
    "| Limitations              | - Assumes linear log-odds                                                                          |\n",
    "|                          | - May underperform if classes are non-separable or heavily overlapping                             |\n",
    "| Relation to LDA          | Logistic regression is **discriminative**; LDA is **generative**                                   |\n",
    "| Confounding handling     | Can adjust for multiple predictors simultaneously                                                  |\n",
    "| Prediction formula       | $\\hat{p}(x) = \\frac{e^{\\hat{\\beta}_0 + \\hat{\\beta}_1 x}}{1 + e^{\\hat{\\beta}_0 + \\hat{\\beta}_1 x}}$ |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\underline{\\text{Case-Control Sampling and Logistic Regression}}$\n",
    "\n",
    "Case-control sampling is often used when we cannot afford to wait and collect a random sample over a long period. Instead of sampling from the entire population, we deliberately select a fixed number of *cases* (individuals with the outcome of interest) and *controls* (individuals without the outcome).\n",
    "\n",
    "For example, suppose we observe 160 cases of myocardial infarction (MI) and 302 controls, all from a specific male age group. In the overall population, the true prevalence of MI is low—about $\\pi = 5.1\\%$. However, in our case-control sample, the proportion of cases is artificially inflated:\n",
    "\n",
    "$$\n",
    "\\tilde{\\pi} = \\frac{160}{160 + 302} \\approx 0.35\n",
    "$$\n",
    "\n",
    "This overrepresentation is intentional—it allows us to study rare events like MI more efficiently. However, it means that the apparent risk in the sample ($\\tilde{\\pi}$) is much higher than the true population risk ($\\pi$). As a result, the intercept $\\hat{\\beta}_0$ in a logistic regression model fitted on the case-control data will be biased.\n",
    "\n",
    "Importantly, even though the intercept is distorted, the slope coefficients $\\beta_j$ can still be consistently estimated—as long as the model is correctly specified. This is because the relative influence of each predictor on the log-odds is preserved by the logistic likelihood, even under case-control sampling.\n",
    "\n",
    "To adjust the biased intercept, we can apply a simple correction:\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_0^* = \\hat{\\beta}_0 \n",
    "+ \\underbrace{\\log\\left( \\frac{\\pi}{1 - \\pi} \\right)}_{\\text{loaded transformation of true risk}} \n",
    "- \\underbrace{\\log\\left( \\frac{\\tilde{\\pi}}{1 - \\tilde{\\pi}} \\right)}_{\\text{loaded transformation of apparent risk}}\n",
    "$$\n",
    "\n",
    "This transformation re-centers the decision boundary to reflect the true base rate of disease in the population.\n",
    "\n",
    "In more detail logistic regression models the **log-odds** of the event (e.g., myocardial infarction) as a linear function:\n",
    "\n",
    "$$\n",
    "\\log\\left( \\frac{P(Y=1 \\mid X)}{1 - P(Y=1 \\mid X)} \\right) = \\beta_0 + \\beta_1 X_1 + \\dots + \\beta_p X_p\n",
    "$$\n",
    "\n",
    "In a **random sample** from the population, $\\hat{\\beta}_0$ is calibrated so that the model reflects the **true base rate** of the event in the population (i.e., prevalence $\\pi$).\n",
    "\n",
    "But in **case-control sampling**, we **intentionally over-sample** the rare event (e.g., disease cases) to ensure we have enough data to analyze. As a result:\n",
    "\n",
    "* The **proportion of cases in the sample** (denoted $\\tilde{\\pi}$) is artificially high.\n",
    "* The logistic regression fit uses this distorted sample proportion as if it were the real one, unless corrected.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 Why Is the Intercept Biased?\n",
    "\n",
    "The intercept $\\hat{\\beta}_0$ captures the **baseline log-odds** when all predictors $X_j = 0$. It's tied directly to the event probability *in the sample*:\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_0 = \\log\\left( \\frac{\\tilde{\\pi}}{1 - \\tilde{\\pi}} \\right) \\quad \\text{(when all } X_j = 0)\n",
    "$$\n",
    "\n",
    "But this is incorrect if $\\tilde{\\pi} \\ne \\pi$, as is the case in case-control sampling.\n",
    "\n",
    "So while the logistic regression learns the **relationship between the predictors and the outcome** (i.e., the slopes $\\beta_j$), it **learns the wrong baseline**—it thinks the event is more common than it truly is.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 Why Aren’t the Slopes Biased?\n",
    "\n",
    "Because logistic regression models the **log-odds ratio**, and the odds ratio is **invariant to sampling the marginal distribution of the outcome**, the slopes are unaffected by the altered class balance. In other words:\n",
    "\n",
    "* The slopes $\\beta_j$ capture the **change in log-odds per unit change in $X_j$**.\n",
    "* This relationship is preserved regardless of how many cases vs. controls you sample, as long as the sampling is independent of $X$ given $Y$.\n",
    "\n",
    "So:\n",
    "\n",
    "* **Slopes**: unaffected by class imbalance (log-odds ratios are preserved)\n",
    "* **Intercept**: biased due to distorted prevalence\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 Fixing the Intercept\n",
    "\n",
    "To restore the correct baseline, we adjust the intercept post-hoc:\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_0^* = \\hat{\\beta}_0 + \\log\\left( \\frac{\\pi}{1 - \\pi} \\right) - \\log\\left( \\frac{\\tilde{\\pi}}{1 - \\tilde{\\pi}} \\right)\n",
    "$$\n",
    "\n",
    "This:\n",
    "\n",
    "* **Adds in the true log-odds**\n",
    "* **Subtracts the apparent (sample) log-odds**\n",
    "* **Recalibrates** the model to reflect the actual prevalence in the population\n",
    "\n",
    "---\n",
    "\n",
    "Let me know if you'd like a diagram or numeric example to make this even more concrete.\n",
    "\n",
    "\n",
    "When working with rare binary outcomes, it's common to include *all* available cases and then sample a subset of controls. Empirically, sampling up to five controls per case is usually sufficient to stabilize the estimates. While adding more controls can reduce variance, the marginal gain in precision diminishes beyond a 5:1 control-to-case ratio.\n",
    "\n",
    "Finally, when the total sample size $n$ is large, it's not necessary to use all the data. Random subsampling of controls can still yield reliable parameter estimates, provided the sampling design is properly accounted for in estimation and inference.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### $\\underline{\\text{Generative Models for Classification}}:$\n",
    "\n",
    "Logistic regression directly models the **posterior probability**:\n",
    "\n",
    "$$\n",
    "\\Pr(Y = k \\mid X = x)\n",
    "$$\n",
    "\n",
    "This is called a **discriminative approach**, since it models the response $Y$ given predictors $X$.\n",
    "\n",
    "### **Generative Approach**\n",
    "\n",
    "Instead of modeling $\\Pr(Y \\mid X)$ directly, we:\n",
    "\n",
    "1. Model the **class priors** $\\pi_k = \\Pr(Y = k)$\n",
    "2. Model the **class-conditional density** $f_k(x) = \\Pr(X = x \\mid Y = k)$\n",
    "\n",
    "Then apply **Bayes' theorem**:\n",
    "\n",
    "$$\n",
    "\\Pr(Y = k \\mid X = x) = \\frac{\\pi_k f_k(x)}{\\sum_{l=1}^K \\pi_l f_l(x)}\n",
    "$$\n",
    "\n",
    "This gives us the **posterior probability** $p_k(x)$, which we can use to classify observations by assigning them to the class with the highest posterior (i.e., **Bayes classifier**).\n",
    "\n",
    "We use this approach because \n",
    "\n",
    "* **More stable** than logistic regression when classes are well separated.\n",
    "* **Better performance** with small sample sizes, assuming normality of $X$ within each class.\n",
    "* Naturally generalizes to multi-class settings.\n",
    "* Enables approximation of the **Bayes classifier**, which has the lowest theoretical error (if $\\pi_k$ and $f_k(x)$ are correctly specified).\n",
    "\n",
    "**Estimating $\\pi_k$ and $f_k(x)$**\n",
    "\n",
    "* $\\pi_k$ can be estimated easily as the proportion of training examples in class $k$.\n",
    "* Estimating $f_k(x)$ is harder and typically requires assumptions (e.g., normal distribution).\n",
    "\n",
    "This framework leads to three common generative classifiers:\n",
    "\n",
    "* **Linear Discriminant Analysis (LDA)**\n",
    "* **Quadratic Discriminant Analysis (QDA)**\n",
    "* **Naive Bayes**\n",
    "\n",
    "Each uses a different assumption about the form of $f_k(x)$, allowing practical approximation of the Bayes classifier.\n",
    "\n",
    "\n",
    "### **Linear Discriminant Analysis for $p=1$:**\n",
    "\n",
    "**LDA** is a generative classification method that models the conditional distribution $X \\mid Y = k$ as **Gaussian** with **class-specific means** $\\mu_k$ and a **shared variance** $\\sigma^2$. Using **Bayes’ theorem**, it computes the posterior $\\Pr(Y = k \\mid X = x)$, and classifies observations by selecting the class with the highest posterior (i.e., the **Bayes classifier**).\n",
    "\n",
    "**Assumptions (Univariate case):**\n",
    "\n",
    "$$\n",
    "f_k(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( -\\frac{(x - \\mu_k)^2}{2\\sigma^2} \\right)\n",
    "$$\n",
    "\n",
    "* $X \\mid Y = k \\sim \\mathcal{N}(\\mu_k, \\sigma^2)$\n",
    "* Common variance across all classes\n",
    "* Class priors $\\pi_k$ are known or estimated from the training data\n",
    "\n",
    "**Discriminant Function:**\n",
    "\n",
    "LDA simplifies the Bayes rule into a **linear discriminant function**:\n",
    "\n",
    "$$\n",
    "\\delta_k(x) = \\frac{\\mu_k}{\\sigma^2}x - \\frac{\\mu_k^2}{2\\sigma^2} + \\log(\\pi_k)\n",
    "$$\n",
    "\n",
    "Predict the class with the largest $\\delta_k(x)$.\n",
    "\n",
    "**Parameter Estimation:**\n",
    "\n",
    "Given training data:\n",
    "\n",
    "* **Class means**:\n",
    "\n",
    "  $$\n",
    "  \\hat{\\mu}_k = \\frac{1}{n_k} \\sum_{i: y_i = k} x_i\n",
    "  $$\n",
    "\n",
    "* **Shared variance**:\n",
    "\n",
    "  $$\n",
    "  \\hat{\\sigma}^2 = \\frac{1}{n - K} \\sum_{k=1}^K \\sum_{i: y_i = k} (x_i - \\hat{\\mu}_k)^2\n",
    "  $$\n",
    "\n",
    "* **Class priors**:\n",
    "\n",
    "  $$\n",
    "  \\hat{\\pi}_k = \\frac{n_k}{n}\n",
    "  $$\n",
    "\n",
    "These are substituted into the estimated discriminant:\n",
    "\n",
    "$$\n",
    "\\hat{\\delta}_k(x) = \\frac{\\hat{\\mu}_k}{\\hat{\\sigma}^2}x - \\frac{\\hat{\\mu}_k^2}{2\\hat{\\sigma}^2} + \\log(\\hat{\\pi}_k)\n",
    "$$\n",
    "\n",
    "\n",
    "**Decision Boundary (Two Classes, Equal Priors):**\n",
    "\n",
    "If $\\pi_1 = \\pi_2$, then the LDA decision boundary is simply:\n",
    "\n",
    "$$\n",
    "x = \\frac{\\mu_1 + \\mu_2}{2}\n",
    "$$\n",
    "\n",
    "**Performance Insight:**\n",
    "\n",
    "In simulated examples, LDA achieves near-optimal performance (e.g., 11.1% test error vs. 10.6% Bayes error), assuming the Gaussian and equal variance assumptions hold.\n",
    "\n",
    "**Model Assumptions:**\n",
    "\n",
    "LDA assumes Gaussian-distributed predictors with **class-specific means** and a **common variance**. When this assumption holds, it closely approximates the Bayes classifier. If not, more flexible models like **QDA** (which allows class-specific variances) may perform better.\n",
    "\n",
    "\n",
    "**Summary Table**\n",
    "\n",
    "| Concept               | Description                                                             |\n",
    "| --------------------- | ----------------------------------------------------------------------- |\n",
    "| Generative assumption | $X \\mid Y = k \\sim \\mathcal{N}(\\mu_k, \\sigma^2)$                        |\n",
    "| Decision rule         | Assign $x$ to class with largest $\\delta_k(x)$                          |\n",
    "| Discriminant function | Linear in $x$                                                           |\n",
    "| Parameters estimated  | $\\mu_k$, shared $\\sigma^2$, class prior $\\pi_k$                         |\n",
    "| Strengths             | Simple, fast, interpretable, near-optimal if assumptions hold           |\n",
    "| Limitations           | Assumes equal variance; less flexible than QDA or nonparametric methods |\n",
    "| Output                | Posterior probabilities, linear decision boundaries                     |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Linear Discriminant Analysis for $p > 1$:**\n",
    "\n",
    "**Assumptions**\n",
    "\n",
    "LDA assumes that the predictor $\\mathbf{X} \\in \\mathbb{R}^p$ follows a **multivariate Gaussian distribution** within each class $k$:\n",
    "\n",
    "$$\n",
    "\\mathbf{X} \\mid Y = k \\sim \\mathcal{N}(\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma})\n",
    "$$\n",
    "\n",
    "* $\\boldsymbol{\\mu}_k$: class-specific mean vector ($p \\times 1$)\n",
    "* $\\boldsymbol{\\Sigma}$: shared covariance matrix ($p \\times p$), same for all $K$ classes\n",
    "* $\\pi_k$: class prior probability, often estimated from class frequencies\n",
    "\n",
    "**Multivariate Gaussian Density**\n",
    "\n",
    "The density for class $k$ is:\n",
    "\n",
    "$$\n",
    "f_k(\\mathbf{x}) = \\frac{1}{(2\\pi)^{p/2} |\\boldsymbol{\\Sigma}|^{1/2}} \\exp\\left( -\\frac{1}{2}(\\mathbf{x} - \\boldsymbol{\\mu}_k)^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}_k) \\right)\n",
    "$$\n",
    "\n",
    "**Discriminant Function**\n",
    "\n",
    "Instead of directly comparing the densities, LDA defines a **discriminant score** $\\delta_k(\\mathbf{x})$, derived from log-posterior:\n",
    "\n",
    "$$\n",
    "\\delta_k(\\mathbf{x}) = \\mathbf{x}^T \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}_k - \\frac{1}{2} \\boldsymbol{\\mu}_k^T \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}_k + \\log(\\pi_k)\n",
    "$$\n",
    "\n",
    "An observation is assigned to the class with the **largest** $\\delta_k(\\mathbf{x})$.\n",
    "\n",
    "Each $\\delta_k(\\mathbf{x})$ is a **linear function** of $\\mathbf{x}$, which gives LDA its name.\n",
    "\n",
    "**Decision Boundaries**\n",
    "\n",
    "* Boundaries between class $k$ and class $\\ell$ are defined by the set of $\\mathbf{x}$ where $\\delta_k(\\mathbf{x}) = \\delta_\\ell(\\mathbf{x})$\n",
    "* These are **linear hyperplanes** in $\\mathbb{R}^p$\n",
    "\n",
    "**Parameter Estimation from Training Data**\n",
    "\n",
    "Given $n$ training examples and $K$ classes:\n",
    "\n",
    "* Sample mean vector for class $k$:\n",
    "\n",
    "  $$\n",
    "  \\hat{\\boldsymbol{\\mu}}_k = \\frac{1}{n_k} \\sum_{i: y_i = k} \\mathbf{x}_i\n",
    "  $$\n",
    "\n",
    "* Shared covariance matrix:\n",
    "\n",
    "  $$\n",
    "  \\hat{\\boldsymbol{\\Sigma}} = \\frac{1}{n - K} \\sum_{k=1}^K \\sum_{i: y_i = k} (\\mathbf{x}_i - \\hat{\\boldsymbol{\\mu}}_k)(\\mathbf{x}_i - \\hat{\\boldsymbol{\\mu}}_k)^T\n",
    "  $$\n",
    "\n",
    "* Priors:\n",
    "\n",
    "  $$\n",
    "  \\hat{\\pi}_k = \\frac{n_k}{n}\n",
    "  $$\n",
    "\n",
    "**Classification Rule**\n",
    "\n",
    "Plug in the estimates into the discriminant function:\n",
    "\n",
    "$$\n",
    "\\hat{\\delta}_k(\\mathbf{x}) = \\mathbf{x}^T \\hat{\\boldsymbol{\\Sigma}}^{-1} \\hat{\\boldsymbol{\\mu}}_k - \\frac{1}{2} \\hat{\\boldsymbol{\\mu}}_k^T \\hat{\\boldsymbol{\\Sigma}}^{-1} \\hat{\\boldsymbol{\\mu}}_k + \\log(\\hat{\\pi}_k)\n",
    "$$\n",
    "\n",
    "Assign $\\mathbf{x}$ to the class with the largest $\\hat{\\delta}_k(\\mathbf{x})$.\n",
    "\n",
    "**Sensitivity to Thresholding**\n",
    "\n",
    "LDA implicitly uses a **0.5 threshold** for the posterior probability. This minimizes overall error but can underperform on specific classes (e.g., defaulters). Lowering the threshold (e.g., to 0.2) increases **sensitivity** (true positive rate) at the cost of **specificity** (true negative rate).\n",
    "\n",
    "This tradeoff is visualized with a **ROC curve**; the **area under the curve (AUC)** summarizes classifier performance across all thresholds. For example, AUC = 0.95 indicates excellent classification.\n",
    "\n",
    "**Confusion Matrix Terminology (Table 4.6)**\n",
    "\n",
    "| True Class   | Predicted Negative  | Predicted Positive  | Total |\n",
    "| ------------ | ------------------- | ------------------- | ----- |\n",
    "| Negative (−) | TN (True Negative)  | FP (False Positive) | N\\*   |\n",
    "| Positive (+) | FN (False Negative) | TP (True Positive)  | P\\*   |\n",
    "| Total        | N                   | P                   |       |\n",
    "\n",
    "**Key Metrics (Table 4.7)**\n",
    "\n",
    "| Name                          | Formula          | Synonyms                               |\n",
    "| ----------------------------- | ---------------- | -------------------------------------- |\n",
    "| **False Positive Rate**       | $\\frac{FP}{N}$   | Type I error, $1 - \\text{Specificity}$ |\n",
    "| **True Positive Rate**        | $\\frac{TP}{P}$   | Power, Sensitivity, Recall             |\n",
    "| **Positive Predictive Value** | $\\frac{TP}{P^*}$ | Precision                              |\n",
    "| **Negative Predictive Value** | $\\frac{TN}{N^*}$ | —                                      |\n",
    "\n",
    "---\n",
    "\n",
    "**LDA $p>1$ Summary Table**\n",
    "\n",
    "| Concept                   | Description                                                                          |\n",
    "| ------------------------- | ------------------------------------------------------------------------------------ |\n",
    "| **Assumption**            | Multivariate normal distribution with class-specific means and shared covariance     |\n",
    "| **Discriminant function** | Linear in $\\mathbf{x}$, derived from Bayes theorem                                   |\n",
    "| **Classification rule**   | Assign to class with highest $\\hat{\\delta}_k(\\mathbf{x})$                            |\n",
    "| **Decision boundary**     | Linear hyperplanes between class regions                                             |\n",
    "| **Parameter estimation**  | Estimate $\\boldsymbol{\\mu}_k$, $\\boldsymbol{\\Sigma}$, and $\\pi_k$ from training data |\n",
    "| **Effect of threshold**   | Lower thresholds increase sensitivity at the cost of specificity                     |\n",
    "| **ROC/AUC**               | Summarize classifier performance across all thresholds                               |\n",
    "| **When LDA works well**   | When classes are approximately Gaussian and have equal covariances                   |\n",
    "| **Limitations**           | Poor performance if assumptions are violated or classes are highly non-linear        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Total Error Rate and Prior Probabilities**\n",
    "\n",
    "In a binary classification problem, we define:\n",
    "\n",
    "* $\\text{FPR} = P(\\text{Predict Positive} \\mid \\text{Actual Negative})$\n",
    "* $\\text{FNR} = P(\\text{Predict Negative} \\mid \\text{Actual Positive})$\n",
    "\n",
    "Let:\n",
    "\n",
    "* $\\pi_+ = P(\\text{Positive})$ (prior for positive class)\n",
    "* $\\pi_- = P(\\text{Negative}) = 1 - \\pi_+$\n",
    "\n",
    "Then the **Total Error Rate** is:\n",
    "\n",
    "$$\n",
    "\\text{Total Error} = P(\\text{False Positive}) + P(\\text{False Negative})\n",
    "$$\n",
    "\n",
    "Using the law of total probability:\n",
    "\n",
    "$$\n",
    "\\text{Total Error} = \\pi_- \\cdot \\text{FPR} + \\pi_+ \\cdot \\text{FNR}\n",
    "$$\n",
    "\n",
    "This is a **weighted average** of the FPR and FNR, where the weights are the prior probabilities $\\pi_-$ and $\\pi_+$. This follows directly from how probabilities of conditional events are combined:\n",
    "\n",
    "* $P(\\text{False Positive}) = P(\\text{Predict Positive} \\mid \\text{Actual Negative}) \\cdot P(\\text{Actual Negative}) = \\text{FPR} \\cdot \\pi_-$\n",
    "* $P(\\text{False Negative}) = P(\\text{Predict Negative} \\mid \\text{Actual Positive}) \\cdot P(\\text{Actual Positive}) = \\text{FNR} \\cdot \\pi_+$\n",
    "\n",
    "Therefore, \n",
    "\n",
    "$$\n",
    "\\text{Total Error} = \\pi_- \\cdot \\text{FPR} + \\pi_+ \\cdot \\text{FNR}\n",
    "$$\n",
    "\n",
    "This expression shows that a **high FNR might not contribute much to the total error** if $\\pi_+$ is very small because it's multiplied by a small weight. Likewise, a high FPR matters more if $\\pi_-$ is large.\n",
    "\n",
    "Imagine a disease test:\n",
    "\n",
    "* The disease (positive class) is very rare: $\\pi_+ = 0.01$\n",
    "* The test has a high FNR (misses many actual cases), say $\\text{FNR} = 0.4$\n",
    "* But because the disease is rare, the actual number of missed positives is small in the population\n",
    "* So the **overall error rate** remains low if FPR is low and $\\pi_-$ is large\n",
    "\n",
    "This is why **prior probabilities play a critical role** in evaluating classifier performance, especially when the classes are imbalanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quadratic Discriminant Analysis (QDA)\n",
    "\n",
    "QDA is a generative classification model that, like LDA, assumes the feature vector $\\mathbf{X} \\in \\mathbb{R}^p$ follows a multivariate Gaussian distribution **within each class**, but **allows a separate covariance matrix** for each class:\n",
    "\n",
    "$$\n",
    "\\mathbf{X} \\mid Y = k \\sim \\mathcal{N}(\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)\n",
    "$$\n",
    "\n",
    "* $\\boldsymbol{\\mu}_k$ is the class-specific mean vector.\n",
    "* $\\boldsymbol{\\Sigma}_k$ is the **class-specific** covariance matrix.\n",
    "* $\\pi_k = \\mathbb{P}(Y = k)$ is the prior probability for class \\$k\\$.\n",
    "\n",
    "**QDA Discriminant Function**\n",
    "\n",
    "Using Bayes’ theorem, the log-posterior (up to proportionality) becomes:\n",
    "\n",
    "$$\n",
    "\\delta_k(\\mathbf{x}) = \n",
    "- \\frac{1}{2} (\\mathbf{x} - \\boldsymbol{\\mu}_k)^T \\boldsymbol{\\Sigma}_k^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}_k)\n",
    "- \\frac{1}{2} \\log |\\boldsymbol{\\Sigma}_k| \n",
    "+ \\log \\pi_k\n",
    "$$\n",
    "\n",
    "This expands to:\n",
    "\n",
    "$$\n",
    "\\delta_k(\\mathbf{x}) \n",
    "= -\\frac{1}{2} \\mathbf{x}^T \\boldsymbol{\\Sigma}_k^{-1} \\mathbf{x}\n",
    "+ \\mathbf{x}^T \\boldsymbol{\\Sigma}_k^{-1} \\boldsymbol{\\mu}_k \n",
    "- \\frac{1}{2} \\boldsymbol{\\mu}_k^T \\boldsymbol{\\Sigma}_k^{-1} \\boldsymbol{\\mu}_k \n",
    "- \\frac{1}{2} \\log |\\boldsymbol{\\Sigma}_k| + \\log \\pi_k\n",
    "$$\n",
    "\n",
    "Since this includes a **quadratic form** in $\\mathbf{x}$, QDA’s decision boundaries are **quadratic surfaces**, unlike the linear boundaries in LDA.\n",
    "\n",
    "**QDA Classification Rule**\n",
    "\n",
    "Assign $\\mathbf{x}$ to the class with the largest discriminant score:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\arg\\max_k \\delta_k(\\mathbf{x})\n",
    "$$\n",
    "\n",
    "**Parameter Estimation**\n",
    "\n",
    "From the training data:\n",
    "\n",
    "- $\\hat{\\mu}_k = \\dfrac{1}{n_k} \\sum_{i: y_i = k} \\mathbf{x}_i$\n",
    "- $\\hat{\\boldsymbol{\\Sigma}}_k = \\dfrac{1}{n_k - 1} \\sum_{i: y_i = k} (\\mathbf{x}_i - \\hat{\\mu}_k)(\\mathbf{x}_i - \\hat{\\mu}_k)^T$\n",
    "- $\\hat{\\pi}_k = \\dfrac{n_k}{n}$\n",
    "\n",
    "**LDA vs. QDA: Bias–Variance Trade-Off**\n",
    "\n",
    "| Model   | Covariance Assumption                | Decision Boundary | Covariance Params               | Variance | Bias                          |\n",
    "|--------|----------------------------------------|-------------------|----------------------------------|----------|-------------------------------|\n",
    "| **LDA** | Shared $\\boldsymbol{\\Sigma}$ across classes | Linear            | $\\frac{p(p+1)}{2}$ total         | Low      | High (if assumption is wrong) |\n",
    "| **QDA** | Class-specific $\\boldsymbol{\\Sigma}_k$     | Quadratic         | $K \\cdot \\frac{p(p+1)}{2}$ total | High     | Low (if assumption holds)     |\n",
    "\n",
    "- **Use LDA** when $n$ is small or classes have similar covariance structure — lower variance improves generalization.\n",
    "- **Use QDA** when $n$ is large or class covariances are clearly different — more flexibility reduces bias.\n",
    "\n",
    "**Geometric Interpretation (ISLR Fig. 4.9)**\n",
    "\n",
    "* **Left Panel:** $\\Sigma_1 = \\Sigma_2$. Bayes boundary is linear. LDA matches Bayes well. QDA slightly overfits.\n",
    "* **Right Panel:** $\\Sigma_1 \\ne \\Sigma_2$. Bayes boundary is curved. QDA matches Bayes better. LDA underfits.\n",
    "\n",
    "\n",
    "**Summary Table — LDA vs. QDA**\n",
    "| Feature               | LDA                                | QDA                                     |\n",
    "|-----------------------|-------------------------------------|------------------------------------------|\n",
    "| Covariance Structure  | Common to all classes ($\\Sigma$)   | Separate for each class ($\\Sigma_k$)     |\n",
    "| Decision Boundary     | Linear                             | Quadratic                                |\n",
    "| Covariance Parameters | $\\frac{p(p+1)}{2}$                 | $K \\cdot \\frac{p(p+1)}{2}$               |\n",
    "| Variance              | Low                                | Higher                                   |\n",
    "| Bias                  | High (if assumption violated)      | Low (if assumption holds)                |\n",
    "| Best Use Case         | Few samples, similar class spreads | Many samples, class-specific spreads     |\n",
    "| Computational Cost    | Low                                | Higher (due to matrix per class)         |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes Theorem Review\n",
    "Recall, to compute the **posterior probability** for a new point $\\mathbf{x}$, we apply:\n",
    "\n",
    "$$\n",
    "\\Pr(Y = k \\mid \\mathbf{x}) =\n",
    "\\frac{\\pi_k f_k(\\mathbf{x})}{\\sum_{\\ell = 1}^{K} \\pi_\\ell f_\\ell(\\mathbf{x})}.\n",
    "$$\n",
    "\n",
    "This expression has:\n",
    "\n",
    "* **Numerator**: Joint likelihood of class $k$: $\\pi_k f_k(\\mathbf{x})$\n",
    "* **Denominator**: Marginal likelihood: $\\Pr(\\mathbf{x}) = \\sum_{\\ell = 1}^{K} \\pi_\\ell f_\\ell(\\mathbf{x})$\n",
    "\n",
    "The denominator **normalizes** the posteriors to sum to 1 across all classes.\n",
    "\n",
    "We model each class-conditional distribution $f_k$ **separately**. This decomposition allows us to:\n",
    "\n",
    "* Handle class imbalance with $\\pi_k$\n",
    "* Compare how well each class explains $\\mathbf{x}$\n",
    "* Make predictions using:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\arg\\max_k \\Pr(Y = k \\mid \\mathbf{x})\n",
    "$$\n",
    "\n",
    "### Naive Bayes\n",
    "\n",
    "Naive Bayes is a **generative classification model** built from Bayes’ Theorem:\n",
    "\n",
    "$$\n",
    "\\Pr(Y = k \\mid X = x) = \\frac{\\pi_k f_k(x)}{\\sum_{l=1}^K \\pi_l f_l(x)}\n",
    "$$\n",
    "\n",
    "* $\\pi_k$ is the **prior**: probability of class $k$.\n",
    "* $f_k(x) = \\Pr(X = x \\mid Y = k)$ is the **class-conditional density**: the likelihood of observing $x$ in class $k$.\n",
    "\n",
    "We want to estimate the full joint density $f_k(x)$ for $x \\in \\mathbb{R}^p$, which is hard when $p$ is large.\n",
    "\n",
    "A solution is to use a *naive assumption* where we assume **conditional independence** of features within each class\n",
    "\n",
    "$$\n",
    "f_k(x) = \\prod_{j=1}^p f_{kj}(x_j)\n",
    "$$\n",
    "\n",
    "This simplifies a high-dimensional density estimation problem into $p$ univariate density estimates per class.\n",
    "\n",
    "**Density Estimation by Feature Type**\n",
    "\n",
    "*Quantitative*:\n",
    "\n",
    "  * Assume $x_j \\mid Y = k \\sim \\mathcal{N}(\\mu_{jk}, \\sigma_{jk}^2)$ (a **univariate** normal, unlike QDA).\n",
    "  * Or use **non-parametric methods** like histograms or kernel density estimates.\n",
    "\n",
    "*Qualitative*:\n",
    "\n",
    "  * Estimate $f_{kj}(x_j)$ by relative frequency:\n",
    "\n",
    "    $$\n",
    "    \\hat{f}_{kj}(x_j) = \\frac{\\text{count of } x_j \\text{ in class } k}{\\text{total samples in class } k}\n",
    "    $$\n",
    "\n",
    "Once $\\pi_k$ and each $f_{kj}$ are estimated:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\arg\\max_k \\left( \\pi_k \\cdot \\prod_{j=1}^p f_{kj}(x_j) \\right)\n",
    "$$\n",
    "\n",
    "Or equivalently, work in log space\n",
    "\n",
    "$$\n",
    "\\log \\Pr(Y=k \\mid X=x) = \\log \\pi_k + \\sum_{j=1}^p \\log f_{kj}(x_j) + C\n",
    "$$\n",
    "\n",
    "*Pros*:\n",
    "\n",
    "  * Fast and works well even with high dimensions ($p$ large).\n",
    "  * Robust to irrelevant features.\n",
    "  * Works with small datasets due to **low variance** from independence assumption.\n",
    "\n",
    "*Cons*:\n",
    "\n",
    "  * Independence assumption is usually false.\n",
    "  * Can underperform when joint dependencies are critical (e.g. image pixels).\n",
    "\n",
    "**Naive Bayes vs. LDA**\n",
    "\n",
    "* Naive Bayes assumes **independence**, not Gaussianity of full $X$.\n",
    "* Can be viewed as a simplification of **QDA with diagonal covariance**.\n",
    "* Works best when $n$ is small, $p$ is large, or many irrelevant features**.\n",
    "\n",
    "**Example (Credit Default Data Set)**\n",
    "\n",
    "* With threshold $0.5$, Naive Bayes correctly identifies more true defaulters than LDA but has slightly higher overall error.\n",
    "* With threshold $0.2$, catches 2/3 of defaulters, trading off false positives.\n",
    "\n",
    "**Summary Table: Naive Bayes vs LDA vs QDA**\n",
    "\n",
    "| Feature               | Naive Bayes                             | LDA                                          | QDA                                           |\n",
    "|-----------------------|------------------------------------------|-----------------------------------------------|-----------------------------------------------|\n",
    "| Covariance Structure  | Diagonal (assumes independence)         | Shared across classes                        | Class-specific                                |\n",
    "| Density Assumption    | Univariate per feature, per class       | Multivariate Gaussian                        | Multivariate Gaussian                         |\n",
    "| Decision Boundary     | Generally non-linear                    | Linear                                       | Quadratic                                     |\n",
    "| Parameters Estimated  | $K \\cdot p$                             | $Kp$ (means) + $\\frac{p(p+1)}{2}$ (shared $\\Sigma$) | $Kp$ (means) + $K \\cdot \\frac{p(p+1)}{2}$ (individual $\\Sigma_k$) |\n",
    "| Variance              | Low                                     | Medium                                       | High                                          |\n",
    "| Bias                  | High (due to independence assumption)   | Medium                                       | Low                                           |\n",
    "| Best Use Case         | High-dim, small $n$, many irrelevant features | Few predictors, moderate $n$             | Large $n$, complex boundaries                 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More on Naive Bayes and Conditional Independence\n",
    "\n",
    "We study a classification problem where the input feature vector is\n",
    "\n",
    "$$\n",
    "\\mathbf{X} = (X_1, X_2)^T \\in \\mathbb{R}^2\n",
    "$$\n",
    "\n",
    "and the class label is\n",
    "\n",
    "$$\n",
    "Y \\in \\{0, 1\\}.\n",
    "$$\n",
    "\n",
    "We assume that the distribution of the features conditional on the class label follows a multivariate normal distribution:\n",
    "\n",
    "$$\n",
    "\\mathbf{X} \\mid Y = k \\sim \\mathcal{N}(\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k),\n",
    "$$\n",
    "\n",
    "where the parameters for each class $k \\in \\{0, 1\\}$ are:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\mu}_k =\n",
    "\\begin{pmatrix}\n",
    "\\mu_{k1} \\\\\n",
    "\\mu_{k2}\n",
    "\\end{pmatrix}, \\quad\n",
    "\\boldsymbol{\\Sigma}_k =\n",
    "\\begin{pmatrix}\n",
    "\\sigma_{k1}^2 & \\rho_k \\sigma_{k1} \\sigma_{k2} \\\\\n",
    "\\rho_k \\sigma_{k1} \\sigma_{k2} & \\sigma_{k2}^2\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "The off-diagonal terms capture the dependence between $X_1$ and $X_2$ within each class via the correlation coefficient $\\rho_k$. This is the general, **non-naive** case.\n",
    "\n",
    "**Case 1: No Conditional Independence (Full Gaussian)**\n",
    "\n",
    "In this setup, the joint class-conditional density is:\n",
    "\n",
    "$$\n",
    "f_k(x_1, x_2) =\n",
    "\\frac{1}{2\\pi |\\boldsymbol{\\Sigma}_k|^{1/2}}\n",
    "\\exp\\left( -\\frac{1}{2}(\\mathbf{x} - \\boldsymbol{\\mu}_k)^T \\boldsymbol{\\Sigma}_k^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}_k) \\right).\n",
    "$$\n",
    "\n",
    "To estimate this density, we need:\n",
    "\n",
    "* Two means: $\\mu_{k1}, \\mu_{k2}$\n",
    "* Two variances: $\\sigma_{k1}^2, \\sigma_{k2}^2$\n",
    "* One correlation coefficient: $\\rho_k \\in [-1, 1]$\n",
    "\n",
    "Total: *5 parameters per class*\n",
    "\n",
    "This gives flexibility, but can be computationally expensive and prone to overfitting in small data settings.\n",
    "\n",
    "**Case 2: Conditional Independence (Naive Bayes Assumption)**\n",
    "\n",
    "Under the **naive Bayes** assumption, we simplify the joint density using:\n",
    "\n",
    "$$\n",
    "f_k(x_1, x_2) = f_{k1}(x_1) \\cdot f_{k2}(x_2).\n",
    "$$\n",
    "\n",
    "Each marginal $f_{kj}(x_j)$ is modeled as a univariate Gaussian:\n",
    "\n",
    "$$\n",
    "f_{kj}(x_j) =\n",
    "\\frac{1}{\\sqrt{2\\pi \\sigma_{kj}^2}}\n",
    "\\exp\\left( -\\frac{(x_j - \\mu_{kj})^2}{2\\sigma_{kj}^2} \\right).\n",
    "$$\n",
    "\n",
    "Now we estimate:\n",
    "\n",
    "* Two means: $\\mu_{k1}, \\mu_{k2}$\n",
    "* Two variances: $\\sigma_{k1}^2, \\sigma_{k2}^2$\n",
    "\n",
    "Total: *4 parameters per class*\n",
    "\n",
    "The correlation between features is ignored, but the model becomes much easier to train and generalizes better when data is limited.\n",
    "\n",
    "**Example**\n",
    "\n",
    "We observe the following data:\n",
    "\n",
    "| Class $Y$ | $X_1$ | $X_2$ |\n",
    "| --------- | ----- | ----- |\n",
    "| 0         | 2.0   | 4.0   |\n",
    "| 0         | 2.5   | 4.5   |\n",
    "| 1         | 6.0   | 1.0   |\n",
    "| 1         | 5.5   | 1.5   |\n",
    "\n",
    "*Class 0:*\n",
    "\n",
    "* $\\hat{\\mu}_0 = \\begin{pmatrix} 2.25 \\\\ 4.25 \\end{pmatrix}$\n",
    "* Sample covariance:\n",
    "\n",
    "$$\n",
    "\\hat{\\Sigma}_0 =\n",
    "\\begin{pmatrix}\n",
    "0.125 & 0.125 \\\\\n",
    "0.125 & 0.125\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "*Class 1:*\n",
    "\n",
    "* $\\hat{\\mu}_1 = \\begin{pmatrix} 5.75 \\\\ 1.25 \\end{pmatrix}$\n",
    "* Sample covariance:\n",
    "\n",
    "$$\n",
    "\\hat{\\Sigma}_1 =\n",
    "\\begin{pmatrix}\n",
    "0.125 & -0.125 \\\\\n",
    "-0.125 & 0.125\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "The covariance matrices are **not diagonal**, which means feature correlation is significant.\n",
    "\n",
    "**Naive Bayes on Same Data**\n",
    "\n",
    "Now assume conditional independence. We compute:\n",
    "\n",
    "*Class 0:*\n",
    "* $\\mu_{01} = 2.25$, $\\sigma_{01}^2 = 0.125$\n",
    "* $\\mu_{02} = 4.25$, $\\sigma_{02}^2 = 0.125$\n",
    "\n",
    "*Class 1:*\n",
    "* $\\mu_{11} = 5.75$, $\\sigma_{11}^2 = 0.125$\n",
    "* $\\mu_{12} = 1.25$, $\\sigma_{12}^2 = 0.125$\n",
    "\n",
    "Thus, the full joint density becomes:\n",
    "\n",
    "$$\n",
    "f_k(x_1, x_2) = f_{k1}(x_1) \\cdot f_{k2}(x_2),\n",
    "$$\n",
    "\n",
    "avoiding any matrix operations or correlation estimation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Comparison of Classification Methods\n",
    "\n",
    "We examine and compare the structure of several widely used classification methods: Linear Discriminant Analysis (LDA), Quadratic Discriminant Analysis (QDA), Naive Bayes, Logistic Regression, and K-Nearest Neighbors (KNN). We focus on how each method models the posterior class probabilities $\\Pr(Y = k \\mid \\mathbf{x})$ and the log-odds between classes.\n",
    "\n",
    "We work in a setting with $K$ classes, where we wish to assign an observation $\\mathbf{x} \\in \\mathbb{R}^p$ to the class that maximizes the posterior probability. Using Bayes’ theorem, this is equivalent to maximizing:\n",
    "\n",
    "$$\n",
    "\\log \\frac{\\Pr(Y = k \\mid \\mathbf{x})}{\\Pr(Y = K \\mid \\mathbf{x})}, \\quad \\text{for } k = 1, \\dots, K - 1\n",
    "$$\n",
    "\n",
    "We now derive the functional form of this expression for each method.\n",
    "\n",
    "**Linear Discriminant Analysis (LDA)**\n",
    "\n",
    "LDA assumes the class-conditional distribution of the predictors follows a multivariate normal distribution with class-specific means and a common covariance matrix:\n",
    "\n",
    "$$\n",
    "\\mathbf{X} \\mid Y = k \\sim \\mathcal{N}(\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}), \\quad \\text{for all } k\n",
    "$$\n",
    "\n",
    "Under this model, the log-odds simplifies to:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\log \\frac{\\Pr(Y = k \\mid \\mathbf{x})}{\\Pr(Y = K \\mid \\mathbf{x})} &= \\log \\left(\\frac {\\pi_k f_k(x)}{\\pi_K f_K(x)} \\right) \\\\\n",
    "&=\\log\\left(\\frac{\\pi_k \\exp(-\\frac{1}{2}(x-\\mu_k)^T\\Sigma^{-1}(x-\\mu_k))}{\\pi_K \\exp(-\\frac{1}{2}(x-\\mu_K)^T\\Sigma^{-1}(x-\\mu_K))}\\right) \\\\\n",
    "&= \\log \\left( \\frac{\\pi_k}{\\pi_K} \\right)\n",
    "+ \\log \\left( \\frac{\n",
    "\\exp\\left(-\\frac{1}{2} (\\mathbf{x} - \\boldsymbol{\\mu}_k)^\\top \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}_k) \\right)\n",
    "}{\n",
    "\\exp\\left(-\\frac{1}{2} (\\mathbf{x} - \\boldsymbol{\\mu}_K)^\\top \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}_K) \\right)\n",
    "} \\right)\\\\\n",
    "&= \\log \\left( \\frac{\\pi_k}{\\pi_K} \\right)\n",
    "- \\frac{1}{2} (\\mathbf{x} - \\boldsymbol{\\mu}_k)^\\top \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}_k)\n",
    "+ \\frac{1}{2} (\\mathbf{x} - \\boldsymbol{\\mu}_K)^\\top \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}_K) \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Now expand and regroup:\n",
    "\n",
    "$$\n",
    "= \\log \\left( \\frac{\\pi_k}{\\pi_K} \\right)\n",
    "+ \\mathbf{x}^\\top \\boldsymbol{\\Sigma}^{-1} (\\boldsymbol{\\mu}_k - \\boldsymbol{\\mu}_K)\n",
    "- \\frac{1}{2} \\left( \\boldsymbol{\\mu}_k^\\top \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}_k\n",
    "- \\boldsymbol{\\mu}_K^\\top \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}_K \\right)\n",
    "$$\n",
    "\n",
    "This can also be rearranged using a symmetric form (completing the square):\n",
    "\n",
    "$$\n",
    "= \\log \\left( \\frac{\\pi_k}{\\pi_K} \\right)\n",
    "- \\frac{1}{2} (\\boldsymbol{\\mu}_k + \\boldsymbol{\\mu}_K)^\\top \\boldsymbol{\\Sigma}^{-1} (\\boldsymbol{\\mu}_k - \\boldsymbol{\\mu}_K)\n",
    "+ \\mathbf{x}^\\top \\boldsymbol{\\Sigma}^{-1} (\\boldsymbol{\\mu}_k - \\boldsymbol{\\mu}_K)\n",
    "$$\n",
    "\n",
    "This gives the final linear discriminant form:\n",
    "\n",
    "$$\n",
    "= a_k + \\mathbf{b}_k^\\top \\mathbf{x}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "a_k = \\log\\left(\\frac{\\pi_k}{\\pi_K}\\right) - \\frac{1}{2}(\\boldsymbol{\\mu}_k + \\boldsymbol{\\mu}_K)^T \\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{\\mu}_k - \\boldsymbol{\\mu}_K), \\quad\n",
    "\\mathbf{b}_k = \\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{\\mu}_k - \\boldsymbol{\\mu}_K)\n",
    "$$\n",
    "\n",
    "\n",
    "Hence, LDA assumes that the log-odds of the posterior probability is a linear function of $\\mathbf{x}$. The decision boundaries are linear hyperplanes.\n",
    "\n",
    "**Quadratic Discriminant Analysis (QDA)**\n",
    "\n",
    "QDA relaxes the assumption of shared covariance and allows for class-specific covariance matrices:\n",
    "\n",
    "$$\n",
    "\\mathbf{X} \\mid Y = k \\sim \\mathcal{N}(\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)\n",
    "$$\n",
    "\n",
    "The resulting log-odds becomes:\n",
    "\n",
    "$$\n",
    "\\log \\frac{\\Pr(Y = k \\mid \\mathbf{x})}{\\Pr(Y = K \\mid \\mathbf{x})}\n",
    "= a_k + \\sum_{j=1}^p b_{kj} x_j + \\sum_{j=1}^p \\sum_{l=1}^p c_{kjl} x_j x_l\n",
    "$$\n",
    "\n",
    "This expression includes both linear and quadratic terms in $\\mathbf{x}$. Therefore, QDA defines quadratic decision boundaries that can model more complex class separation.\n",
    "\n",
    "**Naive Bayes Classifier**\n",
    "\n",
    "Naive Bayes models each class-conditional density as a product of univariate densities, assuming that the features are conditionally independent given the class label:\n",
    "\n",
    "$$\n",
    "f_k(\\mathbf{x}) = \\prod_{j=1}^p f_{kj}(x_j)\n",
    "$$\n",
    "\n",
    "Taking logs, we obtain:\n",
    "\n",
    "$$\n",
    "\\log \\frac{\\Pr(Y = k \\mid \\mathbf{x})}{\\Pr(Y = K \\mid \\mathbf{x})}\n",
    "= \\log \\left(\\frac{\\pi_k}{\\pi_K} \\right) + \\sum_{j=1}^p \\log \\left( \\frac{f_{kj}(x_j)}{f_{Kj}(x_j)} \\right)\n",
    "= a_k + \\sum_{j=1}^p g_{kj}(x_j)\n",
    "$$\n",
    "\n",
    "If each $f_{kj}(x_j)$ is modeled as a univariate Gaussian, then $g_{kj}(x_j)$ becomes linear in $x_j$. This yields a generalized additive model, where the log-odds are additive in $x_j$, but not necessarily linear if the marginals are modeled non-parametrically.\n",
    "\n",
    "**Logistic Regression**\n",
    "\n",
    "Logistic regression directly models the posterior log-odds as a linear function of the predictors:\n",
    "\n",
    "$$\n",
    "\\log \\frac{\\Pr(Y = k \\mid \\mathbf{x})}{\\Pr(Y = K \\mid \\mathbf{x})}\n",
    "= \\beta_{k0} + \\sum_{j=1}^p \\beta_{kj} x_j\n",
    "$$\n",
    "\n",
    "The coefficients $\\beta_{kj}$ are estimated by maximizing the conditional likelihood of the data. Unlike LDA, logistic regression makes no assumptions about the distribution of $\\mathbf{X}$.\n",
    "\n",
    "**Structural Relationships Between Models**\n",
    "\n",
    "From the above derivations, we can draw the following conclusions:\n",
    "\n",
    "* **LDA is a special case of QDA**, in which the covariance matrices are identical across classes. This eliminates the quadratic terms, resulting in linear decision boundaries.\n",
    "\n",
    "* **LDA is also a special case of naive Bayes** when the univariate densities $f_{kj}(x_j)$ are Gaussian with class-specific means and shared variances across classes. In that case, naive Bayes reduces to LDA with a diagonal covariance matrix.\n",
    "\n",
    "* **Naive Bayes is not a special case of QDA**, nor is QDA a special case of naive Bayes. QDA includes multiplicative interaction terms (e.g., $x_j x_l$), while naive Bayes models additive terms only.\n",
    "\n",
    "* **Logistic regression and LDA both produce linear decision boundaries**, but they differ in how the coefficients are obtained. LDA derives its parameters from Gaussian class-conditional models, while logistic regression optimizes them directly via maximum likelihood.\n",
    "\n",
    "**Nonparametric Classification: K-Nearest Neighbors (KNN)**\n",
    "\n",
    "KNN takes a different approach. It does not attempt to model the distribution of $\\mathbf{X}$ within each class. Instead, it classifies a new observation based on the majority label among its $K$ nearest neighbors in the training data.\n",
    "\n",
    "* **KNN is nonparametric**, making no assumptions about class-conditional densities.\n",
    "* It can approximate arbitrarily complex decision boundaries but requires a large sample size relative to the number of predictors to avoid high variance.\n",
    "* **QDA may outperform KNN** in moderate data regimes where class-conditional structure exists but the sample size is not large enough to support fully nonparametric estimation.\n",
    "\n",
    "**Summary Table**\n",
    "\n",
    "| Model             | Log-Odds Form             | Decision Boundary  | Key Assumptions                                                  | Strengths                             | Limitations                         |\n",
    "| ----------------- | ------------------------- | ------------------ | ---------------------------------------------------------------- | ------------------------------------- | ----------------------------------- |\n",
    "| **LDA**           | Linear in $\\mathbf{x}$    | Linear             | Gaussian class-conditional densities, shared covariance          | Simple, low variance, interpretable   | Biased if covariances differ        |\n",
    "| **QDA**           | Quadratic in $\\mathbf{x}$ | Quadratic          | Gaussian class-conditional densities, class-specific covariances | Flexible, captures interactions       | More parameters, higher variance    |\n",
    "| **Naive Bayes**   | Additive in $x_j$         | Possibly nonlinear | Feature independence within classes                              | Scales to high dimensions, fast       | May ignore important dependencies   |\n",
    "| **Logistic Reg.** | Linear in $\\mathbf{x}$    | Linear             | None (no distributional assumptions)                             | Robust, well-calibrated probabilities | Misses nonlinear boundaries         |\n",
    "| **KNN**           | None (nonparametric)      | Arbitrary          | None (data-driven proximity)                                     | Nonlinear, no modeling assumptions    | Needs large $n$, sensitive to noise |\n",
    "\n",
    "**When to Use Which?**\n",
    "\n",
    "* Use **LDA** when class distributions are close to Gaussian and have similar covariances.\n",
    "* Use **QDA** when classes differ significantly in spread or orientation.\n",
    "* Use **Naive Bayes** when features are approximately independent and dimensionality is high.\n",
    "* Use **Logistic Regression** for robust, interpretable linear decision surfaces.\n",
    "* Use **KNN** when you have a lot of data and suspect complex, nonlinear boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certainly. Below is a textbook-style presentation of the empirical comparison of classification methods as described in Section 4.5.2. The focus is on clarity, completeness, and interpretability—suitable for advanced interview preparation.\n",
    "\n",
    "### Empirical Comparison of Classification Methods\n",
    "\n",
    "We now assess the practical performance of five widely used classifiers—Logistic Regression, Linear Discriminant Analysis (LDA), Quadratic Discriminant Analysis (QDA), Naive Bayes, and K-Nearest Neighbors (KNN)—across a variety of simulated scenarios. Each scenario involves a binary classification task with two quantitative predictors, and the goal is to evaluate how well each method generalizes to unseen data.\n",
    "\n",
    "For each scenario, we simulate 100 training sets and compute the test error for each method on a large independent test set. This approach allows us to observe the variability of each method’s performance across different realizations of the training data.\n",
    "\n",
    "\n",
    "**Experimental Design**\n",
    "\n",
    "Each scenario involves $p = 2$ predictors and $K = 2$ classes. In some settings, the Bayes decision boundary is linear; in others, it is nonlinear. We fit each method to the training data and evaluate test error across 100 random replicates. For KNN, we compare two versions:\n",
    "\n",
    "* KNN-1: K = 1 (no smoothing),\n",
    "* KNN-CV: K selected by cross-validation.\n",
    "\n",
    "Naive Bayes assumes univariate Gaussian distributions for each predictor within each class and independence across features.\n",
    "\n",
    "**Linear Decision Boundary Scenarios**\n",
    "\n",
    "*Scenario 1: Uncorrelated Gaussians, Small Sample*\n",
    "\n",
    "* 20 observations per class.\n",
    "* Independent standard normal predictors with different means for each class.\n",
    "* Decision boundary is linear.\n",
    "\n",
    "*Results*:\n",
    "\n",
    "* **LDA** performs best due to correct model assumptions.\n",
    "* **Logistic regression** also performs well due to its linear boundary.\n",
    "* **QDA** overfits, performing worse than LDA.\n",
    "* **Naive Bayes** slightly outperforms QDA, benefiting from correct independence assumption.\n",
    "* **KNN-1** suffers from high variance, leading to poor results.\n",
    "\n",
    "*Scenario 2: Correlated Gaussians ($\\rho = -0.5$)*\n",
    "\n",
    "* Same as Scenario 1, but predictors within each class are correlated.\n",
    "\n",
    "*Results*:\n",
    "\n",
    "* **LDA** and **logistic regression** maintain good performance.\n",
    "* **Naive Bayes** deteriorates due to violation of independence.\n",
    "* **QDA** improves slightly, reflecting its ability to model correlation.\n",
    "* **KNN-1** still suffers from variance.\n",
    "\n",
    "*Scenario 3*: t-distributed Predictors\n",
    "\n",
    "* Negative correlation between predictors.\n",
    "* Predictors follow a heavy-tailed t-distribution.\n",
    "* 50 observations per class.\n",
    "\n",
    "*Results*:\n",
    "\n",
    "* **Logistic regression** outperforms **LDA** due to robustness under non-normality.\n",
    "* **QDA** performs poorly under model misspecification.\n",
    "* **Naive Bayes** fails due to both non-normality and correlation.\n",
    "* **KNN-CV** slightly improves, but **KNN-1** still performs worst.\n",
    "\n",
    "*Nonlinear Decision Boundary Scenarios*\n",
    "\n",
    "*Scenario 4: Opposing Correlation*\n",
    "\n",
    "* Class 1: $\\rho = +0.5$, Class 2: $\\rho = -0.5$\n",
    "* Gaussian predictors.\n",
    "* Bayes boundary is quadratic.\n",
    "\n",
    "*Results*:\n",
    "\n",
    "* **QDA** performs best; it correctly models the class-specific covariances.\n",
    "* **LDA**, **logistic regression**, and **naive Bayes** underperform.\n",
    "* **KNN-CV** is competitive, though outperformed by QDA.\n",
    "\n",
    "*Scenario 5: Nonlinear Decision Boundary via Logistic Function*\n",
    "\n",
    "* Predictors are Gaussian and uncorrelated.\n",
    "* Class labels generated from a nonlinear transformation of predictors.\n",
    "\n",
    "*Results*:\n",
    "\n",
    "* **KNN-CV** performs best due to flexibility.\n",
    "* **QDA** and **naive Bayes** do reasonably well.\n",
    "* **Linear models** (LDA, logistic) struggle to capture the complexity.\n",
    "* **KNN-1** performs worst due to overfitting.\n",
    "\n",
    "*Scenario 6: Diagonal Covariance, Small Sample*\n",
    "\n",
    "* Class-specific diagonal covariance matrices.\n",
    "* \\$n = 6\\$ samples per class (very small).\n",
    "\n",
    "*Results*:\n",
    "\n",
    "* **Naive Bayes** performs best due to correct independence assumption and low complexity.\n",
    "* **QDA** overfits due to high variance in estimating full covariance.\n",
    "* **LDA** and **logistic regression** fail to capture nonlinearity.\n",
    "* **KNN** fails due to insufficient data.\n",
    "\n",
    "**Key Takeaways**\n",
    "\n",
    "* **Model assumptions matter**: Methods perform well when their assumptions align with the data-generating process.\n",
    "* **Bias-variance trade-off** is central:\n",
    "\n",
    "  * **LDA** and **logistic regression** have low variance but high bias when the boundary is nonlinear.\n",
    "  * **QDA** is more flexible but prone to overfitting with small $n$.\n",
    "* **Naive Bayes** excels when independence holds, fails otherwise.\n",
    "* **KNN** adapts to complex structures but requires large $n$ and careful tuning of $K$.\n",
    "* **There is no universally best classifier**—method selection should consider data distribution, dimensionality, and sample size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "### Generalized Linear Models (GLMs)\n",
    "\n",
    "In previous chapters, we explored two main categories of supervised learning problems: **regression**, where the response variable $Y$ is quantitative (e.g., real-valued), and **classification**, where $Y$ is qualitative (e.g., categorical). However, some real-world scenarios involve outcomes that are neither naturally quantitative nor categorical (like count-valued). In such cases, neither standard linear regression nor classification approaches are well-suited.\n",
    "\n",
    "To motivate the need for a more flexible framework, we consider the **Bikeshare** dataset: a real-world application where the response variable $Y$, representing the number of hourly bike users in Washington, D.C., is **count-valued** and thus **non-negative and discrete**.\n",
    "\n",
    "**Inappropriate Use of Linear Regression for Count Data**\n",
    "\n",
    "Let’s begin by applying ordinary least squares (OLS) linear regression to predict the number of bikers. The covariates include:\n",
    "\n",
    "* **mnth** (month of the year),\n",
    "* **hr** (hour of the day),\n",
    "* **workingday** (binary: 1 if not a weekend or holiday),\n",
    "* **temp** (normalized temperature),\n",
    "* **weathersit** (categorical: clear, cloudy/misty, light precipitation, heavy precipitation).\n",
    "\n",
    "For modeling purposes, we treat `mnth`, `hr`, and `weathersit` as **qualitative (factor) variables**, leading to a model of the form:\n",
    "\n",
    "$$\n",
    "\\text{bikers}_i = \\beta_0 + \\sum_{j=1}^{p} \\beta_j x_{ij} + \\varepsilon_i\n",
    "$$\n",
    "\n",
    "**Interpretability of Coefficients**\n",
    "\n",
    "From the linear model fit, we observe intuitive trends:\n",
    "\n",
    "* Cloudy/misty weather leads to 12.89 fewer riders per hour, on average.\n",
    "* Heavy precipitation leads to an estimated decrease of 66.49 bikers/hour.\n",
    "* Spring and fall months show the highest coefficients for ridership.\n",
    "* Peak usage occurs at 9 AM and 6 PM, consistent with commute hours.\n",
    "\n",
    "These results are visualized in Figure 4.13, where month and hour coefficients show seasonal and daily ridership trends.\n",
    "\n",
    "**Violations of Linear Model Assumptions**\n",
    "\n",
    "Despite the interpretability, OLS linear regression violates several key assumptions in this setting:\n",
    "\n",
    "1. **Negative Predictions**:\n",
    "   Approximately 9.6% of predicted values from the linear model are negative, which is not meaningful for a count-valued response.\n",
    "\n",
    "2. **Heteroscedasticity**:\n",
    "   The variance of ridership clearly depends on the mean. For instance:\n",
    "\n",
    "   * During 2 AM snowstorms in winter:\n",
    "     Mean = 5.05 riders, SD = 3.73.\n",
    "   * During 9 AM commutes in spring:\n",
    "     Mean = 243.59 riders, SD = 131.7.\n",
    "\n",
    "   This violates the constant-variance assumption:\n",
    "\n",
    "   $$\n",
    "   \\mathbb{V}(\\varepsilon_i) = \\sigma^2 \\quad \\text{(assumed constant in OLS)}\n",
    "   $$\n",
    "\n",
    "   Figure 4.14 (left) shows the increasing spread of rider counts with the hour of the day.\n",
    "\n",
    "3. **Incompatible Response Distribution**:\n",
    "   OLS assumes a continuous-valued outcome, yet the `bikers` variable is a **non-negative integer**. As a result, the model is misspecified at a fundamental level:\n",
    "\n",
    "   $$\n",
    "   Y = \\beta_0 + \\sum_{j=1}^p \\beta_j x_j + \\varepsilon, \\quad \\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)\n",
    "   $$\n",
    "\n",
    "   But here, $Y \\in \\{0, 1, 2, \\dots\\}$, and so the Gaussian error assumption is inappropriate.\n",
    "\n",
    "**Attempted Fix: Log Transformation**\n",
    "\n",
    "To address negative predictions and heteroscedasticity, one common workaround is to fit a **log-linear model**:\n",
    "\n",
    "$$\n",
    "\\log(Y_i) = \\beta_0 + \\sum_{j=1}^p \\beta_j x_{ij} + \\varepsilon_i\n",
    "$$\n",
    "\n",
    "* This transformation restricts predicted values to be positive.\n",
    "* It also reduces the non-constant variance of the residuals.\n",
    "* Figure 4.14 (right) confirms improved homoscedasticity post-transformation.\n",
    "\n",
    "**But there are limitations**:\n",
    "\n",
    "* The model is now additive in $\\log(Y)$, not $Y$, complicating interpretation.\n",
    "* Predictions must be exponentiated to return to the original scale.\n",
    "* The transformation cannot be applied if $Y = 0$, making it unsuitable for sparse count data.\n",
    "\n",
    "**Why We Need Generalized Linear Models**\n",
    "\n",
    "To address the limitations above, we need a modeling framework that:\n",
    "\n",
    "* Produces **non-negative** predictions,\n",
    "* Allows **heteroscedasticity**, especially a mean-variance relationship,\n",
    "* Supports **discrete** (non-continuous) outcomes,\n",
    "* Retains interpretability via a **linear predictor**.\n",
    "\n",
    "This motivates the introduction of **generalized linear models (GLMs)**. In particular, a **Poisson regression model**, which we'll examine next, offers a more natural and statistically principled approach for modeling count data such as bikers per hour.\n",
    "\n",
    "GLMs generalize linear regression in two key ways:\n",
    "\n",
    "1. They specify the **distribution** of the response variable (e.g., Poisson, Binomial, Gaussian).\n",
    "2. They model the **mean of the response** via a **link function** that connects a linear predictor to the response distribution’s mean.\n",
    "\n",
    "$$\n",
    "g(\\mathbb{E}[Y \\mid \\mathbf{X}]) = \\beta_0 + \\sum_{j=1}^p \\beta_j x_j\n",
    "$$\n",
    "\n",
    "In the Poisson case:\n",
    "\n",
    "* $g(\\mu) = \\log(\\mu)$, i.e., a **log link**,\n",
    "* $Y \\sim \\text{Poisson}(\\mu)$, with $\\mu = \\mathbb{E}[Y]$,\n",
    "* This ensures $\\mu > 0$, avoids negative predictions, and handles heteroscedasticity naturally.\n",
    "\n",
    "**Poisson Regression on the Bikeshare Data**\n",
    "\n",
    "The limitations of applying linear regression to the Bikeshare data—particularly the occurrence of negative predictions and violation of constant variance—suggest the need for a model specifically tailored to count data. The **Poisson regression model** offers a principled and interpretable solution.\n",
    "\n",
    "**The Poisson Distribution**\n",
    "\n",
    "The Poisson distribution models count data, i.e., random variables taking values in the set $\\{0, 1, 2, \\dots\\}$. A random variable $Y$ is said to follow a Poisson distribution with rate parameter $\\lambda > 0$ if:\n",
    "\n",
    "$$\n",
    "\\Pr(Y = k) = \\frac{e^{-\\lambda} \\lambda^k}{k!}, \\quad \\text{for } k = 0, 1, 2, \\dots\n",
    "$$\n",
    "\n",
    "Key properties:\n",
    "\n",
    "* The **mean** of $Y$ is $\\mathbb{E}(Y) = \\lambda$.\n",
    "* The **variance** of $Y$ is also $\\text{Var}(Y) = \\lambda$.\n",
    "\n",
    "Thus, in contrast to linear regression models where the variance is assumed constant, the Poisson model inherently ties the variance to the mean, which is well-suited to overdispersed count data.\n",
    "\n",
    "**Modeling Counts Using Poisson Regression**\n",
    "\n",
    "Suppose $Y$ is the number of users of a bike sharing program in a given hour, under specified conditions (e.g., temperature, time of day, weather). We model $Y$ as following a Poisson distribution with mean $\\lambda = \\mathbb{E}(Y \\mid X_1, \\dots, X_p)$, where $X_1, \\dots, X_p$ are predictor variables such as temperature, hour, and working day.\n",
    "\n",
    "To ensure that the predicted mean $\\lambda$ remains positive regardless of the values of the covariates, we model the **log of the mean** as a linear function:\n",
    "\n",
    "$$\n",
    "\\log \\lambda(X_1, \\dots, X_p) = \\beta_0 + \\beta_1 X_1 + \\dots + \\beta_p X_p\n",
    "\\tag{4.36}\n",
    "$$\n",
    "\n",
    "Equivalently, the mean is expressed as:\n",
    "\n",
    "$$\n",
    "\\lambda(X_1, \\dots, X_p) = \\exp(\\beta_0 + \\beta_1 X_1 + \\dots + \\beta_p X_p)\n",
    "\\tag{4.37}\n",
    "$$\n",
    "\n",
    "This is known as the **Poisson regression model**, a member of the family of generalized linear models (GLMs).\n",
    "\n",
    "**Estimation**\n",
    "\n",
    "As with logistic regression, the parameters $\\beta_0, \\dots, \\beta_p$ are estimated via **maximum likelihood**. Given observations $(x_i, y_i)$, the likelihood is:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\beta) = \\prod_{i=1}^n \\frac{e^{-\\lambda(x_i)} \\lambda(x_i)^{y_i}}{y_i!}, \\quad \\text{where } \\lambda(x_i) = \\exp(\\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_p x_{ip})\n",
    "\\tag{4.38}\n",
    "$$\n",
    "\n",
    "The log-likelihood is then maximized to estimate the coefficients.\n",
    "\n",
    "**Application to Bikeshare Data**\n",
    "\n",
    "The fitted Poisson regression model includes predictors such as:\n",
    "\n",
    "* Month of the year (qualitative)\n",
    "* Hour of the day (qualitative)\n",
    "* Temperature (quantitative)\n",
    "* Working day indicator\n",
    "* Weather conditions (qualitative)\n",
    "\n",
    "**Results (Table 4.11 Summary)**\n",
    "\n",
    "| Predictor                   | Coefficient | z-statistic | p-value |\n",
    "| --------------------------- | ----------- | ----------- | ------- |\n",
    "| Intercept                   | 4.12        | 683.96      | <0.001  |\n",
    "| Working day                 | 0.01        | 68.43       | <0.001  |\n",
    "| Temperature                 | 0.79        | 7.50        | <0.001  |\n",
    "| Cloudy/Misty (vs. clear)    | –0.08       | –5.55       | <0.001  |\n",
    "| Light rain/snow (vs. clear) | –0.58       | –34.53      | <0.001  |\n",
    "| Heavy rain/snow (vs. clear) | –0.93       | –141.91     | <0.001  |\n",
    "\n",
    "These results confirm intuitive relationships: bike usage increases with temperature and decreases with worsening weather. The **working day** effect is significant under the Poisson model, unlike in the linear model.\n",
    "\n",
    "**Interpretability**\n",
    "\n",
    "In Poisson regression, the coefficient $\\beta_j$ implies:\n",
    "\n",
    "$$\n",
    "\\text{Multiplicative change in } \\mathbb{E}(Y) \\text{ for a one-unit increase in } X_j = \\exp(\\beta_j)\n",
    "$$\n",
    "\n",
    "For example:\n",
    "\n",
    "* Transition from clear to cloudy skies ($\\beta = -0.08$) implies:\n",
    "  $\\exp(-0.08) \\approx 0.923$, or a **7.7% decrease** in expected ridership.\n",
    "\n",
    "* From clear to heavy rain ($\\beta = -0.93$) implies:\n",
    "  $\\exp(-0.93) \\approx 0.39$, or a **61% drop** in expected ridership.\n",
    "\n",
    "**Advantages Over Linear Regression**\n",
    "\n",
    "1. **Non-negativity**: Predictions $\\lambda(x)$ are always positive. Linear regression can produce negative predictions for count data, which are nonsensical.\n",
    "\n",
    "2. **Mean-Variance Relationship**: In real-world count data, the variance increases with the mean. The Poisson model naturally accommodates this, unlike linear regression which assumes constant variance (homoscedasticity).\n",
    "\n",
    "3. **Count-Valued Response**: The Poisson model respects the integer nature of the response. Linear regression assumes a continuous response and thus mismatches the data-generating process.\n",
    "\n",
    "**Summary Table: Poisson vs Linear Regression for Count Data**\n",
    "\n",
    "| Feature                        | Linear Regression                    | Poisson Regression                                    |\n",
    "| ------------------------------ | ------------------------------------ | ----------------------------------------------------- |\n",
    "| Response Type                  | Continuous (real-valued)             | Discrete (count-valued)                               |\n",
    "| Domain of Response             | $\\mathbb{R}$                         | $\\mathbb{N}_0 = \\{0, 1, 2, \\dots\\}$                   |\n",
    "| Mean-Variance Assumption       | Constant variance $\\sigma^2$         | $\\text{Var}(Y) = \\mathbb{E}(Y) = \\lambda$             |\n",
    "| Predictive Function            | Linear: $\\beta_0 + \\sum \\beta_j X_j$ | Log-link: $\\log \\lambda = \\beta_0 + \\sum \\beta_j X_j$ |\n",
    "| Prediction Range               | Unbounded (can be negative)          | Positive-only via exponentiation                      |\n",
    "| Interpretation of Coefficients | Additive change in mean              | Multiplicative change in mean via $\\exp(\\beta_j)$     |\n",
    "| Suitable For                   | Gaussian errors, continuous outcomes | Count data with increasing variance                   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalized Linear Models in Greater Generality\n",
    "\n",
    "We have now encountered three fundamental regression models:\n",
    "\n",
    "* **Linear regression** for continuous responses\n",
    "* **Logistic regression** for binary classification\n",
    "* **Poisson regression** for count-valued responses\n",
    "\n",
    "Despite modeling different kinds of outcomes, these approaches share a common structure. This observation leads to the **generalized linear model (GLM)** framework, which unifies them under a single theoretical umbrella.\n",
    "\n",
    "**Common Structure of Linear, Logistic, and Poisson Regression**\n",
    "\n",
    "Let $Y$ denote the response variable and $X_1, \\dots, X_p$ the predictor variables. Each of the three models assumes the following structure:\n",
    "\n",
    "1. **Distributional Assumption for Y**:\n",
    "   The distribution of $Y$, conditional on the predictors, belongs to a known probability distribution:\n",
    "\n",
    "   * Linear regression: $Y \\sim \\mathcal{N}(\\mu, \\sigma^2)$\n",
    "   * Logistic regression: $Y \\sim \\text{Bernoulli}(p)$\n",
    "   * Poisson regression: $Y \\sim \\text{Poisson}(\\lambda)$\n",
    "\n",
    "2. **Modeling the Mean of Y**:\n",
    "   Each model expresses the **mean** of $Y$, denoted $\\mu = \\mathbb{E}[Y \\mid X_1, \\dots, X_p]$, as a function of the predictors.\n",
    "\n",
    "   * **Linear Regression**: The mean is directly modeled as a linear combination of inputs:\n",
    "\n",
    "     $$\n",
    "     \\mathbb{E}[Y \\mid X_1, \\dots, X_p] = \\beta_0 + \\beta_1 X_1 + \\dots + \\beta_p X_p\n",
    "     \\tag{4.39}\n",
    "     $$\n",
    "\n",
    "   * **Logistic Regression**: The probability $\\Pr(Y = 1 \\mid X)$ is modeled using the logistic (sigmoid) function:\n",
    "\n",
    "     $$\n",
    "     \\Pr(Y = 1 \\mid X_1, \\dots, X_p) = \\frac{\\exp(\\beta_0 + \\beta_1 X_1 + \\dots + \\beta_p X_p)}{1 + \\exp(\\beta_0 + \\beta_1 X_1 + \\dots + \\beta_p X_p)}\n",
    "     \\tag{4.40}\n",
    "     $$\n",
    "\n",
    "   * **Poisson Regression**: The log of the mean count is modeled as linear:\n",
    "\n",
    "     $$\n",
    "     \\log \\mathbb{E}[Y \\mid X_1, \\dots, X_p] = \\beta_0 + \\beta_1 X_1 + \\dots + \\beta_p X_p\n",
    "     \\tag{4.41}\n",
    "     $$\n",
    "\n",
    "**The Link Function**\n",
    "\n",
    "To generalize these models, we define a **link function** $\\eta(\\cdot)$ that transforms the mean $\\mu$ so that the transformed mean is a linear function of the inputs:\n",
    "\n",
    "$$\n",
    "\\eta\\left(\\mathbb{E}[Y \\mid X_1, \\dots, X_p] \\right) = \\beta_0 + \\beta_1 X_1 + \\dots + \\beta_p X_p\n",
    "\\tag{4.42}\n",
    "$$\n",
    "\n",
    "Each of the models we’ve seen corresponds to a different link function:\n",
    "\n",
    "| Model               | Distribution | Mean Function                                   | Link Function $\\eta(\\mu)$                     |\n",
    "| ------------------- | ------------ | ----------------------------------------------- | --------------------------------------------- |\n",
    "| Linear Regression   | Gaussian     | $\\mu = \\beta_0 + \\sum \\beta_j X_j$              | Identity: $\\mu$                               |\n",
    "| Logistic Regression | Bernoulli    | $\\mu = \\frac{e^{\\beta^T X}}{1 + e^{\\beta^T X}}$ | Logit: $\\log\\left(\\frac{\\mu}{1 - \\mu}\\right)$ |\n",
    "| Poisson Regression  | Poisson      | $\\mu = e^{\\beta^T X}$                           | Log: $\\log(\\mu)$                              |\n",
    "\n",
    "The **link function** ensures that:\n",
    "\n",
    "* The modeled mean lies in a valid range (e.g. between 0 and 1 for probabilities, or non-negative for counts),\n",
    "* And that the model remains linear in parameters $\\beta$, enabling maximum likelihood estimation.\n",
    "\n",
    "**The Exponential Family**\n",
    "\n",
    "All three models—linear, logistic, and Poisson—assume the response variable $Y$ comes from a member of the **exponential family of distributions**, which have the general form:\n",
    "\n",
    "$$\n",
    "f_Y(y; \\theta) = \\exp\\left\\{ \\frac{y \\cdot \\theta - b(\\theta)}{a(\\phi)} + c(y, \\phi) \\right\\}\n",
    "$$\n",
    "\n",
    "Here:\n",
    "\n",
    "* $\\theta$ is the canonical parameter\n",
    "* $\\phi$ is a dispersion parameter\n",
    "* $a(\\cdot), b(\\cdot), c(\\cdot)$ are known functions that specify the distribution\n",
    "\n",
    "This family includes:\n",
    "\n",
    "* Gaussian\n",
    "* Bernoulli\n",
    "* Poisson\n",
    "* Exponential\n",
    "* Gamma\n",
    "* Negative Binomial (used to handle overdispersion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalized Linear Models (GLMs)\n",
    "\n",
    "A **generalized linear model (GLM)** consists of three components:\n",
    "\n",
    "1. A **distribution** for $Y$ from the exponential family\n",
    "2. A **linear predictor**: $\\eta = \\beta_0 + \\beta_1 X_1 + \\dots + \\beta_p X_p$\n",
    "3. A **link function** $\\eta(\\mu)$ that connects the mean of $Y$ to the linear predictor\n",
    "\n",
    "This general formulation includes:\n",
    "\n",
    "* Linear regression (Gaussian + identity link)\n",
    "* Logistic regression (Bernoulli + logit link)\n",
    "* Poisson regression (Poisson + log link)\n",
    "* And others, such as:\n",
    "\n",
    "  * Gamma regression with inverse or log link\n",
    "  * Negative binomial regression\n",
    "\n",
    "**Summary Table: Generalized Linear Models**\n",
    "\n",
    "| Model Type          | Response Type       | Distribution      | Link Function                  | Domain of Mean (μ) |\n",
    "| ------------------- | ------------------- | ----------------- | ------------------------------ | ------------------ |\n",
    "| Linear Regression   | Continuous          | Gaussian (Normal) | Identity: $\\mu$                | $\\mathbb{R}$       |\n",
    "| Logistic Regression | Binary (0/1)        | Bernoulli         | Logit: $\\log\\frac{\\mu}{1-\\mu}$ | (0, 1)             |\n",
    "| Poisson Regression  | Count (0,1,2,…)     | Poisson           | Log: $\\log \\mu$                | $(0, \\infty)$      |\n",
    "| Gamma Regression    | Continuous (pos.)   | Gamma             | Inverse or log                 | $(0, \\infty)$      |\n",
    "| Neg. Binomial Reg.  | Overdispersed Count | Negative Binomial | Log or sqrt                    | $(0, \\infty)$      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### $\\underline{\\text{Lecture Questions:}}$\n",
    "\n",
    "### Question 1. \n",
    "What is example of a qualitative variable? \n",
    "\n",
    "a) Height\n",
    "\n",
    "b) Age\n",
    "\n",
    "c) Speed\n",
    "\n",
    "d) Color\n",
    "\n",
    "### Answer 1. \n",
    "\n",
    "d)\n",
    "\n",
    "### Question 2. \n",
    "Using $\\hat{p}(X) = \\frac{e^{\\hat{\\beta}_0 + \\hat{\\beta}_1X}}{1 + e^{\\hat{\\beta}_0 + \\hat{\\beta}_1 X}}$ what balance gives default rate of $50\\%$? \n",
    "\n",
    "### Answer 2. \n",
    "\n",
    "We want to find the value of $x$ such that the logistic function equals 0.5 or $50\\%$:\n",
    "\n",
    "$$\n",
    "0.5 = \\frac{e^x}{1 + e^x}\n",
    "$$\n",
    "\n",
    "Multiply both sides by $1 + e^x$\n",
    "\n",
    "$$\n",
    "0.5(1 + e^x) = e^x\n",
    "$$\n",
    "\n",
    "$$\n",
    "0.5 + 0.5e^x = e^x\n",
    "$$\n",
    "\n",
    "subtract $0.5e^x$ from both sides\n",
    "\n",
    "$$\n",
    "0.5 = 0.5e^x\n",
    "$$\n",
    "\n",
    "divide both sides by 0.5\n",
    "\n",
    "$$\n",
    "1 = e^x\n",
    "\\Rightarrow x = \\log(1) = 0\n",
    "$$\n",
    "\n",
    "Since $x = -10.6513 + 0.0055 \\times \\text{Balance}$ then\n",
    "\n",
    "$$\n",
    "-10.6513 + 0.0055 \\times \\text{Balance} = 0\n",
    "\\Rightarrow \\text{Balance} = \\frac{10.6513}{0.0055} \\approx 1936.6\n",
    "$$\n",
    "\n",
    "\n",
    "### Question 3.1 \n",
    "Suppose we collect data for a group of students in a statistics class with variables $X_1 = \\text{hours  studied}$, $X_2 = \\text{undergrad GPA}$, and $Y=\\text{receive an A}$. We fit a logistic regression and produce estimated coefficients $\\hat{\\beta}_0=-6$, $\\hat{\\beta}_1=0.05$, $\\hat{\\beta}_2=1$.\n",
    "\n",
    "Estimate the probability that a student who studies for 40h and has an undergrad GPA of 3.5 gets an A in the class (within 0.01 accuracy).\n",
    "\n",
    "### Answer 3.1\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{\\beta}_0 + \\hat{\\beta}_1X_1 + \\hat{\\beta}_2X_2 &= -6 + 0.05X_1 + X_2 \\\\\n",
    "&=-6 + 0.05(40) + 3.5(1) \\\\\n",
    "&=-0.5\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "and therefore the probability that the student will get an A in the class is \n",
    "\n",
    "$$\\hat{p}(x) = \\frac{e^-0.5}{1 + e^-0.5} = \\frac{0.6065}{1 + 0.6065} = 0.378$$\n",
    "\n",
    "or $38\\%$. \n",
    "\n",
    "### Question 3.2 \n",
    "How many hours would that student need to study to have a 50% chance of getting an A in the class?\n",
    "\n",
    "### Answer 3.2 \n",
    "\n",
    "$$0.5=\\frac{e^x}{1 + e^x} \\Rightarrow 0.5 = 0.5e^x \\Rightarrow x=0$$\n",
    "\n",
    "where $x$ is \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "x &= \\hat{\\beta}_0 + \\hat{\\beta}_1X_1 + \\hat{\\beta}_2X_2 \\\\\n",
    "  &= \\hat{\\beta}_0 + \\hat{\\beta}_1\\text{Hrs} + \\hat{\\beta}_2X_2\\text{Gpa} \\\\\n",
    "  &=-6 + 0.05\\text{Hrs} + 3.5 \\Rightarrow \\text{Hrs} = \\frac{2.5}{0.05} = 50\n",
    "\\end{align*}\n",
    "$$\n",
    "  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## $\\underline{\\text{Facts, Derivations, etc:}}$\n",
    "\n",
    "### $\\text{Generalizing to}\\hspace{0.1cm}\\underset{\\theta}{\\operatorname{arg\\,max}}\\; f(\\theta; x):$\n",
    "\n",
    "$$\\theta^*=\\underset{\\theta}{\\operatorname{arg\\,max}}\\; f(\\theta; x)$$\n",
    "\n",
    "means \"out of all admissable parameter setting, pick one that makes $f$ as large as possible for this fixed dataset.\" The usual calculus recipe is \n",
    "\n",
    "1. *First-order condition (gradient)*: \n",
    "\n",
    "    $$\\nabla_{\\theta}f(x;\\theta) = 0$$\n",
    "\n",
    "    This catches *maxima, minima, and saddle points*. We still need second order information (Hessian) or global arguments (concavity, brute force search) to decide what stationary points are maximizers. \n",
    "\n",
    "2. *Second-order condition (Hessian)*: \n",
    "\n",
    "    Check that the Hessian matrix $H=\\nabla^2_\\theta f$ evaluated at the candidate $\\theta$ is negative definite (all the eigenvalues are negative). That guarantees local concavity and is a local maximum. If $H$ is positive definite you have found a local minimum; if it is indefinite the point is a saddle.  \n",
    "\n",
    "3. *Global check*: \n",
    "    \n",
    "    If $f$ is *concave* in $\\theta$ everywhere (as log-likelihoods often are for exponential families), any local maximum is a global one. When $f$ is not concave, you need additional args - or a numerical search- to ensure your point is the global point (i.e. largest). \n",
    "\n",
    "Examples: \n",
    "\n",
    "1. Scalar parameter, scalar data\n",
    "\n",
    "Suppose a single observation $x$ is known and we choose\n",
    "\n",
    "$$\n",
    "f(x;\\theta)=-(x-\\theta)^2 .\n",
    "$$\n",
    "\n",
    "Only $\\theta$ is variable; $x$ is fixed.\n",
    "\n",
    "*Gradient*\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial\\theta}=2(x-\\theta)\\; .\n",
    "$$\n",
    "\n",
    "*Stationary point*\n",
    "$\\,2(x-\\theta)=0\\;\\Longrightarrow\\;\\theta=x.$\n",
    "\n",
    "*Second derivative*\n",
    "$\\displaystyle\\frac{\\partial^{2}f}{\\partial\\theta^{2}}=-2<0,$ so the point is a local (and, here, global) maximum.\n",
    "\n",
    "*Arg-max statement*\n",
    "\n",
    "$$\n",
    "\\boxed{\\theta^{*}=\\arg\\max_{\\theta} f(x;\\theta)=x.}\n",
    "$$\n",
    "\n",
    "2. Scalar parameter with a different loss\n",
    "\n",
    "Keep $x$ fixed but change the function to illustrate how a sign flip turns a maximum into a minimum:\n",
    "\n",
    "$$\n",
    "g(x;\\theta)=(x-\\theta)^2 .\n",
    "$$\n",
    "\n",
    "*Gradient*\n",
    "$\\displaystyle\\frac{\\partial g}{\\partial\\theta}=-2(x-\\theta)$.\n",
    "\n",
    "*Stationary point* $-2(x-\\theta)=0\\Rightarrow\\theta=x$ again.\n",
    "\n",
    "*Second derivative* $\\displaystyle\\frac{\\partial^{2}g}{\\partial\\theta^{2}}=2>0$; curvature upward → **minimum**.\n",
    "\n",
    "Hence\n",
    "\n",
    "$$\n",
    "\\theta^{*}=\\arg\\min_{\\theta}g(x;\\theta)=x,\n",
    "\\quad\\text{but}\\quad\n",
    "\\arg\\max_{\\theta}g(x;\\theta)\\ \\text{does not lie at }x.\n",
    "$$\n",
    "\n",
    "3. Scalar parameter from a likelihood\n",
    "\n",
    "Imagine $x\\sim\\mathrm{Binomial}(n,\\theta)$ with observed success count $x$.  The log-likelihood is\n",
    "\n",
    "$$\n",
    "\\ell(\\theta)=x\\log\\theta+(n-x)\\log(1-\\theta),\\qquad 0<\\theta<1.\n",
    "$$\n",
    "\n",
    "*Gradient*\n",
    "\n",
    "$$\n",
    "\\frac{\\partial\\ell}{\\partial\\theta}= \\frac{x}{\\theta}-\\frac{n-x}{1-\\theta}.\n",
    "$$\n",
    "\n",
    "*Stationary point*\n",
    "\n",
    "$$\n",
    "\\theta\\bigl(n-x\\bigr)=x(1-\\theta)\n",
    "\\;\\Longrightarrow\\;\n",
    "\\theta=\\frac{x}{n},\n",
    "$$\n",
    "\n",
    "the familiar maximum-likelihood estimate.\n",
    "\n",
    "*Second derivative*\n",
    "$\\displaystyle\\frac{\\partial^{2}\\ell}{\\partial\\theta^{2}} =-\\frac{x}{\\theta^{2}}-\\frac{n-x}{(1-\\theta)^{2}}<0,$\n",
    "so the point is a strict local (hence global) maximum inside $(0,1)$ because \n",
    "$$-\\frac{x}{\\theta^2} < 0\n",
    "\\quad\\text{and}\\quad\n",
    "-\\frac{n - x}{(1-\\theta)^2} < 0\n",
    "\\;\\Longrightarrow\\;\n",
    "\\frac{\\partial^2 \\ell}{\\partial \\theta^2} < 0.\n",
    "$$\n",
    "\n",
    "Thus\n",
    "\n",
    "$$\n",
    "\\boxed{\\hat\\theta=\\arg\\max_{0<\\theta<1}\\ell(\\theta)=x/n.}\n",
    "$$\n",
    "\n",
    "4. Vector parameter, quadratic objective\n",
    "\n",
    "Let $\\theta\\in\\mathbb{R}^{d}$ and choose a **concave** quadratic\n",
    "\n",
    "$$\n",
    "f(\\theta)= -\\bigl(\\theta-a\\bigr)^{\\!\\top}Q\\bigl(\\theta-a\\bigr)+c,\n",
    "$$\n",
    "\n",
    "where $Q\\succ 0$ (positive-definite) and $a,c$ are fixed.\n",
    "\n",
    "*Gradient*\n",
    "\n",
    "$$\n",
    "\\nabla_{\\!\\theta}f = -2Q(\\theta-a).\n",
    "$$\n",
    "\n",
    "*Stationary point* $\\;Q(\\theta-a)=0\\Rightarrow\\theta=a.$\n",
    "\n",
    "*Hessian* $\\nabla^{2}_{\\theta}f=-2Q,$ which is **negative-definite** because $Q$ is positive-definite.\n",
    "Therefore $f$ is globally concave and the stationary point is the unique global maximum.\n",
    "\n",
    "Hence\n",
    "\n",
    "$$\n",
    "\\boxed{\\theta^{*}=\\arg\\max_{\\theta\\in\\mathbb{R}^{d}} f(\\theta)=a.}\n",
    "$$\n",
    "\n",
    "*Side note*: replacing the leading minus sign by a plus would make the Hessian $+2Q\\succ 0$; the same algebra would then identify $a$ as the unique global **minimum** instead.\n",
    "\n",
    "\n",
    "5. Discrete vector arg-max (softmax again)\n",
    "\n",
    "Let $u\\in\\mathbb{R}^{K}$ be any score vector and define\n",
    "\n",
    "$$\n",
    "p_k=\\frac{e^{u_k}}{\\sum_{j=1}^{K}e^{u_j}},\\qquad\n",
    "f(k)=\\log p_k.\n",
    "$$\n",
    "\n",
    "Using $Z=\\sum_j e^{u_j}$ gives\n",
    "$f(k)=u_k-\\log Z.$  Because $\\log Z$ is constant in $k$, $f(k)$ is maximised by the largest $u_k$.  Symbolically\n",
    "\n",
    "$$\n",
    "\\boxed{k^{*}=\\arg\\max_{k}f(k)=\\arg\\max_{k}u_k.}\n",
    "$$\n",
    "\n",
    "Here no calculus is needed: the arg-max over a discrete set is obtained by inspection.\n",
    "\n",
    "**Arg-max notation** specifies the *answer*; the gradient/Hessian route is the *mechanism* for finding it when the domain is continuous.\n",
    "* The first-order condition $\\nabla_{\\!\\theta}f=0$ is **necessary** for interior optima; curvature information tells you whether the point is a maximum, minimum, or saddle.\n",
    "* In strictly concave problems (Examples 1, 3, 4) the combination “gradient = 0 + negative-definite Hessian” guarantees the global arg-max.  Without concavity you may need global search or other arguments.\n",
    "* Switching the sign of a concave objective immediately converts a maximisation into a minimisation—algebra identical, conclusion reversed.\n",
    "\n",
    "### $\\text{Density Function}$: \n",
    "Generative models for classification estimate probabilities by modeling the distribution of the predictors $X$ separately in each of the response classes (for each class label, $Y$). Then we can use Bayes' theorem to flip these around into estimates for $Pr(Y=k|X=x)$. The density function is $f_k(X) = Pr(X|Y=k)$ for an observation that comes from the *k*th class. Therefore, $f_k(x)$ is large relative to the other classes if there is a high probability an observation in the *k*th class has $X\\approx x$. For example suppose $K=2$ and\n",
    "\n",
    "$$\n",
    "  f_1(x) = \\frac{1}{\\sqrt{2\\pi}} e^{-\\tfrac12(x-1)^2},\n",
    "  \\quad\n",
    "  f_2(x) = \\frac{1}{\\sqrt{2\\pi}} e^{-\\tfrac12(x+1)^2}.\n",
    "$$\n",
    "\n",
    "where $f_1$ is a normal density centered at $+1$, $f_2$ at $-1$. At $x=1$, $f_1(1)\\approx 0.40$ but $f_2(1)\\approx 0.05$.  Thus if you observe $X=1$, it’s far more **likely** to have come from class 1 than class 2.\n",
    "\n",
    "In summary:\n",
    "\n",
    "$$\n",
    "  f_k(x)\\;=\\;p_{X\\mid Y}(x\\mid k)\n",
    "  \\quad\\Longrightarrow\\quad\n",
    "  \\text{“the height of the class‑\\(k\\) density at \\(x\\).”}\n",
    "$$\n",
    "\n",
    "Large $f_k(x)$ = high plausibility of $x$ under class $k$.\n",
    "\n",
    "### $\\text{Binomial Distribution}$:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
