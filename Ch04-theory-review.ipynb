{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For many situations, the response variable is *qualitative*. Often, qualitative variables are referred to as *categorical*. The process for predicting a qualitative response is called **classification**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\underline{\\text{Overview of Classification}}:$\n",
    "\n",
    "Like regression, in the classification setting we have a set of training observations $(x_1, y_1),\\dots, (x_n, y_n)$ that we can use to build a classifier. In this chapter, we learn how to build a model to predict *default* $Y$ for any given value of *balance* $X_1$ and *income* $X_2$.\n",
    "\n",
    "\n",
    "**Why not linear regression?**\n",
    "\n",
    "1. A regression method cannot accomodate a qualitative response with more than two classes. \n",
    "\n",
    "*Example* If we encode three diagnoses: $\\text{stroke}, \\text{drug overdose}, \\text{epileptic seizure}$ response variable as $Y$\n",
    "\n",
    "$$\n",
    "Y =     \n",
    "\\begin{cases}\n",
    "1 & \\text{if stroke;} \\\\\\\\\n",
    "2 & \\text{if drug overdose;} \\\\\\\\\n",
    "3 & \\text{if epileptic seizure;} \\\\\\\\\n",
    "\\end{cases}\n",
    "\\tag{1}\n",
    "$$\n",
    "\n",
    "we can use least squares to fit a linear model. However, this implies a false ordering and equal spacing between categories, which may not reflect reality. Linear regression isn’t appropriate when the response is qualitative with no natural numeric structure.\n",
    "\n",
    "2. A regression method will not provide meaningful estimates of $Pr(Y|X)$, even with just two classes. \n",
    "\n",
    "*Example* For a binary response, e.g.,\n",
    "\n",
    "$$\n",
    "Y =\n",
    "\\begin{cases}\n",
    "0 & \\text{if stroke} \\\\\n",
    "1 & \\text{if drug overdose}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "we can fit a linear regression and predict drug overdose if $\\hat{Y} > 0.5$, stroke otherwise. This gives crude probability estimates, though some may fall outside $[0, 1]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\underline{\\text{Logistic Regression:}}$\n",
    "\n",
    "Logistic regression models the **probability** that $Y$ belongs to a paticular category. For the **Default** data, logistic regression models the probability of default given a **balance**\n",
    "\n",
    "$$Pr(default=Yes|balance).$$\n",
    "\n",
    "which can be abbreviated as $p(balance)$, which will range between 0 and 1. For example, any individual for whom $p(balance) > 0.5$ might predict a $default=Yes$. A company could have a more conservative approach and use a lower threshold like $p(balance) > 0.1$.\n",
    "\n",
    "**The Logistic Model**\n",
    "\n",
    "Using linear regression to model $p(X) = \\Pr(Y = 1 \\mid X)$ with\n",
    "\n",
    "$$\n",
    "p(X) = \\beta_0 + \\beta_1 X\n",
    "$$\n",
    "\n",
    "can lead to invalid probability estimates—values below 0 or above 1—especially when $X$ has a wide range. To fix this, we use a function that maps any input to $[0, 1]$, such as the logistic function:\n",
    "\n",
    "$$\n",
    "p(X) = \\frac{e^{\\beta_0 + \\beta_1 X}}{1 + e^{\\beta_0 + \\beta_1 X}}.\n",
    "$$\n",
    "\n",
    "This is the basis of logistic regression. To fit the model, we use *maximum likelihood*, ensuring predicted probabilities stay within $[0, 1]$. The logistic function\n",
    "\n",
    "$$\n",
    "p(X) = \\frac{e^{\\beta_0 + \\beta_1 X}}{1 + e^{\\beta_0 + \\beta_1 X}}\n",
    "$$\n",
    "\n",
    "yields an S-shaped curve—low $X$ leads to probabilities near 0, high $X$ near 1—unlike linear regression, which can produce invalid probabilities.\n",
    "\n",
    "We can rewrite the model as\n",
    "\n",
    "$$\n",
    "\\frac{p(X)}{1 - p(X)} = e^{\\beta_0 + \\beta_1 X}\n",
    "$$\n",
    "\n",
    "where the left-hand side is the **odds**, and its log gives the **logit**:\n",
    "\n",
    "$$\n",
    "\\log\\left( \\frac{p(X)}{1 - p(X)} \\right) = \\beta_0 + \\beta_1 X\n",
    "$$\n",
    "\n",
    "Here, $\\beta_1$ reflects how the **log-odds** change with $X$, and multiplying the odds by $e^{\\beta_1}$ gives the effect of a one-unit increase in $X$. Unlike linear regression, the effect of $X$ on $p(X)$ depends on it's current value,  making the probability response nonlinear. If $\\beta_1 > 0$, increasing $X$ increases $p(X)$; if $\\beta_1 < 0$, increasing $X$ decreases $p(X)$.\n",
    "\n",
    "**Estimating the Regression Coefficients**\n",
    "\n",
    "In logistic regression, the coefficients $\\beta_0$ and $\\beta_1$ are unknown and estimated from training data. Instead of least squares (used in linear regression), we use **maximum likelihood**, which has better statistical properties.\n",
    "\n",
    "The idea is to choose $\\hat{\\beta}_0$, $\\hat{\\beta}_1$ so that the predicted probability $\\hat{p}(x_i)$ is close to 1 for individuals who defaulted and close to 0 for those who didn’t. This is done by maximizing the **likelihood function**:\n",
    "\n",
    "$$\n",
    "\\ell(\\beta_0, \\beta_1) = \\prod_{i: y_i = 1} p(x_i) \\prod_{i': y_{i'} = 0} (1 - p(x_{i'}))\n",
    "$$\n",
    "\n",
    "In practice, software estimates these automatically.\n",
    "\n",
    "For example, if $\\hat{\\beta}_1 = 0.0055$, then a one-unit increase in balance raises the **log-odds** of default by 0.0055. The **z-statistic** tests if this effect is statistically significant, similar to the t-statistic in linear regression. A small **p-value** indicates we reject the null $H_0: \\beta_1 = 0$, meaning balance is associated with default risk.\n",
    "\n",
    "The intercept $\\hat{\\beta}_0$ is usually not of direct interest—it adjusts the overall probability to match the proportion of defaults in the data.\n",
    "\n",
    "**Making Predictions**\n",
    "\n",
    "Once the coefficients are estimated, we can compute predicted probabilities using the logistic model:\n",
    "\n",
    "$$\n",
    "\\hat{p}(X) = \\frac{e^{\\hat{\\beta}_0 + \\hat{\\beta}_1 X}}{1 + e^{\\hat{\\beta}_0 + \\hat{\\beta}_1 X}}\n",
    "$$\n",
    "\n",
    "From **Table 4.1**, for `balance` as the predictor:\n",
    "\n",
    "* $\\hat{\\beta}_0 = -10.6513$\n",
    "* $\\hat{\\beta}_1 = 0.0055$\n",
    "\n",
    "Then:\n",
    "\n",
    "* For a balance of \\$1,000:\n",
    "\n",
    "$$\n",
    "\\hat{p}(X = 1000) = \\frac{e^{-10.6513 + 0.0055 \\cdot 1000}}{1 + e^{-10.6513 + 0.0055 \\cdot 1000}} = \\frac{e^{-5.1513}}{1 + e^{-5.1513}} \\approx 0.00576\n",
    "$$\n",
    "\n",
    "* For a balance of \\$2,000:\n",
    "\n",
    "$$\n",
    "\\hat{p}(X = 2000) = \\frac{e^{-10.6513 + 0.0055 \\cdot 2000}}{1 + e^{-10.6513 + 0.0055 \\cdot 2000}} = \\frac{e^{0.3487}}{1 + e^{0.3487}} \\approx 0.586\n",
    "$$\n",
    "\n",
    "**Table 4.1: Logistic Regression Predicting Default from Balance**\n",
    "\n",
    "| Coefficient | Std. Error | z-Statistic | p-Value |         |\n",
    "| ----------- | ---------- | ----------- | ------- | ------- |\n",
    "| Intercept   | -10.6513   | 0.3612      | -29.5   | <0.0001 |\n",
    "| Balance     | 0.0055     | 0.0002      | 24.9    | <0.0001 |\n",
    "\n",
    "Logistic regression also handles qualitative predictors using **dummy variables**. For example, for the qualitative variable `student` (1 = student, 0 = non-student), the estimated coefficients from **Table 4.2** are:\n",
    "\n",
    "* $\\hat{\\beta}_0 = -3.5041$\n",
    "* $\\hat{\\beta}_1 = 0.4049$\n",
    "\n",
    "Then:\n",
    "\n",
    "* For a student:\n",
    "\n",
    "$$\n",
    "\\hat{p}(\\text{default} = \\text{Yes} \\mid \\text{student} = 1) = \\frac{e^{-3.5041 + 0.4049 \\cdot 1}}{1 + e^{-3.5041 + 0.4049 \\cdot 1}} = \\frac{e^{-3.0992}}{1 + e^{-3.0992}} \\approx 0.0431\n",
    "$$\n",
    "\n",
    "* For a non-student:\n",
    "\n",
    "$$\n",
    "\\hat{p}(\\text{default} = \\text{Yes} \\mid \\text{student} = 0) = \\frac{e^{-3.5041 + 0.4049 \\cdot 0}}{1 + e^{-3.5041 + 0.4049 \\cdot 0}} = \\frac{e^{-3.5041}}{1 + e^{-3.5041}} \\approx 0.0292\n",
    "$$\n",
    "\n",
    "The positive coefficient indicates that students are predicted to have a higher probability of default than non-students.\n",
    "\n",
    "**Table 4.2: Logistic Regression Predicting Default from Student Status**\n",
    "\n",
    "| Coefficient    | Std. Error | z-Statistic | p-Value |         |\n",
    "| -------------- | ---------- | ----------- | ------- | ------- |\n",
    "| Intercept      | -3.5041    | 0.0707      | -49.55  | <0.0001 |\n",
    "| Student \\[Yes] | 0.4049     | 0.1150      | 3.52    | 0.0004  |\n",
    "\n",
    "**Multiple Regression**\n",
    "\n",
    "To predict a binary response using multiple predictors, we generalize the logistic regression model as:\n",
    "\n",
    "$$\n",
    "\\log \\left( \\frac{p(X)}{1 - p(X)} \\right) = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p\n",
    "$$\n",
    "\n",
    "or equivalently,\n",
    "\n",
    "$$\n",
    "\\hat{p}(X) = \\frac{e^{\\hat{\\beta}_0 + \\hat{\\beta}_1 X_1 + \\cdots + \\hat{\\beta}_p X_p}}{1 + e^{\\hat{\\beta}_0 + \\hat{\\beta}_1 X_1 + \\cdots + \\hat{\\beta}_p X_p}}\n",
    "$$\n",
    "\n",
    "**Table 4.3: Logistic Regression Predicting Default from Multiple Predictors**\n",
    "\n",
    "| Coefficient    | Std. Error | z-Statistic | p-Value |         |\n",
    "| -------------- | ---------- | ----------- | ------- | ------- |\n",
    "| Intercept      | -10.8690   | 0.4923      | -22.08  | <0.0001 |\n",
    "| Balance        | 0.0057     | 0.0002      | 24.74   | <0.0001 |\n",
    "| Income         | 0.0030     | 0.0082      | 0.37    | 0.7115  |\n",
    "| Student \\[Yes] | -0.6468    | 0.2362      | -2.74   | 0.0062  |\n",
    "\n",
    "As before, we use **maximum likelihood** to estimate the coefficients. From the table:\n",
    "\n",
    "* Balance is positively and significantly associated with default.\n",
    "* Income is not statistically significant.\n",
    "* Student status has a **negative coefficient**, in contrast to the positive coefficient in the earlier single-variable model (Table 4.2).\n",
    "\n",
    "This reversal occurs due to **confounding variable**: student status and balance are correlated—students tend to carry higher balances, which are strongly associated with higher default rates. So although students have higher **overall** default rates, for the **same balance and income**, students are actually **less likely** to default than non-students.\n",
    "\n",
    "**Predictions from the Multiple Logistic Regression Model**\n",
    "\n",
    "Using the estimated coefficients from Table 4.3, we can compute predicted probabilities:\n",
    "\n",
    "* For a **student** with balance = \\$1,500 and income = \\$40,000:\n",
    "\n",
    "$$\n",
    "\\hat{p}(\\text{default} = \\text{Yes} \\mid \\text{student} = 1, \\text{balance} = 1500, \\text{income} = 40) =\n",
    "\\frac{e^{-10.869 + 0.0057 \\cdot 1500 + 0.0030 \\cdot 40 - 0.6468 \\cdot 1}}{1 + e^{-10.869 + 0.0057 \\cdot 1500 + 0.0030 \\cdot 40 - 0.6468 \\cdot 1}} \\approx 0.058\n",
    "$$\n",
    "\n",
    "* For a **non-student** with the same balance and income:\n",
    "\n",
    "$$\n",
    "\\hat{p}(\\text{default} = \\text{Yes} \\mid \\text{student} = 0, \\text{balance} = 1500, \\text{income} = 40) =\n",
    "\\frac{e^{-10.869 + 0.0057 \\cdot 1500 + 0.0030 \\cdot 40 - 0.6468 \\cdot 0}}{1 + e^{-10.869 + 0.0057 \\cdot 1500 + 0.0030 \\cdot 40 - 0.6468 \\cdot 0}} \\approx 0.105\n",
    "$$\n",
    "\n",
    "Note: Income is measured in **thousands of dollars**, so we use 40 instead of 40,000 in the calculation.\n",
    "\n",
    "**Discussion of confounding**\n",
    "\n",
    "The **overall default rate** refers to the empirical proportion of individuals in the dataset who defaulted, regardless of predictor values.\n",
    "\n",
    "Mathematically, if you have $n$ observations and $y_i \\in \\{0, 1\\}$ indicates whether person $i$ defaulted:\n",
    "\n",
    "$$\n",
    "\\text{Overall default rate} = \\frac{1}{n} \\sum_{i=1}^n y_i\n",
    "$$\n",
    "\n",
    "This is **not** a model-based prediction—it's simply the average value of the binary response across the data. For example, if 3.3% of people in the data defaulted, the overall default rate is 0.033.\n",
    "\n",
    "In plots like **Figure 4.3**, the horizontal dashed lines show this empirical rate **within subgroups**, e.g., students vs. non-students, averaged across all values of balance and income.\n",
    "\n",
    "How can students be less likely to default (in the model), yet have a higher overall default rate?\n",
    "\n",
    "This is the classic signature of **confounding**.\n",
    "\n",
    "* In the **single-variable logistic regression** (Table 4.2), students appear **more likely** to default. That’s because:\n",
    "\n",
    "  * Students tend to have **higher balances** (right panel of Figure 4.3).\n",
    "  * Higher balances are strongly associated with higher default risk.\n",
    "  * So, without adjusting for balance, it *looks* like students are riskier.\n",
    "\n",
    "* In the **multiple logistic regression** (Table 4.3), after controlling for **balance** and **income**, the **coefficient for student becomes negative**:\n",
    "\n",
    "  * This means that **for two people with the same balance and income**, the student is actually **less likely** to default.\n",
    "  * So, once we hold balance constant, the true effect of student status is revealed.\n",
    "\n",
    "* The multiple logistic regression says:\n",
    "\n",
    "  $$\n",
    "  \\log \\left( \\frac{\\Pr(\\text{default} = 1 \\mid X)}{1 - \\Pr(\\text{default} = 1 \\mid X)} \\right) = \\beta_0 + \\beta_1 \\cdot \\text{balance} + \\beta_2 \\cdot \\text{income} + \\beta_3 \\cdot \\text{student}\n",
    "  $$\n",
    "\n",
    "  where $\\hat{\\beta}_3 = -0.6468$ implies students are less likely to default **when balance and income are the same**.\n",
    "\n",
    "The higher **overall** student default rate is due to **students having higher average balances**, not because being a student inherently increases default risk.\n",
    "\n",
    "**Student status is a confounding variable**. It’s associated with higher balance, which increases default risk. Once you control for balance, being a student is actually associated with **lower** default probability.\n",
    "\n",
    "Here’s a concise and clear version for **interview review**, retaining all key ideas and equations from the section on **multinomial logistic regression**:\n",
    "\n",
    "**Multinomial Logistic Regression (for $K > 2$ classes)**\n",
    "\n",
    "Logistic regression can be extended to handle **multi-class classification**—this is called **multinomial logistic regression**.\n",
    "\n",
    "Suppose $Y \\in \\{1, 2, ..., K\\}$ is a response with **K classes**, and we choose one class (say class $K$) as the **baseline**.\n",
    "\n",
    "For $k = 1, \\dots, K-1$:\n",
    "\n",
    "$$\n",
    "\\Pr(Y = k \\mid X = x) = \\frac{e^{\\beta_{k0} + \\beta_{k1}x_1 + \\cdots + \\beta_{kp}x_p}}{1 + \\sum_{l=1}^{K-1} e^{\\beta_{l0} + \\beta_{l1}x_1 + \\cdots + \\beta_{lp}x_p}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Pr(Y = K \\mid X = x) = \\frac{1}{1 + \\sum_{l=1}^{K-1} e^{\\beta_{l0} + \\beta_{l1}x_1 + \\cdots + \\beta_{lp}x_p}}\n",
    "$$\n",
    "\n",
    "And the **log-odds** for class $k$ vs. the baseline class $K$ is:\n",
    "\n",
    "$$\n",
    "\\log \\left( \\frac{\\Pr(Y = k \\mid X = x)}{\\Pr(Y = K \\mid X = x)} \\right) = \\beta_{k0} + \\beta_{k1}x_1 + \\cdots + \\beta_{kp}x_p\n",
    "$$\n",
    "\n",
    "This generalizes binary logistic regression: the log-odds between any class and the baseline is linear in the predictors.\n",
    "\n",
    "* The choice of **baseline class** doesn’t affect predicted probabilities or log-odds between any pair of classes, but it **does affect coefficient interpretation**.\n",
    "* For example, if **epileptic seizure** is the baseline, then $\\beta_{\\text{stroke}, j}$ is the change in log-odds of stroke vs. epileptic seizure for a one-unit increase in $x_j$.\n",
    "* The **odds ratio** increases by $e^{\\beta_{\\text{stroke}, j}}$ per unit increase in $x_j$.\n",
    "\n",
    "**Softmax Coding**\n",
    "\n",
    "An alternative formulation used widely in **machine learning** is **softmax coding**, which treats all classes symmetrically:\n",
    "\n",
    "$$\n",
    "\\Pr(Y = k \\mid X = x) = \\frac{e^{\\beta_{k0} + \\beta_{k1}x_1 + \\cdots + \\beta_{kp}x_p}}{\\sum_{l=1}^K e^{\\beta_{l0} + \\beta_{l1}x_1 + \\cdots + \\beta_{lp}x_p}} \\quad \\text{for } k = 1, \\dots, K\n",
    "$$\n",
    "\n",
    "In this version\n",
    "\n",
    "* Coefficients are estimated for **all K classes**.\n",
    "* The **log-odds between any two classes** $k$ and $k'$ is:\n",
    "\n",
    "$$\n",
    "\\log \\left( \\frac{\\Pr(Y = k \\mid X = x)}{\\Pr(Y = k' \\mid X = x)} \\right)\n",
    "= (\\beta_{k0} - \\beta_{k'0}) + (\\beta_{k1} - \\beta_{k'1})x_1 + \\cdots + (\\beta_{kp} - \\beta_{k'p})x_p\n",
    "$$\n",
    "\n",
    "Softmax coding produces the **same predicted probabilities and log-odds** as the baseline coding—it’s just a different parameterization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
