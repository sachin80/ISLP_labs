{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For many situations, the response variable is *qualitative*. Often, qualitative variables are referred to as *categorical*. The process for predicting a qualitative response is called **classification**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\underline{\\text{Overview of Classification}}:$\n",
    "\n",
    "Like regression, in the classification setting we have a set of training observations $(x_1, y_1),\\dots, (x_n, y_n)$ that we can use to build a classifier. In this chapter, we learn how to build a model to predict *default* $Y$ for any given value of *balance* $X_1$ and *income* $X_2$.\n",
    "\n",
    "\n",
    "**Why not linear regression?**\n",
    "\n",
    "1. A regression method cannot accomodate a qualitative response with more than two classes. \n",
    "\n",
    "*Example* If we encode three diagnoses: $\\text{stroke}, \\text{drug overdose}, \\text{epileptic seizure}$ response variable as $Y$\n",
    "\n",
    "$$\n",
    "Y =     \n",
    "\\begin{cases}\n",
    "1 & \\text{if stroke;} \\\\\\\\\n",
    "2 & \\text{if drug overdose;} \\\\\\\\\n",
    "3 & \\text{if epileptic seizure;} \\\\\\\\\n",
    "\\end{cases}\n",
    "\\tag{1}\n",
    "$$\n",
    "\n",
    "we can use least squares to fit a linear model. However, this implies a false ordering and equal spacing between categories, which may not reflect reality. Linear regression isn’t appropriate when the response is qualitative with no natural numeric structure.\n",
    "\n",
    "2. A regression method will not provide meaningful estimates of $Pr(Y|X)$, even with just two classes. \n",
    "\n",
    "*Example* For a binary response, e.g.,\n",
    "\n",
    "$$\n",
    "Y =\n",
    "\\begin{cases}\n",
    "0 & \\text{if stroke} \\\\\n",
    "1 & \\text{if drug overdose}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "we can fit a linear regression and predict drug overdose if $\\hat{Y} > 0.5$, stroke otherwise. This gives crude probability estimates, though some may fall outside $[0, 1]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\underline{\\text{Logistic Regression:}}$\n",
    "\n",
    "Logistic regression models the **probability** that $Y$ belongs to a paticular category. For the **Default** data, logistic regression models the probability of default given a **balance**\n",
    "\n",
    "$$Pr(\\text{default=Yes|balance}).$$\n",
    "\n",
    "which can be abbreviated as $p(\\text{balance})$, which will range between 0 and 1. For example, any individual for whom $p(\\text{balance}) > 0.5$ might predict a $default=Yes$. A company could have a more conservative approach and use a lower threshold like $p(\\text{balance}) > 0.1$.\n",
    "\n",
    "**The Logistic Model**\n",
    "\n",
    "Using linear regression to model $p(X) = \\Pr(Y = 1 \\mid X)$ with\n",
    "\n",
    "$$\n",
    "p(X) = \\beta_0 + \\beta_1 X\n",
    "$$\n",
    "\n",
    "can lead to invalid probability estimates—values below 0 or above 1—especially when $X$ has a wide range. To fix this, we use a function that maps any input to $[0, 1]$, such as the logistic function:\n",
    "\n",
    "$$\n",
    "p(X) = \\frac{e^{\\beta_0 + \\beta_1 X}}{1 + e^{\\beta_0 + \\beta_1 X}}.\n",
    "$$\n",
    "\n",
    "This is the basis of logistic regression. To fit the model, we use *maximum likelihood*, ensuring predicted probabilities stay within $[0, 1]$. The logistic function\n",
    "\n",
    "$$\n",
    "p(X) = \\frac{e^{\\beta_0 + \\beta_1 X}}{1 + e^{\\beta_0 + \\beta_1 X}}\n",
    "$$\n",
    "\n",
    "yields an S-shaped curve—low $X$ leads to probabilities near 0, high $X$ near 1—unlike linear regression, which can produce invalid probabilities.\n",
    "\n",
    "We can rewrite the model as\n",
    "\n",
    "$$\n",
    "\\frac{p(X)}{1 - p(X)} = e^{\\beta_0 + \\beta_1 X}\n",
    "$$\n",
    "\n",
    "where the left-hand side is the **odds**, and its log gives the **logit**:\n",
    "\n",
    "$$\n",
    "\\log\\left( \\frac{p(X)}{1 - p(X)} \\right) = \\beta_0 + \\beta_1 X\n",
    "$$\n",
    "\n",
    "Here, $\\beta_1$ reflects how the **log-odds** change with $X$, and multiplying the odds by $e^{\\beta_1}$ gives the effect of a one-unit increase in $X$. Unlike linear regression, the effect of $X$ on $p(X)$ depends on it's current value,  making the probability response nonlinear. If $\\beta_1 > 0$, increasing $X$ increases $p(X)$; if $\\beta_1 < 0$, increasing $X$ decreases $p(X)$.\n",
    "\n",
    "**Estimating the Regression Coefficients**\n",
    "\n",
    "In logistic regression, the coefficients $\\beta_0$ and $\\beta_1$ are unknown and estimated from training data. Instead of least squares (used in linear regression), we use **maximum likelihood**, which has better statistical properties.\n",
    "\n",
    "The idea is to choose $\\hat{\\beta}_0$, $\\hat{\\beta}_1$ so that the predicted probability $\\hat{p}(x_i)$ is close to 1 for individuals who defaulted and close to 0 for those who didn’t. This is done by maximizing the **likelihood function**:\n",
    "\n",
    "$$\n",
    "\\ell(\\beta_0, \\beta_1) = \\prod_{i: y_i = 1} p(x_i) \\prod_{i': y_{i'} = 0} (1 - p(x_{i'}))\n",
    "$$\n",
    "\n",
    "In practice, software estimates these automatically.\n",
    "\n",
    "For example, if $\\hat{\\beta}_1 = 0.0055$, then a one-unit increase in balance raises the **log-odds** of default by 0.0055. The **z-statistic** tests if this effect is statistically significant, similar to the t-statistic in linear regression. A small **p-value** indicates we reject the null $H_0: \\beta_1 = 0$, meaning balance is associated with default risk.\n",
    "\n",
    "The intercept $\\hat{\\beta}_0$ is usually not of direct interest—it adjusts the overall probability to match the proportion of defaults in the data.\n",
    "\n",
    "**Making Predictions**\n",
    "\n",
    "Once the coefficients are estimated, we can compute predicted probabilities using the logistic model:\n",
    "\n",
    "$$\n",
    "\\hat{p}(X) = \\frac{e^{\\hat{\\beta}_0 + \\hat{\\beta}_1 X}}{1 + e^{\\hat{\\beta}_0 + \\hat{\\beta}_1 X}}\n",
    "$$\n",
    "\n",
    "From **Table 4.1**, for `balance` as the predictor:\n",
    "\n",
    "* $\\hat{\\beta}_0 = -10.6513$\n",
    "* $\\hat{\\beta}_1 = 0.0055$\n",
    "\n",
    "Then:\n",
    "\n",
    "* For a balance of \\$1,000:\n",
    "\n",
    "$$\n",
    "\\hat{p}(X = 1000) = \\frac{e^{-10.6513 + 0.0055 \\cdot 1000}}{1 + e^{-10.6513 + 0.0055 \\cdot 1000}} = \\frac{e^{-5.1513}}{1 + e^{-5.1513}} \\approx 0.00576\n",
    "$$\n",
    "\n",
    "* For a balance of \\$2,000:\n",
    "\n",
    "$$\n",
    "\\hat{p}(X = 2000) = \\frac{e^{-10.6513 + 0.0055 \\cdot 2000}}{1 + e^{-10.6513 + 0.0055 \\cdot 2000}} = \\frac{e^{0.3487}}{1 + e^{0.3487}} \\approx 0.586\n",
    "$$\n",
    "\n",
    "**Table 4.1: Logistic Regression Predicting Default from Balance**\n",
    "\n",
    "| Coefficient | Std. Error | z-Statistic | p-Value |         |\n",
    "| ----------- | ---------- | ----------- | ------- | ------- |\n",
    "| Intercept   | -10.6513   | 0.3612      | -29.5   | <0.0001 |\n",
    "| Balance     | 0.0055     | 0.0002      | 24.9    | <0.0001 |\n",
    "\n",
    "Logistic regression also handles qualitative predictors using **dummy variables**. For example, for the qualitative variable `student` (1 = student, 0 = non-student), the estimated coefficients from **Table 4.2** are:\n",
    "\n",
    "* $\\hat{\\beta}_0 = -3.5041$\n",
    "* $\\hat{\\beta}_1 = 0.4049$\n",
    "\n",
    "Then:\n",
    "\n",
    "* For a student:\n",
    "\n",
    "$$\n",
    "\\hat{p}(\\text{default} = \\text{Yes} \\mid \\text{student} = 1) = \\frac{e^{-3.5041 + 0.4049 \\cdot 1}}{1 + e^{-3.5041 + 0.4049 \\cdot 1}} = \\frac{e^{-3.0992}}{1 + e^{-3.0992}} \\approx 0.0431\n",
    "$$\n",
    "\n",
    "* For a non-student:\n",
    "\n",
    "$$\n",
    "\\hat{p}(\\text{default} = \\text{Yes} \\mid \\text{student} = 0) = \\frac{e^{-3.5041 + 0.4049 \\cdot 0}}{1 + e^{-3.5041 + 0.4049 \\cdot 0}} = \\frac{e^{-3.5041}}{1 + e^{-3.5041}} \\approx 0.0292\n",
    "$$\n",
    "\n",
    "The positive coefficient indicates that students are predicted to have a higher probability of default than non-students.\n",
    "\n",
    "**Table 4.2: Logistic Regression Predicting Default from Student Status**\n",
    "\n",
    "| Coefficient    | Std. Error | z-Statistic | p-Value |         |\n",
    "| -------------- | ---------- | ----------- | ------- | ------- |\n",
    "| Intercept      | -3.5041    | 0.0707      | -49.55  | <0.0001 |\n",
    "| Student \\[Yes] | 0.4049     | 0.1150      | 3.52    | 0.0004  |\n",
    "\n",
    "**Multiple Regression**\n",
    "\n",
    "To predict a binary response using multiple predictors, we generalize the logistic regression model as:\n",
    "\n",
    "$$\n",
    "\\log \\left( \\frac{p(X)}{1 - p(X)} \\right) = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p\n",
    "$$\n",
    "\n",
    "or equivalently,\n",
    "\n",
    "$$\n",
    "\\hat{p}(X) = \\frac{e^{\\hat{\\beta}_0 + \\hat{\\beta}_1 X_1 + \\cdots + \\hat{\\beta}_p X_p}}{1 + e^{\\hat{\\beta}_0 + \\hat{\\beta}_1 X_1 + \\cdots + \\hat{\\beta}_p X_p}}\n",
    "$$\n",
    "\n",
    "**Table 4.3: Logistic Regression Predicting Default from Multiple Predictors**\n",
    "\n",
    "| Coefficient    | Std. Error | z-Statistic | p-Value |         |\n",
    "| -------------- | ---------- | ----------- | ------- | ------- |\n",
    "| Intercept      | -10.8690   | 0.4923      | -22.08  | <0.0001 |\n",
    "| Balance        | 0.0057     | 0.0002      | 24.74   | <0.0001 |\n",
    "| Income         | 0.0030     | 0.0082      | 0.37    | 0.7115  |\n",
    "| Student \\[Yes] | -0.6468    | 0.2362      | -2.74   | 0.0062  |\n",
    "\n",
    "As before, we use **maximum likelihood** to estimate the coefficients. From the table:\n",
    "\n",
    "* Balance is positively and significantly associated with default.\n",
    "* Income is not statistically significant.\n",
    "* Student status has a **negative coefficient**, in contrast to the positive coefficient in the earlier single-variable model (Table 4.2).\n",
    "\n",
    "This reversal occurs due to **confounding variable**: student status and balance are correlated—students tend to carry higher balances, which are strongly associated with higher default rates. So although students have higher **overall** default rates, for the **same balance and income**, students are actually **less likely** to default than non-students.\n",
    "\n",
    "**Predictions from the Multiple Logistic Regression Model**\n",
    "\n",
    "Using the estimated coefficients from Table 4.3, we can compute predicted probabilities:\n",
    "\n",
    "* For a **student** with balance = \\$1,500 and income = \\$40,000:\n",
    "\n",
    "$$\n",
    "\\hat{p}(\\text{default} = \\text{Yes} \\mid \\text{student} = 1, \\text{balance} = 1500, \\text{income} = 40) =\n",
    "\\frac{e^{-10.869 + 0.0057 \\cdot 1500 + 0.0030 \\cdot 40 - 0.6468 \\cdot 1}}{1 + e^{-10.869 + 0.0057 \\cdot 1500 + 0.0030 \\cdot 40 - 0.6468 \\cdot 1}} \\approx 0.058\n",
    "$$\n",
    "\n",
    "* For a **non-student** with the same balance and income:\n",
    "\n",
    "$$\n",
    "\\hat{p}(\\text{default} = \\text{Yes} \\mid \\text{student} = 0, \\text{balance} = 1500, \\text{income} = 40) =\n",
    "\\frac{e^{-10.869 + 0.0057 \\cdot 1500 + 0.0030 \\cdot 40 - 0.6468 \\cdot 0}}{1 + e^{-10.869 + 0.0057 \\cdot 1500 + 0.0030 \\cdot 40 - 0.6468 \\cdot 0}} \\approx 0.105\n",
    "$$\n",
    "\n",
    "Note: Income is measured in **thousands of dollars**, so we use 40 instead of 40,000 in the calculation.\n",
    "\n",
    "**Discussion of confounding**\n",
    "\n",
    "The **overall default rate** refers to the empirical proportion of individuals in the dataset who defaulted, regardless of predictor values.\n",
    "\n",
    "Mathematically, if you have $n$ observations and $y_i \\in \\{0, 1\\}$ indicates whether person $i$ defaulted:\n",
    "\n",
    "$$\n",
    "\\text{Overall default rate} = \\frac{1}{n} \\sum_{i=1}^n y_i\n",
    "$$\n",
    "\n",
    "This is **not** a model-based prediction—it's simply the average value of the binary response across the data. For example, if 3.3% of people in the data defaulted, the overall default rate is 0.033.\n",
    "\n",
    "In plots like **Figure 4.3**, the horizontal dashed lines show this empirical rate **within subgroups**, e.g., students vs. non-students, averaged across all values of balance and income.\n",
    "\n",
    "How can students be less likely to default (in the model), yet have a higher overall default rate?\n",
    "\n",
    "This is the classic signature of **confounding**.\n",
    "\n",
    "* In the **single-variable logistic regression** (Table 4.2), students appear **more likely** to default. That’s because:\n",
    "\n",
    "  * Students tend to have **higher balances** (right panel of Figure 4.3).\n",
    "  * Higher balances are strongly associated with higher default risk.\n",
    "  * So, without adjusting for balance, it *looks* like students are riskier.\n",
    "\n",
    "* In the **multiple logistic regression** (Table 4.3), after controlling for **balance** and **income**, the **coefficient for student becomes negative**:\n",
    "\n",
    "  * This means that **for two people with the same balance and income**, the student is actually **less likely** to default.\n",
    "  * So, once we hold balance constant, the true effect of student status is revealed.\n",
    "\n",
    "* The multiple logistic regression says:\n",
    "\n",
    "  $$\n",
    "  \\log \\left( \\frac{\\Pr(\\text{default} = 1 \\mid X)}{1 - \\Pr(\\text{default} = 1 \\mid X)} \\right) = \\beta_0 + \\beta_1 \\cdot \\text{balance} + \\beta_2 \\cdot \\text{income} + \\beta_3 \\cdot \\text{student}\n",
    "  $$\n",
    "\n",
    "  where $\\hat{\\beta}_3 = -0.6468$ implies students are less likely to default **when balance and income are the same**.\n",
    "\n",
    "The higher **overall** student default rate is due to **students having higher average balances**, not because being a student inherently increases default risk.\n",
    "\n",
    "**Student status is a confounding variable**. It’s associated with higher balance, which increases default risk. Once you control for balance, being a student is actually associated with **lower** default probability.\n",
    "\n",
    "Here’s a concise and clear version for **interview review**, retaining all key ideas and equations from the section on **multinomial logistic regression**:\n",
    "\n",
    "**Multinomial Logistic Regression (for $K > 2$ classes)**\n",
    "\n",
    "Logistic regression can be extended to handle **multi-class classification**—this is called **multinomial logistic regression**.\n",
    "\n",
    "Suppose $Y \\in \\{1, 2, ..., K\\}$ is a response with **K classes**, and we choose one class (say class $K$) as the **baseline**.\n",
    "\n",
    "For $k = 1, \\dots, K-1$:\n",
    "\n",
    "$$\n",
    "\\Pr(Y = k \\mid X = x) = \\frac{e^{\\beta_{k0} + \\beta_{k1}x_1 + \\cdots + \\beta_{kp}x_p}}{1 + \\sum_{l=1}^{K-1} e^{\\beta_{l0} + \\beta_{l1}x_1 + \\cdots + \\beta_{lp}x_p}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Pr(Y = K \\mid X = x) = \\frac{1}{1 + \\sum_{l=1}^{K-1} e^{\\beta_{l0} + \\beta_{l1}x_1 + \\cdots + \\beta_{lp}x_p}}\n",
    "$$\n",
    "\n",
    "And the **log-odds** for class $k$ vs. the baseline class $K$ is:\n",
    "\n",
    "$$\n",
    "\\log \\left( \\frac{\\Pr(Y = k \\mid X = x)}{\\Pr(Y = K \\mid X = x)} \\right) = \\beta_{k0} + \\beta_{k1}x_1 + \\cdots + \\beta_{kp}x_p\n",
    "$$\n",
    "\n",
    "This generalizes binary logistic regression: the log-odds between any class and the baseline is linear in the predictors.\n",
    "\n",
    "* The choice of **baseline class** doesn’t affect predicted probabilities or log-odds between any pair of classes, but it **does affect coefficient interpretation**.\n",
    "* For example, if **epileptic seizure** is the baseline, then $\\beta_{\\text{stroke}, j}$ is the change in log-odds of stroke vs. epileptic seizure for a one-unit increase in $x_j$.\n",
    "* The **odds ratio** increases by $e^{\\beta_{\\text{stroke}, j}}$ per unit increase in $x_j$.\n",
    "\n",
    "**Softmax Coding**\n",
    "\n",
    "An alternative formulation used widely in **machine learning** is **softmax coding**, which treats all classes symmetrically:\n",
    "\n",
    "$$\n",
    "\\Pr(Y = k \\mid X = x) = \\frac{e^{\\beta_{k0} + \\beta_{k1}x_1 + \\cdots + \\beta_{kp}x_p}}{\\sum_{l=1}^K e^{\\beta_{l0} + \\beta_{l1}x_1 + \\cdots + \\beta_{lp}x_p}} \\quad \\text{for } k = 1, \\dots, K\n",
    "$$\n",
    "\n",
    "In this version\n",
    "\n",
    "* Coefficients are estimated for **all K classes**.\n",
    "* The **log-odds between any two classes** $k$ and $k'$ is:\n",
    "\n",
    "$$\n",
    "\\log \\left( \\frac{\\Pr(Y = k \\mid X = x)}{\\Pr(Y = k' \\mid X = x)} \\right)\n",
    "= (\\beta_{k0} - \\beta_{k'0}) + (\\beta_{k1} - \\beta_{k'1})x_1 + \\cdots + (\\beta_{kp} - \\beta_{k'p})x_p\n",
    "$$\n",
    "\n",
    "Softmax coding produces the **same predicted probabilities and log-odds** as the baseline coding—it’s just a different parameterization.\n",
    "\n",
    "**Summary Table**\n",
    "\n",
    "| Concept                  | Description                                                                                        |\n",
    "| ------------------------ | -------------------------------------------------------------------------------------------------- |\n",
    "| Modeling approach        | **Discriminative**: directly models $\\Pr(Y = k \\mid X = x)$                                        |\n",
    "| Functional form (binary) | $\\log \\frac{p(x)}{1 - p(x)} = \\beta_0 + \\beta_1 x$                                                 |\n",
    "| Output                   | Probability estimates for each class                                                               |\n",
    "| Probability function     | $p(x) = \\frac{e^{\\beta_0 + \\beta_1 x}}{1 + e^{\\beta_0 + \\beta_1 x}}$                               |\n",
    "| Fitting method           | Maximum likelihood estimation                                                                      |\n",
    "| Link function            | **Logit** (log-odds)                                                                               |\n",
    "| Assumptions              | Linear relationship between predictors and log-odds                                                |\n",
    "| Decision rule            | Predict class 1 if $p(x) > t$, with threshold $t$ (e.g., 0.5)                                      |\n",
    "| Interpretability         | Coefficients represent **change in log-odds** per unit change in predictor                         |\n",
    "| Extensions               | - Multinomial logistic regression (for $K > 2$)                                                    |\n",
    "|                          | - Softmax regression (symmetric multiclass generalization)                                         |\n",
    "| Strengths                | - Probabilistic output                                                                             |\n",
    "|                          | - Handles binary and multiclass tasks                                                              |\n",
    "|                          | - Can include interactions and non-linearities via features                                        |\n",
    "| Limitations              | - Assumes linear log-odds                                                                          |\n",
    "|                          | - May underperform if classes are non-separable or heavily overlapping                             |\n",
    "| Relation to LDA          | Logistic regression is **discriminative**; LDA is **generative**                                   |\n",
    "| Confounding handling     | Can adjust for multiple predictors simultaneously                                                  |\n",
    "| Prediction formula       | $\\hat{p}(x) = \\frac{e^{\\hat{\\beta}_0 + \\hat{\\beta}_1 x}}{1 + e^{\\hat{\\beta}_0 + \\hat{\\beta}_1 x}}$ |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### $\\underline{\\text{Generative Models for Classification}}:$\n",
    "\n",
    "Logistic regression directly models the **posterior probability**:\n",
    "\n",
    "$$\n",
    "\\Pr(Y = k \\mid X = x)\n",
    "$$\n",
    "\n",
    "This is called a **discriminative approach**, since it models the response $Y$ given predictors $X$.\n",
    "\n",
    "### **Generative Approach**\n",
    "\n",
    "Instead of modeling $\\Pr(Y \\mid X)$ directly, we:\n",
    "\n",
    "1. Model the **class priors** $\\pi_k = \\Pr(Y = k)$\n",
    "2. Model the **class-conditional density** $f_k(x) = \\Pr(X = x \\mid Y = k)$\n",
    "\n",
    "Then apply **Bayes' theorem**:\n",
    "\n",
    "$$\n",
    "\\Pr(Y = k \\mid X = x) = \\frac{\\pi_k f_k(x)}{\\sum_{l=1}^K \\pi_l f_l(x)}\n",
    "$$\n",
    "\n",
    "This gives us the **posterior probability** $p_k(x)$, which we can use to classify observations by assigning them to the class with the highest posterior (i.e., **Bayes classifier**).\n",
    "\n",
    "We use this approach because \n",
    "\n",
    "* **More stable** than logistic regression when classes are well separated.\n",
    "* **Better performance** with small sample sizes, assuming normality of $X$ within each class.\n",
    "* Naturally generalizes to multi-class settings.\n",
    "* Enables approximation of the **Bayes classifier**, which has the lowest theoretical error (if $\\pi_k$ and $f_k(x)$ are correctly specified).\n",
    "\n",
    "**Estimating $\\pi_k$ and $f_k(x)$**\n",
    "\n",
    "* $\\pi_k$ can be estimated easily as the proportion of training examples in class $k$.\n",
    "* Estimating $f_k(x)$ is harder and typically requires assumptions (e.g., normal distribution).\n",
    "\n",
    "This framework leads to three common generative classifiers:\n",
    "\n",
    "* **Linear Discriminant Analysis (LDA)**\n",
    "* **Quadratic Discriminant Analysis (QDA)**\n",
    "* **Naive Bayes**\n",
    "\n",
    "Each uses a different assumption about the form of $f_k(x)$, allowing practical approximation of the Bayes classifier.\n",
    "\n",
    "\n",
    "### **Linear Discriminant Analysis for $p=1$:**\n",
    "\n",
    "**LDA** is a generative classification method that models the conditional distribution $X \\mid Y = k$ as **Gaussian** with **class-specific means** $\\mu_k$ and a **shared variance** $\\sigma^2$. Using **Bayes’ theorem**, it computes the posterior $\\Pr(Y = k \\mid X = x)$, and classifies observations by selecting the class with the highest posterior (i.e., the **Bayes classifier**).\n",
    "\n",
    "**Assumptions (Univariate case):**\n",
    "\n",
    "$$\n",
    "f_k(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( -\\frac{(x - \\mu_k)^2}{2\\sigma^2} \\right)\n",
    "$$\n",
    "\n",
    "* $X \\mid Y = k \\sim \\mathcal{N}(\\mu_k, \\sigma^2)$\n",
    "* Common variance across all classes\n",
    "* Class priors $\\pi_k$ are known or estimated from the training data\n",
    "\n",
    "**Discriminant Function:**\n",
    "\n",
    "LDA simplifies the Bayes rule into a **linear discriminant function**:\n",
    "\n",
    "$$\n",
    "\\delta_k(x) = \\frac{\\mu_k}{\\sigma^2}x - \\frac{\\mu_k^2}{2\\sigma^2} + \\log(\\pi_k)\n",
    "$$\n",
    "\n",
    "Predict the class with the largest $\\delta_k(x)$.\n",
    "\n",
    "**Parameter Estimation:**\n",
    "\n",
    "Given training data:\n",
    "\n",
    "* **Class means**:\n",
    "\n",
    "  $$\n",
    "  \\hat{\\mu}_k = \\frac{1}{n_k} \\sum_{i: y_i = k} x_i\n",
    "  $$\n",
    "\n",
    "* **Shared variance**:\n",
    "\n",
    "  $$\n",
    "  \\hat{\\sigma}^2 = \\frac{1}{n - K} \\sum_{k=1}^K \\sum_{i: y_i = k} (x_i - \\hat{\\mu}_k)^2\n",
    "  $$\n",
    "\n",
    "* **Class priors**:\n",
    "\n",
    "  $$\n",
    "  \\hat{\\pi}_k = \\frac{n_k}{n}\n",
    "  $$\n",
    "\n",
    "These are substituted into the estimated discriminant:\n",
    "\n",
    "$$\n",
    "\\hat{\\delta}_k(x) = \\frac{\\hat{\\mu}_k}{\\hat{\\sigma}^2}x - \\frac{\\hat{\\mu}_k^2}{2\\hat{\\sigma}^2} + \\log(\\hat{\\pi}_k)\n",
    "$$\n",
    "\n",
    "\n",
    "**Decision Boundary (Two Classes, Equal Priors):**\n",
    "\n",
    "If $\\pi_1 = \\pi_2$, then the LDA decision boundary is simply:\n",
    "\n",
    "$$\n",
    "x = \\frac{\\mu_1 + \\mu_2}{2}\n",
    "$$\n",
    "\n",
    "**Performance Insight:**\n",
    "\n",
    "In simulated examples, LDA achieves near-optimal performance (e.g., 11.1% test error vs. 10.6% Bayes error), assuming the Gaussian and equal variance assumptions hold.\n",
    "\n",
    "**Model Assumptions:**\n",
    "\n",
    "LDA assumes Gaussian-distributed predictors with **class-specific means** and a **common variance**. When this assumption holds, it closely approximates the Bayes classifier. If not, more flexible models like **QDA** (which allows class-specific variances) may perform better.\n",
    "\n",
    "\n",
    "**Summary Table**\n",
    "\n",
    "| Concept               | Description                                                             |\n",
    "| --------------------- | ----------------------------------------------------------------------- |\n",
    "| Generative assumption | $X \\mid Y = k \\sim \\mathcal{N}(\\mu_k, \\sigma^2)$                        |\n",
    "| Decision rule         | Assign $x$ to class with largest $\\delta_k(x)$                          |\n",
    "| Discriminant function | Linear in $x$                                                           |\n",
    "| Parameters estimated  | $\\mu_k$, shared $\\sigma^2$, class prior $\\pi_k$                         |\n",
    "| Strengths             | Simple, fast, interpretable, near-optimal if assumptions hold           |\n",
    "| Limitations           | Assumes equal variance; less flexible than QDA or nonparametric methods |\n",
    "| Output                | Posterior probabilities, linear decision boundaries                     |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Linear Discriminant Analysis for $p > 1$:**\n",
    "\n",
    "**Assumptions**\n",
    "\n",
    "LDA assumes that the predictor $\\mathbf{X} \\in \\mathbb{R}^p$ follows a **multivariate Gaussian distribution** within each class $k$:\n",
    "\n",
    "$$\n",
    "\\mathbf{X} \\mid Y = k \\sim \\mathcal{N}(\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma})\n",
    "$$\n",
    "\n",
    "* $\\boldsymbol{\\mu}_k$: class-specific mean vector ($p \\times 1$)\n",
    "* $\\boldsymbol{\\Sigma}$: shared covariance matrix ($p \\times p$), same for all $K$ classes\n",
    "* $\\pi_k$: class prior probability, often estimated from class frequencies\n",
    "\n",
    "**Multivariate Gaussian Density**\n",
    "\n",
    "The density for class $k$ is:\n",
    "\n",
    "$$\n",
    "f_k(\\mathbf{x}) = \\frac{1}{(2\\pi)^{p/2} |\\boldsymbol{\\Sigma}|^{1/2}} \\exp\\left( -\\frac{1}{2}(\\mathbf{x} - \\boldsymbol{\\mu}_k)^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}_k) \\right)\n",
    "$$\n",
    "\n",
    "**Discriminant Function**\n",
    "\n",
    "Instead of directly comparing the densities, LDA defines a **discriminant score** $\\delta_k(\\mathbf{x})$, derived from log-posterior:\n",
    "\n",
    "$$\n",
    "\\delta_k(\\mathbf{x}) = \\mathbf{x}^T \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}_k - \\frac{1}{2} \\boldsymbol{\\mu}_k^T \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}_k + \\log(\\pi_k)\n",
    "$$\n",
    "\n",
    "An observation is assigned to the class with the **largest** $\\delta_k(\\mathbf{x})$.\n",
    "\n",
    "Each $\\delta_k(\\mathbf{x})$ is a **linear function** of $\\mathbf{x}$, which gives LDA its name.\n",
    "\n",
    "**Decision Boundaries**\n",
    "\n",
    "* Boundaries between class $k$ and class $\\ell$ are defined by the set of $\\mathbf{x}$ where $\\delta_k(\\mathbf{x}) = \\delta_\\ell(\\mathbf{x})$\n",
    "* These are **linear hyperplanes** in $\\mathbb{R}^p$\n",
    "\n",
    "**Parameter Estimation from Training Data**\n",
    "\n",
    "Given $n$ training examples and $K$ classes:\n",
    "\n",
    "* Sample mean vector for class $k$:\n",
    "\n",
    "  $$\n",
    "  \\hat{\\boldsymbol{\\mu}}_k = \\frac{1}{n_k} \\sum_{i: y_i = k} \\mathbf{x}_i\n",
    "  $$\n",
    "\n",
    "* Shared covariance matrix:\n",
    "\n",
    "  $$\n",
    "  \\hat{\\boldsymbol{\\Sigma}} = \\frac{1}{n - K} \\sum_{k=1}^K \\sum_{i: y_i = k} (\\mathbf{x}_i - \\hat{\\boldsymbol{\\mu}}_k)(\\mathbf{x}_i - \\hat{\\boldsymbol{\\mu}}_k)^T\n",
    "  $$\n",
    "\n",
    "* Priors:\n",
    "\n",
    "  $$\n",
    "  \\hat{\\pi}_k = \\frac{n_k}{n}\n",
    "  $$\n",
    "\n",
    "**Classification Rule**\n",
    "\n",
    "Plug in the estimates into the discriminant function:\n",
    "\n",
    "$$\n",
    "\\hat{\\delta}_k(\\mathbf{x}) = \\mathbf{x}^T \\hat{\\boldsymbol{\\Sigma}}^{-1} \\hat{\\boldsymbol{\\mu}}_k - \\frac{1}{2} \\hat{\\boldsymbol{\\mu}}_k^T \\hat{\\boldsymbol{\\Sigma}}^{-1} \\hat{\\boldsymbol{\\mu}}_k + \\log(\\hat{\\pi}_k)\n",
    "$$\n",
    "\n",
    "Assign $\\mathbf{x}$ to the class with the largest $\\hat{\\delta}_k(\\mathbf{x})$.\n",
    "\n",
    "**Sensitivity to Thresholding**\n",
    "\n",
    "LDA implicitly uses a **0.5 threshold** for the posterior probability. This minimizes overall error but can underperform on specific classes (e.g., defaulters). Lowering the threshold (e.g., to 0.2) increases **sensitivity** (true positive rate) at the cost of **specificity** (true negative rate).\n",
    "\n",
    "This tradeoff is visualized with a **ROC curve**; the **area under the curve (AUC)** summarizes classifier performance across all thresholds. For example, AUC = 0.95 indicates excellent classification.\n",
    "\n",
    "**Confusion Matrix Terminology (Table 4.6)**\n",
    "\n",
    "| True Class   | Predicted Negative  | Predicted Positive  | Total |\n",
    "| ------------ | ------------------- | ------------------- | ----- |\n",
    "| Negative (−) | TN (True Negative)  | FP (False Positive) | N\\*   |\n",
    "| Positive (+) | FN (False Negative) | TP (True Positive)  | P\\*   |\n",
    "| Total        | N                   | P                   |       |\n",
    "\n",
    "**Key Metrics (Table 4.7)**\n",
    "\n",
    "| Name                          | Formula          | Synonyms                               |\n",
    "| ----------------------------- | ---------------- | -------------------------------------- |\n",
    "| **False Positive Rate**       | $\\frac{FP}{N}$   | Type I error, $1 - \\text{Specificity}$ |\n",
    "| **True Positive Rate**        | $\\frac{TP}{P}$   | Power, Sensitivity, Recall             |\n",
    "| **Positive Predictive Value** | $\\frac{TP}{P^*}$ | Precision                              |\n",
    "| **Negative Predictive Value** | $\\frac{TN}{N^*}$ | —                                      |\n",
    "\n",
    "---\n",
    "\n",
    "**LDA $p>1$ Summary Table**\n",
    "\n",
    "| Concept                   | Description                                                                          |\n",
    "| ------------------------- | ------------------------------------------------------------------------------------ |\n",
    "| **Assumption**            | Multivariate normal distribution with class-specific means and shared covariance     |\n",
    "| **Discriminant function** | Linear in $\\mathbf{x}$, derived from Bayes theorem                                   |\n",
    "| **Classification rule**   | Assign to class with highest $\\hat{\\delta}_k(\\mathbf{x})$                            |\n",
    "| **Decision boundary**     | Linear hyperplanes between class regions                                             |\n",
    "| **Parameter estimation**  | Estimate $\\boldsymbol{\\mu}_k$, $\\boldsymbol{\\Sigma}$, and $\\pi_k$ from training data |\n",
    "| **Effect of threshold**   | Lower thresholds increase sensitivity at the cost of specificity                     |\n",
    "| **ROC/AUC**               | Summarize classifier performance across all thresholds                               |\n",
    "| **When LDA works well**   | When classes are approximately Gaussian and have equal covariances                   |\n",
    "| **Limitations**           | Poor performance if assumptions are violated or classes are highly non-linear        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quadratic Discriminant Analysis (QDA)\n",
    "\n",
    "QDA is a generative classification model that, like LDA, assumes the feature vector $\\mathbf{X} \\in \\mathbb{R}^p$ follows a multivariate Gaussian distribution **within each class**, but **allows a separate covariance matrix** for each class:\n",
    "\n",
    "$$\n",
    "\\mathbf{X} \\mid Y = k \\sim \\mathcal{N}(\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)\n",
    "$$\n",
    "\n",
    "* $\\boldsymbol{\\mu}_k$ is the class-specific mean vector.\n",
    "* $\\boldsymbol{\\Sigma}_k$ is the **class-specific** covariance matrix.\n",
    "* $\\pi_k = \\mathbb{P}(Y = k)$ is the prior probability for class \\$k\\$.\n",
    "\n",
    "**QDA Discriminant Function**\n",
    "\n",
    "Using Bayes’ theorem, the log-posterior (up to proportionality) becomes:\n",
    "\n",
    "$$\n",
    "\\delta_k(\\mathbf{x}) = \n",
    "- \\frac{1}{2} (\\mathbf{x} - \\boldsymbol{\\mu}_k)^T \\boldsymbol{\\Sigma}_k^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}_k)\n",
    "- \\frac{1}{2} \\log |\\boldsymbol{\\Sigma}_k| \n",
    "+ \\log \\pi_k\n",
    "$$\n",
    "\n",
    "This expands to:\n",
    "\n",
    "$$\n",
    "\\delta_k(\\mathbf{x}) \n",
    "= -\\frac{1}{2} \\mathbf{x}^T \\boldsymbol{\\Sigma}_k^{-1} \\mathbf{x}\n",
    "+ \\mathbf{x}^T \\boldsymbol{\\Sigma}_k^{-1} \\boldsymbol{\\mu}_k \n",
    "- \\frac{1}{2} \\boldsymbol{\\mu}_k^T \\boldsymbol{\\Sigma}_k^{-1} \\boldsymbol{\\mu}_k \n",
    "- \\frac{1}{2} \\log |\\boldsymbol{\\Sigma}_k| + \\log \\pi_k\n",
    "$$\n",
    "\n",
    "Since this includes a **quadratic form** in $\\mathbf{x}$, QDA’s decision boundaries are **quadratic surfaces**, unlike the linear boundaries in LDA.\n",
    "\n",
    "**QDA Classification Rule**\n",
    "\n",
    "Assign $\\mathbf{x}$ to the class with the largest discriminant score:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\arg\\max_k \\delta_k(\\mathbf{x})\n",
    "$$\n",
    "\n",
    "**Parameter Estimation**\n",
    "\n",
    "From the training data:\n",
    "\n",
    "- $\\hat{\\mu}_k = \\dfrac{1}{n_k} \\sum_{i: y_i = k} \\mathbf{x}_i$\n",
    "- $\\hat{\\boldsymbol{\\Sigma}}_k = \\dfrac{1}{n_k - 1} \\sum_{i: y_i = k} (\\mathbf{x}_i - \\hat{\\mu}_k)(\\mathbf{x}_i - \\hat{\\mu}_k)^T$\n",
    "- $\\hat{\\pi}_k = \\dfrac{n_k}{n}$\n",
    "\n",
    "**LDA vs. QDA: Bias–Variance Trade-Off**\n",
    "\n",
    "| Model   | Covariance Assumption                | Decision Boundary | Covariance Params               | Variance | Bias                          |\n",
    "|--------|----------------------------------------|-------------------|----------------------------------|----------|-------------------------------|\n",
    "| **LDA** | Shared $\\boldsymbol{\\Sigma}$ across classes | Linear            | $\\frac{p(p+1)}{2}$ total         | Low      | High (if assumption is wrong) |\n",
    "| **QDA** | Class-specific $\\boldsymbol{\\Sigma}_k$     | Quadratic         | $K \\cdot \\frac{p(p+1)}{2}$ total | High     | Low (if assumption holds)     |\n",
    "\n",
    "- **Use LDA** when $n$ is small or classes have similar covariance structure — lower variance improves generalization.\n",
    "- **Use QDA** when $n$ is large or class covariances are clearly different — more flexibility reduces bias.\n",
    "\n",
    "**Geometric Interpretation (ISLR Fig. 4.9)**\n",
    "\n",
    "* **Left Panel:** $\\Sigma_1 = \\Sigma_2$. Bayes boundary is linear. LDA matches Bayes well. QDA slightly overfits.\n",
    "* **Right Panel:** $\\Sigma_1 \\ne \\Sigma_2$. Bayes boundary is curved. QDA matches Bayes better. LDA underfits.\n",
    "\n",
    "\n",
    "**Summary Table — LDA vs. QDA**\n",
    "| Feature               | LDA                                | QDA                                     |\n",
    "|-----------------------|-------------------------------------|------------------------------------------|\n",
    "| Covariance Structure  | Common to all classes ($\\Sigma$)   | Separate for each class ($\\Sigma_k$)     |\n",
    "| Decision Boundary     | Linear                             | Quadratic                                |\n",
    "| Covariance Parameters | $\\frac{p(p+1)}{2}$                 | $K \\cdot \\frac{p(p+1)}{2}$               |\n",
    "| Variance              | Low                                | Higher                                   |\n",
    "| Bias                  | High (if assumption violated)      | Low (if assumption holds)                |\n",
    "| Best Use Case         | Few samples, similar class spreads | Many samples, class-specific spreads     |\n",
    "| Computational Cost    | Low                                | Higher (due to matrix per class)         |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
