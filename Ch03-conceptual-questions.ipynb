{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9210ffaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from matplotlib.pyplot import subplots \n",
    "import statsmodels.api as sm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "358b84db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence \\\n",
    "import variance_inflation_factor as VIF\n",
    "from statsmodels.stats.anova import anova_lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9627fe91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ISLP import load_data \n",
    "from ISLP.models import (ModelSpec as MS, \n",
    "                        summarize, \n",
    "                        poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86b2955",
   "metadata": {},
   "source": [
    "### Conceptual Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446188cb",
   "metadata": {},
   "source": [
    "### Question 1. \n",
    "Describe the null hypothesis to which p-values given in Table 3.4 correspond. Explain what conclusions you can draw based on these p-values. Your explaination should be in terms of 'sales', 'TV', 'radio', and 'newspaper'. \n",
    "\n",
    "### Answer.\n",
    "\n",
    "For TV $H_0$: $\\beta_1 = 0$\n",
    "For radio $H_0$: $\\beta_2 = 0$\n",
    "For newspaper $H_0$: $\\beta_3 = 0$\n",
    "\n",
    "For the given p-values of TV and radio $p<0.0001$ infers that there is a relationship between these predictors and sales. Therefore, we can reject the null hypothesis for these variables. For the p-value associated with newspaper $p=0.8599$, there is barely a relationship between spending on newspaper ads and sales. It means that the chance that the that probability that the null hypothesis for newspaper ads is true is 0.8599.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72863bfd",
   "metadata": {},
   "source": [
    "### Question 2. \n",
    "Carefully explain the differences between the KNN classifier and KNN regression methods. \n",
    "\n",
    "### Answer.\n",
    "K-Nearest Neighbors (KNN) classifier is used for categorical (qualitative) data, where the goal is to assign a test observation to the most frequent class among its \n",
    "K nearest neighbors. The class label is determined by whichever class appears most frequently among the K nearest neighbors.\n",
    "\n",
    "$$P(Y = j \\mid X = x_o) = \\frac{1}{K} \\sum_{i \\in N_o} I(y_i = j)$$\n",
    "\n",
    "where: \n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "    \\bullet & Y \\text{ is the class label.} \\\\\n",
    "    \\bullet & x_o \\text{ is the test observation (the point for which we want to predict a class).} \\\\\n",
    "    \\bullet & K \\text{ is the number of nearest neighbors we are considering.} \\\\\n",
    "    \\bullet & N_o \\text{ is the set of indices of the } K \\text{ nearest neighbors of } x_o. \\\\\n",
    "    \\bullet & y_i \\text{ is the class label of the } i \\text{th training sample.} \\\\\n",
    "    \\bullet & I(y_i = j) \\text{ is an indicator function that returns:} \\\\\n",
    "    & \\quad \\text{1 if } y_i = j \\text{ (i.e., if the } i \\text{th neighbor belongs to class } j). \\\\\n",
    "    & \\quad \\text{0 otherwise.}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "KNN regression is used for continuous (quantitative) data. Instead of assigning a class, it predicts a numerical value by averaging (or sometimes weighting) the values of the \n",
    "K nearest neighbors\n",
    "\n",
    "$$\n",
    "\\hat{f}(x) = \\frac{1}{K} \\sum_{x_i \\in N_o} y_i\n",
    "$$\n",
    "\n",
    "where: \n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "    \\bullet & \\hat{f}(x) \\text{ is the predicted output for the test observation } x. \\\\\n",
    "    \\bullet & K \\text{ is the number of nearest neighbors.} \\\\\n",
    "    \\bullet & N_o \\text{ is the set of indices of the } K \\text{ nearest neighbors of } x. \\\\\n",
    "    \\bullet & y_i \\text{ is the observed value of the } i \\text{th nearest neighbor.} \\\\\n",
    "    \\bullet & \\text{The formula computes the mean (or weighted mean) of the } y_i \\text{ values of the } K \\text{ nearest neighbors.}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28d50e4",
   "metadata": {},
   "source": [
    "### Question 3. \n",
    "Suppose we have a data set with five predictors, $X_1$=GPA, $X_2$=IQ, $X_3$=Level (1 for College, 0 for High School), $X_4$=Interaction between GPA and IQ, and $X5$=Interaction between GPA and Level. The response is starting salary after graduation in thousands of dollars. Suppost we use least squares to fit the model and get $\\hat{\\beta_0}$=50, $\\hat{\\beta_1}$=20, $\\hat{\\beta_2}$=0.07, $\\hat{\\beta_3}$=35, $\\hat{\\beta_4}$=0.01, $\\hat{\\beta_5}$=-10.\n",
    "\n",
    "a. Which answer is correct and why? \n",
    "\n",
    "i. For a fixed value of IQ and GPA, high school graduates earn more, on average, than college graduates?\n",
    "\n",
    "Ans.\n",
    "False. Since the model is: \n",
    "$Y = 50 + 20X_1 + 0.07X_2 + 35X_3 + 0.01X_4 - 10X_5$\n",
    "\n",
    "Then since the indicator for the level is \n",
    "$X_3 =\\begin{cases} 1 & College\\\\\n",
    "                     0 &  High\\hspace{0.1cm}School \n",
    "       \\end{cases}$\n",
    "\n",
    "Then the model for the high school student will be \n",
    "$Y = 50 + 20X_1 + 0.07X_2 + 0.01X_4 - 10X_5$ which will \n",
    "be less since there is no contributed from the $X_3$.\n",
    "\n",
    "ii. For a fixed value of IQ and GPA, college graduates earn more, on average, than high school graduates?  \n",
    "\n",
    "Ans. True, similar logic as before.\n",
    "\n",
    "iii. For a fixed value of IQ and GPA, high school graduates earn, on average, more than college graduates provided that the GPA is high enough.\n",
    "\n",
    "Ans. IQ Range = 50 - 150, GPA Range = 1 - 4. Then we can see the following:\n",
    "Let's set the IQ for both to 100.  \n",
    "Let's try a low GPA for the college graduate, GPA = 1 and \n",
    "high GPA for the high school graduate, GPA = 4. \n",
    "\n",
    "For the college grad:\n",
    "$Y = 50 + 20\\times1 + 0.07\\times100 + 35\\times1 + 0.01\\times100 - 10\\times1 = 50 + 20 + 7 + 35 + 1 - 10 = 103$\n",
    "\n",
    "For the high school grad: \n",
    "$Y = 50 + 20\\times4 + 0.07\\times100 + 0.01\\times400 = 50 + 60 + 7 + 4 = 121$.\n",
    "\n",
    "iv. For a fixed value of IQ and GPA, college graduates earn, on average, more than high school graduates provided that the GPA is high enough.\n",
    "\n",
    "For the college grad:\n",
    "$Y = 50 + 20\\times4 + 0.07\\times100 + 35\\times1 + 0.01\\times400 - 10\\times4 = 50 + 60 + 7 + 35 + 4 - 40 = 116$\n",
    "\n",
    "For the high school grad: \n",
    "$Y = 50 + 20\\times1 + 0.07\\times100 + 0.01\\times100 = 50 + 20 + 7 + 1 = 78$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8277e6fe",
   "metadata": {},
   "source": [
    "### Question 4. \n",
    "\n",
    "I collect a set of data ($n$=100 observations) containing a single predictor and quantitative response. I then fit a linear regression model to the data, as well as a separate cubic regression \n",
    "$Y = \\beta_0 + \\beta_1X + \\beta_2X^2 + \\beta_3X^3 +\\epsilon$.\n",
    "\n",
    "a. Suppose that the true relationship between $X$ and $Y$ is linear $Y = \\beta_0 + \\beta_1X + \\epsilon$. Consider training residual sum of squares (RSS) for the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the other, would we expect thme to be the same, or is there not enough information to tell? Justify your answer. \n",
    "\n",
    "The RSS due to linear regression will be \n",
    "\n",
    "$$\\begin{align*}\n",
    "RSS &= \\sum_{i=1}^ne_i^2 \\\\ \n",
    "&= \\sum_{i=1}^n(y_i - \\hat{y}_i)^2 \\\\ \n",
    "&= \\sum_{i=1}^n((\\beta_0 + \\beta_1X_i + \\epsilon) - (\\hat{\\beta}_0 + \\hat{\\beta}_1X_i))^2 \\\\\n",
    "&= \\sum_{i=1}^n((\\beta_0 - \\hat{\\beta}_0) + (\\beta_1 - \\hat{\\beta}_1)X_i+ \\epsilon)^2 \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The RSS due to cubic regression will be \n",
    "\n",
    "$$\\begin{align*}\n",
    "RSS &= \\sum_{i=1}^ne_i^2 \\\\ \n",
    "&= \\sum_{i=1}^n(y_i - \\hat{y}_i)^2 \\\\ \n",
    "&= \\sum_{i=1}^n(\\beta_0 + \\beta_1X_i + \\epsilon) - (\\hat{\\beta}_0 + \\hat{\\beta}_1X_i + {\\beta}_2X_i^2 + {\\beta}_3X_i^3) \\\\\n",
    "&= \\sum_{i=1}^n((\\beta_0 - \\hat{\\beta}_0) + (\\beta_1 - \\hat{\\beta}_1)X_i - {\\beta}_2X_i^2 - {\\beta}_3X_i^3+ \\epsilon)^2\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "If we let $\\beta_2 = \\beta_3 = 0$ in the cubic model such that \n",
    "\n",
    "$Y = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\beta_3 X^3 + \\epsilon$ \n",
    "\n",
    "we can recover the linear model as \n",
    "\n",
    "$Y = \\beta_0 + \\beta_1 X + \\epsilon$. \n",
    "\n",
    "For the training model we use least squares to minimizes the sum of squares residuals over a larger parameter space (parameter vector $\\hat{\\beta}$ is 4 by 1 instead of 2 by 1). Minimizing over a larger set of possible solutions can't give a higher minimum RSS, and will at worst reproduce a linear fits by setting the extra coefficients to 0 and can often find a way to reduce the training RSS even further. \n",
    "\n",
    "\n",
    "## Fitting Both Models by Ordinary Least Squares\n",
    "\n",
    "We want to solve:\n",
    "\n",
    "### Linear fit\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_{\\text{lin}}\n",
    "=\n",
    "\\arg\\min_{(\\beta_0,\\;\\beta_1)}\n",
    "\\;\\|\\,y - X_{\\text{lin}}\\,\\beta\\|^2.\n",
    "$$\n",
    "\n",
    "### Cubic fit\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_{\\text{cub}}\n",
    "=\n",
    "\\arg\\min_{(\\beta_0,\\;\\beta_1,\\;\\beta_2,\\;\\beta_3)}\n",
    "\\;\\|\\,y - X_{\\text{cub}}\\,\\beta\\|^2.\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b762293b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1.],\n",
       "       [1., 2.],\n",
       "       [1., 3.],\n",
       "       [1., 4.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([1, 2, 3, 4], dtype=float)\n",
    "y = np.array([2, 3, 6, 9], dtype=float)\n",
    "\n",
    "# Constuct the design matrix for the linear model, expects one argument \n",
    "X_lin = np.column_stack((np.ones_like(x), x))\n",
    "X_lin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5fd60e7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  2.,  4.,  8.],\n",
       "       [ 1.,  3.,  9., 27.],\n",
       "       [ 1.,  4., 16., 64.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_cub = np.column_stack((np.ones_like(x), x, x**2, x**3))\n",
    "X_cub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "507fb14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RSS (linear) = 1.200000\n",
      "Training RSS (cubic) = 0.000000\n"
     ]
    }
   ],
   "source": [
    "beta_lin = np.linalg.inv(X_lin.T @ X_lin) @ (X_lin.T @ y)\n",
    "beta_cub = np.linalg.inv(X_cub.T @ X_cub) @ (X_cub.T @ y)\n",
    "\n",
    "# compute training RSS for each \n",
    "y_lin_hat = X_lin @ beta_lin\n",
    "RSS_lin = np.sum((y - y_lin_hat)**2)\n",
    "\n",
    "y_cub_hat = X_cub @ beta_cub\n",
    "RSS_cub = np.sum((y - y_cub_hat)**2)\n",
    "\n",
    "print(f\"Training RSS (linear) = {RSS_lin:.6f}\")\n",
    "print(f\"Training RSS (cubic) = {RSS_cub:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bf43d1",
   "metadata": {},
   "source": [
    "b. Answer (a) using test $RSS$ instead. \n",
    "\n",
    "Answer. When you switch to test $RSS$ there is no gurantee that the cubic model will have equal of lower $RSS$ than the linear model. In fact, the test $RSS$ can get worse if you add parameters due to $\\bold{overfitting}$.\n",
    "\n",
    "### Overfitting:\n",
    "\n",
    "When the cubic model uses extra coefficients $\\beta_2$ and $\\beta_3$ it can chase some random noise in the training set that doesn't generalize. This might reduce the training $RSS$ but might not improve predictions on new data. Sometimes the cubic model might capture the true relationship (especially if it's nonlinear!), giving it a lower $RSS$ than the linear model. But these extra parameters can cause overfitting, causing the test $RSS$ to go up for the cubic model. Therefore, unlike training $RSS$, the test $RSS$ can increase, decrease, or be approximately equal when adding parameters. \n",
    "\n",
    "### Bias-Variance Trade-Off: \n",
    "\n",
    "$Test\\:MSE(\\hat{f}) = \\underbrace{Bias^2[\\hat{f}(x_0)]}_\\text{systematic deviation} + \\underbrace{Var[\\hat{f}(x_0)]}_\\text{variance} + \\text{irreducible error}$\n",
    "\n",
    "Adding parameters like $\\beta_2, \\beta_3$ usually reduces bias but increases variance. Therefore, if the variance is greater than the bias reduction, then your test error can go up. On the training set, the variance does not cause you to do worse, since more parameters allow you to fit atleast as well, so training RSS never rises. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8a5a9c",
   "metadata": {},
   "source": [
    "c. Suppose the true relationship $X$ and $Y$ is not linear, but we don't know how far it's from linear. The training $RSS$ for linear regression, and also the training $RSS$ for cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer.\n",
    "\n",
    "Answer. \n",
    "\n",
    "Since the linear model is a special case of the cubic model when $\\beta_2=\\beta_3=0$, it can replicate the linear fit or acheive a better fit by adjusting the additional parameters. The cubic regression has atleast the same flexibility on the training set as linear regression. Therefore, regardless of how non-linear the true relatioship is $RSS_{cubic} \\leq RSS_{linear}$. If the true relationship is only a little bit non-linear, the higher order terms in $\\beta_2,\\beta_3$ might not help much. The training RSS for linear and cubic fits could be similar. The cubic model still cannot do worse on the training set because it can always revert to $\\beta_2=\\beta_3=0$. If the relatiionship is very non-linear (high curvature), then we see a lower training $RSS$ for the cubic model because the extra polynomial terms can track that curvature and reduce the sum of squared residuals. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4742ea",
   "metadata": {},
   "source": [
    "d. Answer (c) using test rather than training $RSS$. \n",
    "\n",
    "A more flexible model that includes polynomial expansions (Section 3.3.2) can better approximate a genuinely non-linear relationship if the sample size is large enough to reliably estimate the additional terms. In that scenario, the linear model often has higher bias and tends to produce a higher test RSS. However, flexibility can lead to overfitting, especially with limited or noisy data, thus increasing variance and potentially hurting test RSS. As a result, there is no guarantee that the polynomial model will outperform the linear model on new data; sometimes, the simpler linear approach yields better generalization precisely because it avoids chasing noisy patterns in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b2b5b1",
   "metadata": {},
   "source": [
    "### Question 5. \n",
    "Consider the fitted values that result from performing linear regression without an intercept. In this setting, the $ith$ fitted value takes the form\n",
    "\n",
    "$$\n",
    "\\hat{y}_i = x_i\\hat{\\beta} \n",
    "$$\n",
    "where \n",
    "$$\n",
    "\\hat{\\beta} = (\\sum_{i=1}^n x_i y_i) / (\\sum_{i'=1}^n x_{i'}^2). \n",
    "$$\n",
    "\n",
    "Show that we can write\n",
    "$$\n",
    "\\hat{y}_i = \\sum_{i'=1}^na_{i'}y_{i'}.\n",
    "$$\n",
    "\n",
    "What is $a_{i'}$?\n",
    "\n",
    "Note: We interpret this result by saying that the fitted values from linear regression are linear combinations of the response values.\n",
    "\n",
    "### Answer.\n",
    "\n",
    "Let's first re-state the question because it's a little confusing how's it's presented in the text. \n",
    "\n",
    "We have $n$ data points $(x_1, y_1),....,(x_n, y_n)$. We fit a no-intercept model $y\\approx\\beta x$. Let's variables used for the indices so there is no confusion as in the text: \n",
    "\n",
    "$\\hat{\\beta} = (\\sum_{i'=1}^n x_{i'} y_{i'}) / (\\sum_{m=1}^n x_{m}^2).$ \n",
    "\n",
    "For each data point, $i$, the fitted value is $\\hat{y}_i = x_i\\hat{\\beta}$. We want to prove $\\hat{y}_i$ is a linear combination of all the $y_{i'}$ such that \n",
    "\n",
    "$$\\hat{y}_i = \\sum_{i'=1}^na_{i'}y_{i'}.$$\n",
    "\n",
    "Let's go through this: \n",
    "$$\\begin{align*}\n",
    "\\hat{y}_i &= x_i(\\sum_{i'=1}^nx_{i'} y_{i'})/(\\sum_{m=1}x_m^2) \\\\\n",
    "&=\\frac{\\sum_{i'=1}^n x_i x_{i'}}{\\sum_{m=1}^n x_m}y_{i'} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Let $a_{i'}=\\frac{x_i x_{i'}}{\\sum_{m=1}^n x_m}$ then\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\hat{y}_i &=\\frac{\\sum_{i'=1}^n x_i x_{i'}}{\\sum_{m=1}^n x_m}y_{i'} \\\\\n",
    "&= \\sum_{i'=1}^n a_{i'}y_{i'}.\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288c1df0",
   "metadata": {},
   "source": [
    "### Question 6.\n",
    "\n",
    "Using (3.4) argue, that in the case of simple linear regressioin, the least squares line always passes through the point $(\\bar{x}, \\bar{y})$.\n",
    "\n",
    "### Answer.\n",
    "\n",
    "We know from eq. (3.4) in ISLP that \n",
    "\n",
    "$$\\hat{\\beta}_0 = \\bar{y}- \\hat{\\beta}_1\\bar{x}$$\n",
    "\n",
    "and for $x = \\bar{x}$ and $y = \\bar{y}$: \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{y} &= \\hat{\\beta}_0 + \\hat{\\beta}_1 x\\\\\n",
    "&=\\hat{\\beta}_0 + \\hat{\\beta}_1 \\bar{x} \\\\\n",
    "&=(\\bar{y} - \\hat{\\beta}_1\\bar{x}) +\\hat{\\beta}_1 \\bar{x}\\\\\n",
    "&=\\bar{y}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Therefore, the least squares line must pass through $(\\bar{x}, \\bar{y})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b0b550",
   "metadata": {},
   "source": [
    "### Question 7. \n",
    "\n",
    "It is claimed in the text that in the case of simple linear regression of $Y$ onto $X$, the $R^2$ statistic (3.17) is equal to the square of the correlation between $X$ and $Y$ (3.18). Prove that this is the case. For simplicity, you may assume $\\bar{x}=\\bar{y}=0$. \n",
    "\n",
    "\n",
    "### Answer. (first solution)\n",
    "\n",
    "Let's first define $R^2$ statistic and correlation between $X$ and $Y$. \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "R^2 &= \\frac{TSS - RSS}{TSS} \\\\\n",
    "&= \\frac{\\sum_{i=1}^n(y_i - \\bar{y})^2 -\\sum_{i=1}^n(y_i - \\hat{y}_i)^2}{\\sum_{i=1}^n(y_i - \\bar{y})^2}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$$ Cor(X, Y) = \\frac{\\sum_{i=1}^n(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^n(x_i - \\bar{x})^2}\\sqrt{\\sum_{i=1}^n(y_i - \\bar{y})^2}}.$$\n",
    "\n",
    "If $\\bar{x} = \\bar{y} = 0$ then we have \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "R^2 &= \\frac{\\sum_{i=1}^ny_i^2 - \\sum_{i=1}^n(y_i - \\hat{y}_i)^2}{\\sum_{i=1}^ny_i^2} \\\\\n",
    "&= \\frac{\\sum_{i=1}^ny_i^2 - \\sum_{i=1}^n(y_i^2 - 2\\hat{y}_iy_i +\\hat{y}_i^2)}{\\sum_{i=1}^ny_i^2}\\\\\n",
    "&=\\frac{\\sum_{i=1}^n(2\\hat{y}_iy_i -\\hat{y}_i^2)}{\\sum_{i=1}^ny_i^2}.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "We know that \n",
    "\n",
    "$$\\sum_{i=1}^n \\hat{y}_i = \\sum_{i=1}^n\\beta_i x_i=\\frac{\\sum_{i'=1}^n x_{i'}y_{i'}}{\\sum_{m=1}^n x_{m}^2}x_i$$\n",
    "\n",
    "and we will make the following argument: since $i'$ is simmeing over the same set of integers then $i=i'=m$ so we now have\n",
    "$$\\sum_{i=1}^n \\hat{y}_i =\\frac{\\sum_{i=1}^n x_{i}y_{i}}{\\sum_{i=1}^n x_{i}^2}x_i=\\sum_{i=1}^n y_i.$$\n",
    "\n",
    "Plugging this back into the $R^2$ equation\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "R^2 &=\\frac{\\sum_{i=1}^n(2\\hat{y}_i y_i -\\hat{y}_i^2)}{\\sum_{i=1}^ny_i^2}\\\\\n",
    "&=\\frac{\\sum_{i=1}^n(2\\hat{y}_i^2 - \\hat{y}_i^2)}{\\sum_{i=1}^n y_i^2}\\\\\n",
    "&=\\frac{\\sum_{i=1}^n\\hat{y}_{i}^2}{\\sum_{i=1}^ny_i^2}.\\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d369b70d",
   "metadata": {},
   "source": [
    "The correlation for $\\bar{x} = \\bar{y}=0$\n",
    "$$\n",
    "\\begin{align*}\n",
    "Cor(X, Y) &= \\frac{\\sum_{i=1}^n(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^n(x_i - \\bar{x})^2}\\sqrt{\\sum_{i=1}^n(y_i - \\bar{y})^2}} \\\\\n",
    "&=\\frac{\\sum_{i=1}^nx_i y_i}{\\sqrt{\\sum_{i=1}^nx_i^2}\\sqrt{\\sum_{i=1}^ny_i^2}}. \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Let's now square this result\n",
    "$$\n",
    "\\begin{align*}\n",
    "Corr(X, Y)^2 &= \\frac{(\\sum_{i=1}^nx_i y_i)^2}{{\\sum_{i=1}^nx_i^2}{\\sum_{i=1}^ny_i^2}}\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "And since $\\hat{y}_{i}^2 =\\sum_{i=1}^nx_i^2 \\left( \\frac{\\sum_{j=1}^n x_j y_j}{\\sum_{j=1}^n x_j^2}\\right)^2=\\frac{(\\sum_{j=1}^n x_i y_i)^2}{\\sum_{i=1}^n x_i^2}$ then we have \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "Corr(X, Y)^2 &= \\frac{(\\sum_{i=1}^nx_i y_i)^2}{{\\sum_{i=1}^nx_i^2}}\\cdot\n",
    "\\frac{1}{\\sum_{i=1}^ny_i^2}\\\\\n",
    "&=\\frac{\\sum_{i=1}^n\\hat{y}_i^2}{\\sum_{i=1}^n y_i^2}.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d292665",
   "metadata": {},
   "source": [
    "### Answer. (second solution)\n",
    "\n",
    "We know that \n",
    "\n",
    "$$R^2 = \\frac{TSS - RSS}{TSS} = \\frac{ESS}{TSS}.$$\n",
    "\n",
    "where total sum of squares (TSS) \n",
    "\n",
    "$$TSS = \\sum_{i=1}^n (y_i - \\bar{y})^2,$$\n",
    "\n",
    "residual sum of squares (RSS)\n",
    "\n",
    "$$RSS = \\sum_{i=1}^n(y_i - \\hat{y}_i)^2,$$\n",
    "\n",
    "explained sum of squares (ESS)\n",
    "\n",
    "$$ESS = \\sum_{i=1}^n(\\hat{y}_i - \\bar{y})^2.$$\n",
    "\n",
    "We want to prove that $ESS = TSS - RSS$. We can first decompose $y_i - \\bar{y}$ as\n",
    "\n",
    "$$y_i - \\bar{y} = (y_i - \\hat{y}_i) + (\\hat{y}_i - \\bar{y}).$$\n",
    "\n",
    "Since $TSS = \\sum_{i=1}^n(y_i - \\bar{y})^2$ and using our decomposition \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "(y - \\bar{y})^2 &= [(y_i - \\hat{y}_i) + (\\hat{y}_i - \\bar{y})]^2 \\\\\n",
    "&=(y_i - \\hat{y}_i)^2 + 2(y_i - \\hat{y}_i)(\\hat{y}_i - \\bar{y}) + (\\hat{y}_i - \\bar{y})^2.\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "And now let's sum over $i$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\underbrace{\\sum_{i=1}^n(y_i - \\bar{y})^2}_\\text{TSS} = \\underbrace{\\sum_{i=1}^n(y_i - \\hat{y}_i)^2}_\\text{RSS} + 2\\sum_{i=1}^n(y_i - \\hat{y}_i)(\\hat{y}_i - \\bar{y}) + \\underbrace{\\sum_{i=1}^n(\\hat{y}_i - \\bar{y})^2}_\\text{ESS}.\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "So we need to prove that $\\sum_{i=1}^n(y_i - \\hat{y}_i)(\\hat{y}_i - \\bar{y})=0$. In ordinary least squares, one can show that using partial derivatives or geometric projection arguments that $\\sum_{i=1}^n(y_i - \\hat{y}_i) = 0$.\n",
    "\n",
    "\n",
    "If we assume that the linear model is \n",
    "\n",
    "$$y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i.$$\n",
    "\n",
    "The predcition (fitted model) is \n",
    "\n",
    "$$\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1x_i.$$\n",
    "\n",
    "The residual of the $ith$ observation is \n",
    "\n",
    "$$e_i = y_i - \\hat{y}_i.$$\n",
    "\n",
    "We want to choose $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ to minimize the sum of the squares of residuals \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "f(\\hat{\\beta}_0, \\hat{\\beta}_1) &= \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 \\\\\n",
    "&=\\sum_{i=1}^n (y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1x_i))^2.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "To minimize $f$, we can take the derivative of $f$ with respect to $\\hat{\\beta}_0$ and set it equal to zero. \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial f}{\\partial \\beta_0} &=-2\\sum_{i=1}^n (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i) = 0\\\\\n",
    "&=\\sum_{i=1}^n (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i) = 0\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Recall the fitted value for the $ith$ observation is \n",
    "\n",
    "$$\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1x_i$$\n",
    "\n",
    "therefore\n",
    "\n",
    "$$\\sum_{i=1}^n (y_i - \\hat{y}_i) = 0.$$\n",
    "\n",
    "In matrix terms, if you write the regression model as:\n",
    "\n",
    "$$\n",
    "Y = X\\beta + \\varepsilon,\n",
    "$$\n",
    "\n",
    "with \\( X \\) including a column of ones (for the intercept), then the residual vector\n",
    "\n",
    "$$\n",
    "e = Y - \\hat{Y}\n",
    "$$\n",
    "\n",
    "is orthogonal to every column of \\( X \\). In particular, it is orthogonal to the column of ones, which implies:\n",
    "\n",
    "$$\n",
    "\\mathbf{1}^\\top e = \\sum_{i=1}^{n} e_i = 0.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4837dc7",
   "metadata": {},
   "source": [
    "So we have proved that \n",
    "\n",
    "$$ESS = TSS - RSS$$\n",
    "\n",
    "and so we can write $R^2$ as \n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "R^2 &= \\frac{ESS}{TSS}\\\\\n",
    "&=\\frac{\\sum_{i=1}^n(\\hat{y}_i - \\bar{y})^2}{\\sum_{i=1}^n(y_i - \\bar{y})^2}\\\\\n",
    "&=\\frac{\\sum_{i=1}^n\\hat{y}_i^2}{\\sum_{i=1}^ny_i^2}.\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Recall \n",
    "\n",
    "$$\\hat{y}_i = \\hat{\\beta}x_i$$ \n",
    "\n",
    "where $\\hat{\\beta}=\\frac{\\sum_{j=1}^n x_j y_j}{\\sum_{j=1}^n x_j^2}$.\n",
    "\n",
    "Therefore \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{y}_i^2 &= \\frac{(\\sum_{j=1}^n x_j y_j)^2}{(\\sum_{j=1}^n x_j^2)^2}\\sum_{i=1}^nx_i^2\\\\\n",
    "&= \\frac{(\\sum_{j=1}^n x_j y_j)^2}{\\sum_{j=1}^n x_j^2}.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Substituting into the equation for $R^2$\n",
    "\n",
    "$$R^2 = \\frac{\\frac{(\\sum_{j=1}^n x_j y_j)^2}{\\sum_{j=1}^n x_j^2}}{\\sum_{i=1}^n y_i^2}=\\frac{(\\sum_{j=1}^n x_j y_j)^2}{(\\sum_{j=1}^n x_j^2)(\\sum_{i=1}^n y_i^2)}.$$\n",
    "\n",
    "\n",
    "The correlation for $\\bar{x} = \\bar{y}=0$\n",
    "$$\n",
    "\\begin{align*}\n",
    "Corr(x, y) &= \\frac{\\sum_{i=1}^n(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^n(x_i - \\bar{x})^2}\\sqrt{\\sum_{i=1}^n(y_i - \\bar{y})^2}} \\\\\n",
    "&=\\frac{\\sum_{i=1}^nx_i y_i}{\\sqrt{\\sum_{i=1}^nx_i^2}\\sqrt{\\sum_{i=1}^ny_i^2}}. \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Let's now square this result\n",
    "$$\n",
    "\\begin{align*}\n",
    "Corr(x, y)^2 &= \\frac{(\\sum_{i=1}^nx_i y_i)^2}{{\\sum_{i=1}^nx_i^2}{\\sum_{i=1}^ny_i^2}}\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "These are identical expressions and so \n",
    "\n",
    "$$R^2 = Corr(x, y)^2.$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
