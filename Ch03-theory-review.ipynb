{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39306530",
   "metadata": {},
   "source": [
    "### $\\underline{\\text{Simple Linear Regression Summary:}}$\n",
    "\n",
    "Approximate **linear relationship** between $Y$ and $X$ (regress $Y$ on $X$) is:\n",
    "\n",
    "$$ Y \\approx \\beta_0 + \\beta_1 X$$ \n",
    "\n",
    "where $\\beta_0$ is the intercept and $\\beta_1$ is the slope. \n",
    "\n",
    "Use the **training data** to estimate $\\beta_0$ and $\\beta_1$:\n",
    "\n",
    "$$\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1.$$\n",
    "\n",
    "Estimate coefficients using observations pairs consisting of $(x_1, y_1), \\cdots, (x_n, y_n)$\n",
    "\n",
    "Goal is to obtain **coefficient estimates** \n",
    "$\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ such that the linear model fits the data: \n",
    "\n",
    "$$\\sum_{i=1}^n \\hat{y}_i \\approx \\sum_{i=1}^n( \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)$$\n",
    "\n",
    "\n",
    "**Residual** is difference between the observed response value and the $i$ th response value predicted by the linear model:\n",
    "$$e_i = y_i - \\hat{y}_i.$$ \n",
    "\n",
    "**Residual Sum of Squares ($RSS$)** is minimized using least squares to find the best  $\\hat{\\beta}_0$, $\\hat{\\beta}_1$ in the model. \n",
    "\n",
    "**Best Coefficient Estimate, $\\hat{\\beta_0}$, $\\hat{\\beta_1}$**:\n",
    "\n",
    "$$\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1\\bar{x}$$\n",
    "\n",
    "$$\\hat{\\beta_1} = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i -\\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2}.$$\n",
    "\n",
    "**Sample means**:\n",
    "\n",
    "$$\\bar{y} = \\frac{1}{n}\\sum_{i=1}^n y_i$$\n",
    "\n",
    "$$\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i$$\n",
    "\n",
    "Note: $\\sum_{i=1}^n y_i = n \\bar{y}$, $\\sum_{i=1}^n x_i = n \\bar{x}$,\n",
    "\n",
    "\n",
    "**True relationship**:\n",
    "\n",
    "$$Y = \\beta_0 + \\beta_1 x + \\varepsilon$$\n",
    "\n",
    "where $\\varepsilon$ is a non-zero random error or noise. \n",
    "\n",
    "\n",
    "**Population regression line**: is the best linear approx. to the true relationship between $X$ and $Y$, where coefficients $\\beta_0$, $\\beta_1$ define the populaion regression line. \n",
    "\n",
    "**Least squares line**: least squares coefficient estimates characterize this line. The more spread out the $x_i$'s the more precise the slope due the denominotor of $\\hat{\\beta_1}$, $\\sum_{i=1}^n(x_i - \\bar{x})^2$. If points are concentrated we can easily \"turn\" the slope, but if they are spread, we can more easily pin down a slope. For experimental design we prefer $x_i$'s more spread out. \n",
    "\n",
    "**Population mean**: on random variable $Y$ is $\\mu$.\n",
    "\n",
    "**Sample mean**: have access to $n$ observations from $Y$ then a reasonable estimate for $\\mu$ is \n",
    "\n",
    "$$\\hat{\\mu}=\\bar{y}=\\frac{1}{n}\\sum_{i=1}^ny_i$$\n",
    "\n",
    "where $\\hat{\\mu}$ and $\\mu$ are different but $\\hat{\\mu}$ is a good estimate of $\\mu$. The sample mean is a unbiased estimate of the population mean . If we could average a large number of estimates of $\\mu$ from a massive number of observations than averaging $\\hat{\\mu}$ is exactly $\\mu$.\n",
    "\n",
    "\n",
    "**Standard error**: is the average amount the estimate $\\hat{\\mu}$ difers from $\\mu$. The estimate shrinks with $n$\n",
    "\n",
    "$$Var(\\hat\\mu)=SE(\\hat{\\mu})^2=\\frac{\\sigma^2}{n}$$\n",
    "\n",
    "where $\\sigma^2=Var(\\varepsilon)$ is the standard deviation of each realization $y_i$ of $Y$. \n",
    "\n",
    "\n",
    "$$SE(\\hat{\\beta}_0)^2 = \\sigma^2 \\left[\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum_{i=1}^n(x_i - \\bar{x})^2} \\right]$$\n",
    "\n",
    "$$SE(\\hat{\\beta}_1)^2 = \\frac{\\sigma^2}{\\sum_{i=1}^n(x_i - \\bar{x})^2}.$$\n",
    "\n",
    "For $SE(\\hat{\\beta}_0)^2$, $SE(\\hat{\\beta}_1)^2$ to be valid the error $\\varepsilon_i$ for each observation have common variance $\\sigma^2$ and uncorrelated. \n",
    "\n",
    "**Leverage**: $SE(\\hat{\\beta}_1)$ is smaller if $x_i$ is more spread out, then there is more leverage to estimate the slope. \n",
    "\n",
    "**Residual standard error ($RSE$)**: $\\sigma^2$ is usually not known but can be estimated from data \n",
    "\n",
    "$$RSE = \\sqrt{\\frac{RSS}{n-p-1}}$$\n",
    "\n",
    "where the $RSS = \\sum_{i=1}^n(y_i - \\hat{y_i})^2$. The $RSE$ esimates the standard deviation of $\\varepsilon$ and is the average amount the response deviates from the true regression line.  For example if $RSE=3.26$ then the actual sales in each market deviate from the true regression line by $3,260$ units on average. If the mean sales over all markets is approximately $14,000$ units, then the percentage error is $\\frac{3260}{14000}=23\\%$. The RSE is a measure of lack of fit of the model to the data. When $\\sigma^2$ is estimated then we write $\\hat{SE}(\\hat{\\beta}_1)$ for standard error. \n",
    "\n",
    "**Confidence intervals** are a range of values s.t. with $95\\%$ probability this range contains the true unknown value of the parameter: \n",
    "\n",
    "$$\\hat{\\beta}_1 \\pm 2 \\cdot SE(\\hat{\\beta}_1)$$\n",
    "\n",
    "where each confidence interval is drawn from a new population sample.\n",
    "\n",
    "The $95\\%$ confidence interval property we take repeated samples and construct confidence intervals for each sample, then $95\\%$ of the intervals will contain the true unknown  value of the parameter \n",
    "\n",
    "$$[\\hat{\\beta}_1 - 2 \\cdot SE(\\hat{\\beta}_1), \\hat{\\beta}_1 - 2 \\cdot SE(\\hat{\\beta}_1)].$$\n",
    "\n",
    "**Hypothesis tests**: the *null hypothesis* is $H_0:$ there is no relationship between $X$ and $Y$ or $\\beta_1=0$. The *alternative hypothesis* is $H_a:$ there is a relationship between $X$ and $Y$ or $\\beta_1\\neq0$.\n",
    "\n",
    "**T-statistic**: is used to test the null hypothesis, or is $\\hat{\\beta_1}$ sufficient far from $0$ such that $\\beta_1$ is non-zero where it's the number of standard deviations $\\hat{\\beta}$ is from $0$: \n",
    "\n",
    "$$\\hat{t}=\\frac{\\hat{\\beta}_1 - 0}{SE(\\hat{\\beta}_1)}.$$\n",
    "\n",
    "**P-value**: a small p-value infers there is a relationship between $X$ and $Y$ and so we can reject the null hypothesis. The typical cutoff is $0.05$ or $5\\%$ to reject the null hypothesis. \n",
    "\n",
    "**$R^2$ stat**: it's not always clear what a good $RSE$ is and so instead use $R^2$ \n",
    "\n",
    "$$R^2=\\frac{TSS - RSS}{TSS} = 1 - \\frac{RSS}{TSS}$$\n",
    "\n",
    "where $TSS=\\sum(y_i - \\bar{y})^2$ is called the *total sum of squares* and measures the total variance in response $Y$, is the variability in response before regresssion is performed. \n",
    "\n",
    "$R^2$ measures the proportion of variablility in $Y$ that can be explained using $X$, is a measure of the linear relationship between $X$ and $Y$. \n",
    "\n",
    " - $R^2\\approx 1$ means a large proportion of variability in response explained by regression. \n",
    " - $R^2\\approx 0$ regression doesn't explain variability in the response; because the linear model is wrong; or the error variance $\\sigma^2$ is large; or both. \n",
    " - A $R^2=0.612$ tells us that using the predictor, $p$, we reduced the variance in the response, $y$,  by $61\\%$. Therefore, $p$, is a strong predictor of the response, $y$. \n",
    "\n",
    "**Correlation** measures the relation between $Y$ and $X$. \n",
    "\n",
    "$$Cor(X, Y) = \\frac{\\sum_{i=1}^n(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^n(x_i - \\bar{x})^2}\\sqrt{\\sum_{i=1}^n(y_i - \\bar{y})^2}}$$\n",
    "\n",
    "where $r = Cor(X, Y)$ and for simple linear regression $R^2 = r^2$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a055fc4",
   "metadata": {},
   "source": [
    "---\n",
    "### $\\underline{\\text{Multiple Regression Summary:}}$\n",
    "\n",
    "If we have $p$ distinct predictors such that \n",
    "\n",
    "$$y = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p + \\varepsilon$$\n",
    "\n",
    "where $\\beta_j$ is the average effect of $Y$ on one unit increase in $X$, holding other predictors fixed. \n",
    "\n",
    "We use the training data to estimate the regression coefficients\n",
    "\n",
    "$$\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_1 + \\cdots + \\hat{\\beta}_p X_p$$\n",
    "\n",
    "and we can choose $\\hat{\\beta}_0$, $\\hat{\\beta}_1$, $\\cdots$, $\\hat{\\beta}_p$ to minimize the residual sum of squares $RSS$\n",
    "\n",
    "$$\\begin{align*}\n",
    "RSS &= \\sum_{i=1}^n(y_i - \\hat{y}_i)^2 \\\\\n",
    "&=\\sum_{i=1}^n[y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 X_1 + \\cdots + \\hat{\\beta}_p X_p)]^2.\n",
    "\\end{align*}$$\n",
    "\n",
    "In multiple regression, a predictor like newspaper ads is not directly related to sales but higher values for the newspaper ads associated is associated with higher sales and so newspaper ads serve as a **surrogate** for another predictor like radio ads so newspaper ads get credit for their association between radio and sales. \n",
    "\n",
    "**Hypothesis test**: for mulitiple regression the null hypothesis is $H_0: \\beta_1 = \\beta_2 = \\cdots = \\beta_p = 0$. The alternative hypothesis is \n",
    "$H_a: \\beta_j \\neq 0$, or atleast one coeffiecient is non-zero.\n",
    "\n",
    "**F-statistic**: for multiple regression this is the hypothesis test \n",
    "\n",
    "$$F = \\frac{(TSS - RSS)/p}{RSS/(n-p-1)}$$\n",
    "\n",
    "where $TSS=\\sum(y_i - \\bar{y})^2$, $RSS=\\sum(y_i - \\hat{y}_i)^2$. \n",
    "\n",
    "- If $H_0$ is true then $F-stat \\sim 1$.\n",
    "\n",
    "- If $H_a$ is true then $\\mathbb{E}[(TSS-RSS)/p]>\\sigma^2\\rightarrow F-stat>1$\n",
    "\n",
    "- If $n$ is large and the $F-stat$ is a little bigger than 1 this can provide evidence against $H_0$ but if $n$ is small, $H_0$ is true the $\\varepsilon_i$ errors have a normal distribution, $F-stat$ follows a $F-dist$.\n",
    "\n",
    "- $F-stat$ is good if $p$ is small; if $p>n$ then can't apply the $F-stat$. \n",
    "\n",
    "**Partial effect of adding a variable**: \n",
    "\n",
    "Testing the null hypothesis on a subset of coefficients \n",
    "\n",
    "$$H_0: \\beta_{p-q+1} = \\beta_{p-q+2}= \\cdots = \\beta_p = 0$$\n",
    "\n",
    "and fit a new model that uses all variables except the subset, then $RSS=RSS_0$\n",
    "\n",
    "$$F = \\frac{(RSS_0 - RSS)/q}{RSS/(n-p-1)}$$\n",
    "\n",
    "where this is similar to the $F-test$ that omits single variable from the model while keeping the others to observe the partial effect of adding a variable.\n",
    "\n",
    "**Variable selection**: determines what predictors associated with response to fit single model using only those predictors. Examples of automated methods to decide on best models are: forward selection, backward selection, and mixed selection (covered in later chapters). \n",
    "\n",
    "**$R^2 for multiple regression model** $R^2 \\sim 1$ means that the model explain a large portion of variance in the response variable. The $R^2$ icnreases as more variables are added because adding a variable decreases the residual sum of squares on training data\n",
    "\n",
    "$$RSE = \\sqrt{\\frac{RSS}{n - p -1}}$$\n",
    "\n",
    "where models with more variable can have a higher $RSE$ if a decrease in $ RSS$ is small relative to an increase in $p$. \n",
    "\n",
    "**Interaction effect(synergy)**: is between predictors where combining predictors results in an overestimated response then suing a single predictors. \n",
    "\n",
    "\n",
    "**Prediction**: there are 3 types of prediction uncertainty\n",
    "\n",
    "- *Reducible error*: inaccuracy in the coefficient estimates are due to this type of error. Use confidence intervals to see how close $\\hat{y}=\\hat{\\beta_0} + \\hat{\\beta_1}x_1 + \\cdots + \\hat{\\beta_p}x_p$ is to $f(X)=\\beta_0 + \\beta_1x_1 + \\cdots + \\beta_px_p$. \n",
    "- *Model bias*: is an additional source of reducible error. \n",
    "- *Irreducible error*: true values of $\\beta_1,\\cdots,\\beta_p$ can't be predicted perfectly because of random error $\\varepsilon$. Use prediction intervals to quantify how far $y$ is from $\\hat{y}$.\n",
    "\n",
    "**Confidence intervals vs. prediction intervals**: the $95\\%$ confidence interval quantifies uncertainty around average sales over large number of cities (repeated samples drawn). $95\\%$ of these intervals will contain the true value of $f(X)$. The $95\\%$ prediction interval quantifies uncertainty in sales for a particular city. Both intervals are centered at the same value but the prediction interval is wider because there is more uncertainty about sales for a given city versus average sales over many locations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b99a39",
   "metadata": {},
   "source": [
    "---\n",
    "### $\\underline{\\text{Qualitative Predictors:}}$ \n",
    "\n",
    "Suppose that we wish to investigate differences in credit card balance between those who own a house and those who don’t, ignoring the other variables for the moment. If a qualitative predictor (also known as a *factor*) only has two levels, or possible values, then incorporating it into a regression model is very simple. \n",
    "\n",
    "We simply create an **indicator** or **dummy variable** that takes on two possible numerical values. For example, based on the variable `Own`, we can define a new variable:\n",
    "\n",
    "$$\n",
    "x_i = \n",
    "\\begin{cases}\n",
    "1 & \\text{if the $i$th person owns a house} \\\\\\\\\n",
    "0 & \\text{if the $i$th person does not own a house}\n",
    "\\end{cases}\n",
    "\\tag{1}\n",
    "$$\n",
    "\n",
    "We then use this variable as a predictor in the regression model:\n",
    "\n",
    "$$\n",
    "y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i =\n",
    "\\begin{cases}\n",
    "\\beta_0 + \\beta_1 + \\varepsilon_i & \\text{if the $i$th person owns a house} \\\\\\\\\n",
    "\\beta_0 + \\varepsilon_i & \\text{if the $i$th person does not}\n",
    "\\end{cases}\n",
    "\\tag{2}\n",
    "$$\n",
    "\n",
    "Here, $\\beta_0$ represents the **average credit card balance among non-owners**, $\\beta_0 + \\beta_1$ is the **average balance among owners**, and $\\beta_1$ is the **average difference in balance between owners and non-owners**.\n",
    "\n",
    "When a qualitative predictor has more than two levels, a single dummy\n",
    "variable cannot represent all possible values. In this situation, we can create\n",
    "additional dummy variables. For example, for the **region** variable we create\n",
    "two dummy variables. The first could be\n",
    "\n",
    "$$\n",
    "x_{i1} = \n",
    "\\begin{cases}\n",
    "1 & \\text{if the $i$th person is from the South} \\\\\\\\\n",
    "0 & \\text{if the $i$th person is not from the South,}\n",
    "\\end{cases}\n",
    "\\tag{3}\n",
    "$$\n",
    "\n",
    "and the second could be \n",
    "\n",
    "$$\n",
    "x_{i2} = \n",
    "\\begin{cases}\n",
    "1 & \\text{if the $i$th person is from the West} \\\\\\\\\n",
    "0 & \\text{if the $i$th person is not from the West,}\n",
    "\\end{cases}\n",
    "\\tag{4}\n",
    "$$\n",
    "\n",
    "Then both of these variables can be used in the regression equation to obtain the model:\n",
    "\n",
    "$$\n",
    "y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\varepsilon_i =\n",
    "\\begin{cases}\n",
    "\\beta_0 + \\beta_1 + \\varepsilon_i & \\text{if the $i$th person is from the South} \\\\\\\\\n",
    "\\beta_0 + \\beta_2 + \\varepsilon_i & \\text{if the $i$th person is from the West} \\\\\\\\\n",
    "\\beta_0 + \\varepsilon_i & \\text{if the $i$th person is from the East}\n",
    "\\end{cases}\n",
    "\\tag{5}\n",
    "$$\n",
    "\n",
    "Here:\n",
    "\n",
    "- $\\beta_0$ represents the **average credit card balance** for individuals from the **East**.\n",
    "- $\\beta_1$ is the **difference** in average balance between people from the **South** and those from the East.\n",
    "- $\\beta_2$ is the **difference** between those from the **West** and the East.\n",
    "\n",
    "There will always be **one fewer dummy variable than the number of levels**. The level with no dummy variable—**East** in this example—is called the **baseline**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf3ffe2",
   "metadata": {},
   "source": [
    "---\n",
    "### $\\underline{\\text{Extensions of the Linear Model}}$\n",
    "\n",
    "The linear model assumes the *additive assumption*, which roughly the association between predictor $X_j$ and response $Y$ does not depend on other predictors. It also depends on the *linearity assumption* such that the change in $Y$ associated with a one-unit change in $Y_j$ is constant regardless of the value of $X_j$. \n",
    "\n",
    "**Interaction effect**: \n",
    "\n",
    "One way of extending this model is to include a **third predictor**, called an **interaction term**, which is constructed by multiplying $X_1$ and $X_2$. This gives the model:\n",
    "\n",
    "$$\n",
    "Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_1 X_2 + \\varepsilon\n",
    "\\tag{3.31}\n",
    "$$\n",
    "\n",
    "How does the inclusion of this interaction term **relax the additive assumption**?\n",
    "\n",
    "Note that equation (3.31) can be rewritten as:\n",
    "\n",
    "$$\n",
    "Y = \\beta_0 + (\\beta_1 + \\beta_3 X_2) X_1 + \\beta_2 X_2 + \\varepsilon\n",
    "\\tag{3.32}\n",
    "$$\n",
    "\n",
    "Letting:\n",
    "\n",
    "$$\n",
    "\\tilde{\\beta}_1 = \\beta_1 + \\beta_3 X_2,\n",
    "$$\n",
    "\n",
    "we can see that $\\tilde{\\beta}_1$ is now a **function of $X_2$**. This means that the **effect of $X_1$ on $Y$** depends on the value of $X_2$ — the association between $X_1$ and $Y$ is no longer constant.\n",
    "\n",
    "Similarly, the effect of $X_2$ on $Y$ also depends on the value of $X_1$. Therefore, the model allows for **interaction between the predictors**, relaxing the assumption that their effects are strictly additive.\n",
    "\n",
    "We now return to the **Advertising** example. A linear model that uses `radio`, `TV`, and an **interaction** between the two to predict `sales` takes the form:\n",
    "\n",
    "$$\n",
    "\\text{sales} = \\beta_0 + \\beta_1 \\cdot \\text{TV} + \\beta_2 \\cdot \\text{radio} + \\beta_3 \\cdot (\\text{radio} \\times \\text{TV}) + \\varepsilon\n",
    "$$\n",
    "\n",
    "This can be rewritten as:\n",
    "\n",
    "$$\n",
    "\\text{sales} = \\beta_0 + (\\beta_1 + \\beta_3 \\cdot \\text{radio}) \\cdot \\text{TV} + \\beta_2 \\cdot \\text{radio} + \\varepsilon\n",
    "\\tag{3.33}\n",
    "$$\n",
    "\n",
    "From this form, we can interpret $\\beta_3$ as the **change in the effectiveness of TV advertising** associated with a **one-unit increase in radio advertising** — or vice versa.\n",
    "\n",
    "That is, the **marginal effect** of TV on sales depends on the level of radio advertising, and vice versa, due to the interaction term. This captures a **non-additive** relationship between the two predictors.\n",
    "\n",
    "**Nonlinear relationships**: \n",
    "\n",
    "Polynomial regression extends the linear model to non-linear relatioships. By plotting the data we can observe a non-linear relationship between the mpg and horsepower (response and predictor), the quadratic shape suggests the model is \n",
    "\n",
    "$$mpg = \\beta_0 + \\beta_1 \\times horsepower + \\beta_2 \\times horsepower^2 + \\varepsilon.$$\n",
    "\n",
    "This involves prediction $mpg$ using nonlinear function of $horsepower$ but this is still a linear model since $X_1 = horsepower$, $X_2=horsepower^2$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec5e764",
   "metadata": {},
   "source": [
    "### $\\underline{\\text{Problems with Linear Regression:}}$\n",
    "\n",
    "1. **Non-linear data**: linear regression assumes a straight-line relationship between predictors and the response variable\n",
    "    - use *residual plots*, $\\hat{y}$ vs. $y_i - \\hat{y}_i$ to identify non-linearity, this plot shows no non-linearity ideally. \n",
    "    - if the residual plot shows a non-linear pattern a simple solution is to use a non-linear transformation $\\log x, \\sqrt{x}, x^2$ in the model. \n",
    "\n",
    "2. **Correlation of error terms**: the standard errors for the estimates are based on assumption that noise $\\varepsilon_1, \\varepsilon_2, \\cdots, \\varepsilon_n$ are uncorrelated. \n",
    "    - if the $\\varepsilon_i$'s are correlated then the estimated standard error will underestimate the true standard error, confidence interval will be narrower, p-values smaller, and result in the erroneous conclusion that the parameter is statistically significant. \n",
    "\n",
    "3. **Non-constant variance of error terms**: usually the error terms have constant variance $Var(\\varepsilon_i)=\\sigma^2$. \n",
    "    - this non-constant variance is seen as a funnel shape in the residual plot. \n",
    "    - one solution is to transform $Y$ using a concave function like $logY$ or $\\sqrt{Y}$ causing a larger shrinkage of the larger responses. \n",
    "    - another solution is the $i$th response could be the average of $n_i$ raw obseravtions if each observation is uncorrelated with variance $\\sigma^2$ then the average variance is $\\sigma_i^2=\\sigma^2/n_i$ and fit using a weighted least squares where the weights are proportional to the increase variances or $w_i = n_i$. \n",
    "\n",
    "4. **Outliers**: point for which $y_i$ is far from the value predicted by the model. \n",
    "    - can use the residuals plot to identify the outliers\n",
    "    - can use plot the studentized residuals, which divides each residual $e_i$ by estimated standard error and the observations where studentized residuals > $|3|$ are possible outliers\n",
    "    - outliers can be dut to error in data collection and can be remedied by removing, can also mean missing predictor\n",
    "\n",
    "5. **High leverage points**: when an observation has an unusually high value for $x_i$ such that the predictor value for an observation is large relative to other observations.\n",
    "    - removing high leverage points can impact fit, and identify these points using leverage statistic \n",
    "    \n",
    "    $$h_i = \\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{\\sum_{i'=1}^n(x_{i'} - \\bar{x})^2}$$ \n",
    "\n",
    "    which can be used for simple regression and as $h_i$ increases with distance of $x_i$ from $\\bar{x}$ and statistic is always between \\frac{1}{n} and 1. \n",
    "    - the average leverage for all observations $(p + 1)/n$ so if the observation has a leverage stat $>> (p+1)/n$ then suspect a high leverage point. \n",
    "    - high leverage and high studentized residual is a bad combination! \n",
    "\n",
    "6. **Collinearity**: when predictors are highly correlated with each other (closely related) can be an issue for regression since it's difficult to separate individual effects on the response. \n",
    "    - collinearity reduces the accuracy of estimats, causing the standard error for $\\beta_j$ to grow. \n",
    "    - collinearity reduces the $t-stat$ so may fail to reject $H_0:\\beta_j=0$. \n",
    "    - can detect paris of highly correlated values with *correlation matrix*. \n",
    "    - *multicollinearity* is collinearity between 3 or more variables and so we have to use **variance inflation factor(VIF)**. \n",
    "\n",
    "    $$VIF(\\hat{\\beta_j}) = \\frac{1}{1 - R_{X_j|X_{_j}}^2}$$\n",
    "\n",
    "    - $R_{X_j|X_{_j}}^2$ is $R^2$ from regressing $X_j$ onto all other predictors, VIF = 1 means no multicollinearity, VIF = 5-10 means collinearity. \n",
    "    - one solution is to drop the problematic variable from regression since it's redundant dut to collinearity. \n",
    "    - another solution is to combine collinear variables into single predictor. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81dd6f06",
   "metadata": {},
   "source": [
    "---\n",
    "### $\\underline{\\text{Cubic vs. Linear Model:}}$\n",
    "\n",
    "\n",
    "For the training model we use least squares to minimizes the sum of squares residuals over a larger parameter space (parameter vector $\\hat{\\beta}$ is 4 by 1 instead of 2 by 1). Minimizing over a larger set of possible solutions can't give a higher minimum RSS, and will at worst reproduce a linear fits by setting the extra coefficients to 0 and can often find a way to reduce the training RSS even further. \n",
    "\n",
    "### Linear Model Fit\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_{\\text{lin}}\n",
    "= \n",
    "\\begin{pmatrix}\n",
    "\\beta_0\\\\\n",
    "\\beta_1\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "minimize\n",
    "\n",
    "$$\n",
    "\\mathrm{RSS}_{\\text{lin}}\n",
    "= \\sum_{i=1}^n \n",
    "\\Bigl[\n",
    "    y_i \\;-\\; (\\beta_0 + \\beta_1\\,x_i)\n",
    "\\Bigr]^2.\n",
    "$$\n",
    "\n",
    "In matrix form, let\n",
    "\n",
    "$$\n",
    "X_{\\text{lin}}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "1 & x_1\\\\\n",
    "1 & x_2\\\\\n",
    "\\vdots & \\vdots\\\\\n",
    "1 & x_n\n",
    "\\end{pmatrix},\n",
    "\\quad\n",
    "y\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "y_1\\\\\n",
    "y_2\\\\\n",
    "\\vdots\\\\\n",
    "y_n\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "Then\n",
    "\n",
    "$$\n",
    "\\mathrm{RSS}_{\\text{lin}}\n",
    "= \n",
    "\\min_{\\beta_0,\\;\\beta_1}\n",
    "\\;\\bigl\\|\\,y \\;-\\; X_{\\text{lin}}\\,\\beta\\bigr\\|^2.\n",
    "$$\n",
    "\n",
    "\n",
    "### Cubic Model Fit\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_{\\text{cub}}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "\\beta_0\\\\\n",
    "\\beta_1\\\\\n",
    "\\beta_2\\\\\n",
    "\\beta_3\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "minimize\n",
    "\n",
    "$$\n",
    "\\mathrm{RSS}_{\\text{cub}}\n",
    "= \\sum_{i=1}^n \n",
    "\\Bigl[\n",
    "    y_i \n",
    "    \\;-\\; \n",
    "    (\\beta_0 + \\beta_1\\,x_i + \\beta_2\\,x_i^2 + \\beta_3\\,x_i^3)\n",
    "\\Bigr]^2.\n",
    "$$\n",
    "\n",
    "In matrix form, let\n",
    "\n",
    "$$\n",
    "X_{\\text{cub}}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "1 & x_1 & x_1^2 & x_1^3\\\\\n",
    "1 & x_2 & x_2^2 & x_2^3\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots\\\\\n",
    "1 & x_n & x_n^2 & x_n^3\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "Then\n",
    "\n",
    "$$\n",
    "\\mathrm{RSS}_{\\text{cub}}\n",
    "= \n",
    "\\min_{\\beta_0,\\;\\beta_1,\\;\\beta_2,\\;\\beta_3}\n",
    "\\;\\bigl\\|\\,y \\;-\\; X_{\\text{cub}}\\,\\beta\\bigr\\|^2.\n",
    "$$\n",
    "\n",
    "### Minimizing Over a Larger Set\n",
    "\n",
    "In least squares, \n",
    "$$\n",
    "\\hat{\\beta}_{\\text{cub}}\n",
    "\\text{ is chosen to minimize }\n",
    "\\|\\,y - X_{\\text{cub}}\\,\\beta\\|^2\n",
    "\\text{ over }\n",
    "\\mathbb{R}^4.\n",
    "$$\n",
    "\n",
    "Because $\\mathbb{R}^2$ (linear parameters) is embedded in $\\mathbb{R}^4$ (cubic parameters) via\n",
    "$$\n",
    "(\\beta_0,\\,\\beta_1)\n",
    "\\;\\mapsto\\;\n",
    "(\\beta_0,\\,\\beta_1,\\,0,\\,0),\n",
    "$$\n",
    "the minimum over the bigger space can only be smaller or equal to the minimum over the smaller space. Symbolically:\n",
    "\n",
    "$$\n",
    "\\min_{(\\beta_0,\\beta_1,\\beta_2,\\beta_3)\\in \\mathbb{R}^4}\n",
    "\\;\\bigl\\|\\,y - X_{\\text{cub}}\\,\\beta\\bigr\\|^2\n",
    "\\;\\;\\le\\;\\;\n",
    "\\min_{(\\beta_0,\\beta_1)\\in \\mathbb{R}^2}\n",
    "\\;\\bigl\\|\\,y - X_{\\text{cub}}(\\beta_0,\\beta_1,0,0)\\bigr\\|^2.\n",
    "$$\n",
    "\n",
    "But\n",
    "$$\n",
    "X_{\\text{cub}}(\\beta_0,\\,\\beta_1,\\,0,\\,0)\n",
    "= \n",
    "X_{\\text{lin}}(\\beta_0,\\,\\beta_1).\n",
    "$$\n",
    "\n",
    "For any $(\\beta_0,\\beta_1)$, the vector of fitted values in the cubic model collapses to the linear model’s fitted values if $\\beta_2=0,\\;\\beta_3=0$. Hence:\n",
    "$$\n",
    "\\bigl\\|\\,y - X_{\\text{cub}}(\\beta_0,\\beta_1,0,0)\\bigr\\|^2\n",
    "=\n",
    "\\bigl\\|\\,y - X_{\\text{lin}}(\\beta_0,\\beta_1)\\bigr\\|^2.\n",
    "$$\n",
    "\n",
    "Therefore,\n",
    "$$\n",
    "\\min_{\\beta_0,\\beta_1,\\beta_2,\\beta_3}\n",
    "\\bigl\\|\\,y - X_{\\text{cub}}\\,\\beta\\bigr\\|^2\n",
    "\\;\\;\\le\\;\\;\n",
    "\\min_{\\beta_0,\\beta_1}\n",
    "\\bigl\\|\\,y - X_{\\text{lin}}(\\beta_0,\\beta_1)\\bigr\\|^2.\n",
    "$$\n",
    "\n",
    "By definition, these minima are precisely the **training RSS** for each model:\n",
    "\n",
    "$$\n",
    "\\mathrm{RSS}_{\\text{cub}}\n",
    "\\;\\le\\;\n",
    "\\mathrm{RSS}_{\\text{lin}}.\n",
    "$$\n",
    "\n",
    "\n",
    "Let’s use 4 observations:\n",
    "\n",
    "$$\n",
    "x = [\\,1,\\;2,\\;3,\\;4\\,], \n",
    "\\quad\n",
    "y = [\\,2,\\;3,\\;6,\\;9\\,].\n",
    "$$\n",
    "\n",
    "So we have \\(n=4\\) data points:\n",
    "$$\n",
    "(x_1,\\,y_1)=(1,\\,2), \n",
    "\\quad\n",
    "(x_2,\\,y_2)=(2,\\,3), \n",
    "\\quad\n",
    "(x_3,\\,y_3)=(3,\\,6), \n",
    "\\quad\n",
    "(x_4,\\,y_4)=(4,\\,9).\n",
    "$$\n",
    "\n",
    "### Design Matrices\n",
    "\n",
    "The linear model is\n",
    "\n",
    "$$\n",
    "Y = \\beta_0 \\;+\\; \\beta_1\\,X \\;+\\; \\varepsilon.\n",
    "$$\n",
    "\n",
    "In matrix form, we write $X_{\\text{lin}}$ as\n",
    "\n",
    "$$\n",
    "X_{\\text{lin}}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "1 & 1\\\\\n",
    "1 & 2\\\\\n",
    "1 & 3\\\\\n",
    "1 & 4\n",
    "\\end{pmatrix}_{4\\times 2}.\n",
    "$$\n",
    "\n",
    "- The first column is all 1’s (for $\\beta_0$).\n",
    "- The second column is $x_i$.\n",
    "\n",
    "\n",
    "The cubic model is\n",
    "\n",
    "$$\n",
    "Y = \\beta_0 \\;+\\; \\beta_1\\,X \\;+\\; \\beta_2\\,X^2 \\;+\\; \\beta_3\\,X^3 \\;+\\; \\varepsilon.\n",
    "$$\n",
    "\n",
    "Hence $X_{\\text{cub}}$ is\n",
    "\n",
    "$$\n",
    "X_{\\text{cub}}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "1 & 1 & 1^2 & 1^3\\\\\n",
    "1 & 2 & 2^2 & 2^3\\\\\n",
    "1 & 3 & 3^2 & 3^3\\\\\n",
    "1 & 4 & 4^2 & 4^3\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "1 & 1 & 1 & 1\\\\\n",
    "1 & 2 & 4 & 8\\\\\n",
    "1 & 3 & 9 & 27\\\\\n",
    "1 & 4 & 16 & 64\n",
    "\\end{pmatrix}_{4\\times 4}.\n",
    "$$\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e477d1d9",
   "metadata": {},
   "source": [
    "---\n",
    "### $\\underline{\\text{Overfitting:}}$\n",
    "\n",
    "When you switch to test $RSS$ there is no gurantee that the cubic model will have equal of lower $RSS$ than the linear model. In fact, the test $RSS$ can get worse if you add parameters due to $\\bold{overfitting}$.\n",
    "\n",
    "\n",
    "When the cubic model uses extra coefficients $\\beta_2$ and $\\beta_3$ it can chase some random noise in the training set that doesn't generalize. This might reduce the training $RSS$ but might not improve predictions on new data. Sometimes the cubic model might capture the true relationship (especially if it's nonlinear!), giving it a lower $RSS$ than the linear model. But these extra parameters can cause overfitting, causing the test $RSS$ to go up for the cubic model. Therefore, unlike training $RSS$, the test $RSS$ can increase, decrease, or be approximately equal when adding parameters. \n",
    "\n",
    "---\n",
    "### $\\underline{\\text{Bias-Variance Trade-Off:}}$ \n",
    "The expected test MSE for $x_0$ is \n",
    "\n",
    "$$Test\\:MSE(\\hat{f}) =\\mathbb{E}(y_0 - \\hat{f}(x_0))^2=\\underbrace{Bias^2[\\hat{f}(x_0)]}_\\text{systematic deviation} + \\underbrace{Var[\\hat{f}(x_0)]}_\\text{variance} + \\text{irreducible error}$$\n",
    "\n",
    "where $\\text{irreducible error}=Var(\\varepsilon).$ Adding parameters like $\\beta_2, \\beta_3$ usually reduces bias but increases variance. Therefore, if the variance is greater than the bias reduction, then your test error can go up. On the training set, the variance does not cause you to do worse, since more parameters allow you to fit atleast as well, so training RSS never rises. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c7885f",
   "metadata": {},
   "source": [
    "---\n",
    "### $\\underline{\\text{Lecture Questions:}}$\n",
    "\n",
    "### Question 1. \n",
    "Run linear regression with a slope estimate of 0.5 and with an estimate standard error of 0.2. What's the largest value of $\\beta$ for which we would NOT reject the null hypothesis (assume a normal approximation to the distribution using $5\\%$ significance level for a 2-sided test; need the critical value for standard normal approximation assumming a large sample assumption)\n",
    "\n",
    "### Answer 1. \n",
    "\n",
    "This is a hypothesis test where we are interested in whethere the parameter is significantly higher or lower than a hypothesized value. We split the $5\\%$ significance level evenly across both tails of the distribution - $2.5\\%$ in the lower tail and $2.5\\%$ in the upper tail.Using a standard normal approximation (Z-test) and assuming a large sample (so the Central Limit Theorem applies), we want to find the Z-values that correspond to cutoffs at $2.5\\%$ in each tail which are quantiles at 0.025 and 0.975 where the values are \n",
    "\n",
    "$z_{0.025}=-1.96$ (lower critical value)\n",
    "\n",
    "$z_{0.975}=+1.96$ (upper critical value)\n",
    "\n",
    "and so the critical region is \n",
    "\n",
    "$$|z| > 1.96.$$\n",
    "\n",
    "We need to use the *z-score*, which tells us how many standard deviations a data point (or estimate) is from the mean (hypothesized value). The equation for the z-score is \n",
    "\n",
    "$$z = \\frac{\\hat{\\beta} - \\beta_0}{SE}$$ \n",
    "\n",
    "which is the same as the t-stat, except the z-score assumes the sample size is large and you use the standard normal distribution. Also, the z-score is used if the population standard deviation is known (rare in practice). The t-stat assumes a small sample size, estimating the standard deviation from data, errors are assumed to follow a normal distribution, and we use a student's t-distribution, which has heavier tails to account for additional uncertainty. Back to our question\n",
    "\n",
    "$$z = \\frac{\\hat{\\beta} - \\beta_0}{SE}\\in[-1.96, 1.96]$$\n",
    "\n",
    "and solve \n",
    "\n",
    "$$-1.96 \\leq \\frac{0.5 - \\beta}{0.2} \\leq 1.96$$\n",
    "\n",
    "and solving for $\\beta$\n",
    "\n",
    "$$0.108 \\leq \\beta \\leq 0.892$$\n",
    "\n",
    "and so the largest value of $\\beta$ for which we would **not** reject the null hypothesis is 0.892. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b767a6d2",
   "metadata": {},
   "source": [
    "### Question 2. \n",
    "T or F? Estimate $\\hat{\\beta_1}$ in linear regression that controls for many variables (many predictors in addition to $x_1$) is a more reliable measure of the casual relationship then $\\hat{\\beta_1}$ from a univariate regression? \n",
    "\n",
    "### Answer 2. \n",
    "\n",
    "Consider two regression models for estimating the effect of $x_1$ on $Y$:\n",
    "\n",
    "1. **Simple regression**:\n",
    "\n",
    "   $$\n",
    "   Y = \\beta_0 + \\beta_1\\,x_1 + \\varepsilon,\\qquad \\hat\\beta_1^{(S)}\n",
    "   $$\n",
    "\n",
    "   This slope $\\hat\\beta_1^{(S)}$ ignores other predictors and thus can suffer from **omitted-variable bias** whenever a true predictor—say $x_2$—is correlated both with $x_1$ and with $Y$.  In that case, $\\hat\\beta_1^{(S)}$ “soaks up” part of $x_2$’s effect, so its expectation no longer equals the true $\\beta_1$. Therefore, $\\mathbb{E}[\\hat{\\beta}_1] \\neq \\beta_1$.\n",
    "\n",
    "2. **Multiple regression**:\n",
    "\n",
    "   $$\n",
    "   Y = \\beta_0 + \\beta_1\\,x_1 + \\beta_2\\,x_2 + \\cdots + \\beta_p\\,x_p + \\varepsilon,\\qquad \\hat\\beta_1^{(M)}\n",
    "   $$\n",
    "\n",
    "   Here $\\hat\\beta_1^{(M)}$ is the **partial slope** of $x_1$ “holding all other $x_j$ fixed.”  If each added predictor truly belongs—i.e., is associated with both $x_1$ and $Y$—then including it removes omitted-variable bias, so\n",
    "\n",
    "   $$\n",
    "   E\\bigl[\\hat\\beta_1^{(M)}\\bigr] \\;=\\; \\beta_1.\n",
    "   $$\n",
    "\n",
    "However, each extra predictor also increases the **variance** of $\\hat\\beta_1^{(M)}$.  In Chapter 3 terms, adding a variable that is highly correlated with $x_1$ inflates $\\text{Var}(\\hat\\beta_1^{(M)})$, which in turn raises its **standard error**.  A larger standard error makes hypothesis tests and confidence intervals for $\\beta_1$ less precise.\n",
    "\n",
    "**Bias–Variance Trade‐Off**\n",
    "\n",
    "* If those additional predictors genuinely explain part of $Y$ that would otherwise be absorbed into $x_1$’s slope, then $\\hat\\beta_1^{(M)}$ corrects **omitted‐variable bias** and is more reliable despite the larger SE.   \n",
    "\n",
    "* If the extra variables are only weakly related to $Y$ or nearly collinear with $x_1$, the **variance inflation** can outweigh any bias reduction, making $\\hat\\beta_1^{(M)}$ less precise than $\\hat\\beta_1^{(S)}$.\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "Whether $\\hat\\beta_1^{(M)}$ is “more reliable” than $\\hat\\beta_1^{(S)}$ depends on the bias–variance trade‐off.  Include only those predictors that meaningfully reduce omitted‐variable bias; otherwise, a simpler model may yield a more precise (though biased) estimate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e12713a",
   "metadata": {},
   "source": [
    "### Question 3. \n",
    "For our model for sales vs. TV interacted with radio, then what's the effect of an additional $1 of radio ad if TV = $50? \n",
    "\n",
    "\n",
    "### Answer 3. \n",
    "\n",
    "The model with the interaction term is\n",
    "\n",
    "$$sales = \\beta_0 + \\beta_1 \\times TV + \\beta_2 \\times radio + \\beta_3 \\times (radio \\times TV) + \\varepsilon.$$\n",
    "\n",
    "To find the effect that radio have on sales we take the partial \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial sales}{\\partial radio} &= \\beta_2 + \\beta_3 \\times TV\\\\\n",
    "&= 0.0289 + 0.0011 \\times 50 \\\\\n",
    "&= 0.0839\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The $R^2$ for including the interaction term is $96.8\\%$ compared to $89.7\\%$ without. Therefore, $\\frac{(96.8 - 89.8)}{(100 - 89.7)} = 69\\%$ of the variability in sales that remains after fitting the model has been explained by the interaction term. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bee161",
   "metadata": {},
   "source": [
    "---\n",
    "### $\\underline{\\text{Facts, Derivations, etc:}}$\n",
    "\n",
    "### Expected Value (mean, average, or most likely value):\n",
    "\n",
    "For discrete random values \n",
    "\n",
    "$$\\mathbb{E}[x]  = \\sum_{x\\in\\mathcal{X}} xp(x)= \\mu$$\n",
    "\n",
    "\n",
    "### Sum Rule for Marginals: \n",
    "\n",
    "For discrete random variables \n",
    "\n",
    "$$p(x) = \\sum_{y}p(x, y)$$\n",
    "\n",
    "where $p(x)$ is the marginal distribution of random variable $x$ and $p(x, y)$ is the joint distirbution.\n",
    "\n",
    "### Variance: \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "Var(x) &= \\mathbb{V}[x]\\\\\n",
    "&=\\mathbb{E}[(x - \\mu)^2]\\\\\n",
    "&= \\mathbb{E}[(x^2 - 2x\\mu + \\mu^2)] \\\\\n",
    "&= \\mathbb{E}[x^2] - 2\\mu\\mathbb{E}[x] + \\mathbb{E}[\\mu^2] \\\\\n",
    "&= \\mathbb{E}[x^2] - 2\\mu^2 + \\mu^2 \\\\\n",
    "&= \\mathbb{E}[x^2] - \\mu^2 \\\\\n",
    "&= \\sigma^2\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "note: sometimes $\\mu^2$ is written as $\\mathbb{E}[x]^2.$ \n",
    "\n",
    "### Linearity of Expectation: \n",
    "\n",
    "And you can split sums and  pull constants \n",
    "\n",
    "$$\\mathbb{E}[X + Y] = \\mathbb{E}[X] + \\mathbb{E}[Y].$$\n",
    "\n",
    "$$\\mathbb{E}[cX] = c\\mathbb{E}[X].$$\n",
    "\n",
    "### Linear Transformation of a Random Variable: \n",
    "\n",
    "$x$ is a random variable and $y$ is a linear transformation of $x$ where \n",
    "\n",
    "$$y = ax + b.$$\n",
    "\n",
    "The expectation is\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbb{E}[y] &= \\mathbb{E}[ax + b]\\\\\n",
    "&= \\mathbb{E}[ax] + \\mathbb{E}[b] \\\\\n",
    "&= a\\mathbb{E}[x] + b \\\\\n",
    "&= a\\mu + b. \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "And the variance is \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbb{V}[y] &= \\mathbb{E}[y^2] - \\mathbb{E}[y]^2\\\\\n",
    "&= \\mathbb{E}[(ax + b)^2] - \\mathbb{E}[ax + b]\\cdot\\mathbb{E}[ax + b] \\\\\n",
    "&=  \\mathbb{E}[(ax)^2 + 2abx + b^2] - (a\\mu + b)^2 \\\\\n",
    "&= a^2\\mathbb{E}[x^2] + 2ab\\mu + b^2 -[(a\\mu)^2 + 2ab\\mu + b^2] \\\\\n",
    "&= a^2\\mathbb{E}[x^2] - a^2\\mu^2 \\\\\n",
    "&= a^2(\\mathbb{E}[x^2] - \\mu^2) \\\\\n",
    "&= a^2\\mathbb{V}[x]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "### Covariance between $x \\in \\mathbb{R}$ and $y \\in \\mathbb{R}$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "Cov[x, y] &= \\mathbb{E}[(x - \\mathbb{E}[x])(y - \\mathbb{E}[y])]\\\\\n",
    "&= \\mathbb{E}[xy - y\\mathbb{E}[x] -x\\mathbb{E}[y] + \\mathbb{E}[x]\\mathbb{E}[y]] \\\\\n",
    "&= \\mathbb{E}[xy - y\\mu_x - x\\mu_y + \\mu_x\\mu_y] \\\\\n",
    "&= \\mathbb{E}[xy] - 2\\mu_x\\mu_y + \\mu_x\\mu_y \\\\\n",
    "&= \\mathbb{E}[xy] - \\mu_x\\mu_y \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "### Expected value of a random vector, $\\bf{x}\\in\\mathbb{R}^n$:\n",
    "\n",
    "We can calculate this expected value elementwise representing the averages of each component\n",
    "\n",
    "$$\\mathbb{E}[\\bf{x}] = \\begin{bmatrix}\n",
    "\\mathbb{E}[x_1] \\\\\n",
    "\\vdots \\\\ \n",
    "\\mathbb{E}[x_n] \\\\\n",
    "\\end{bmatrix} = \\mathbf{\\mu} \\in \\mathbb{R}^n.\n",
    "$$\n",
    "\n",
    "### Covariance of two random vectors, $\\bf{x}\\in\\mathbb{R}^n$ and $\\bf{y}\\in\\mathbb{R}^m$:\n",
    "$$\n",
    "\\begin{align*}\n",
    "Cov[\\bf{x}, \\bf{y}] &= \\mathbb{E}[(\\bf{x} - \\mathbb{E}[\\bf{x}])(\\bf{y} - \\mathbb{E}[\\bf{y}])^T]\\\\\n",
    "&= \\mathbb{E}[\\bf{x}\\bf{y}^T - \\mathbb{E}[\\bf{x}]\\bf{y}^T - \\bf{x}\\mathbb{E}[\\bf{y}]^T + \\mathbb{E}[\\bf{x}]\\mathbb{E}[\\bf{y}]^T] \\\\\n",
    "&= \\mathbb{E}[\\bf{x}\\bf{y}^T] - \\mu_x\\mu_y^T - \\mu_x\\mu_y^T + \\mu_x\\mu_y^T\\\\\n",
    "&= \\mathbb{E}[\\bf{x}\\bf{y}^T] - \\mu_x\\mu_y^T \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "### Uncorrelated random vectors \n",
    "Two random variables being **uncorrelated** (i.e., having zero covariance) does **not** imply they are **independent**.\n",
    "\n",
    "Consider the example:\n",
    "\n",
    "$$\n",
    "X \\sim \\text{Uniform}(-1, 1), \\qquad Y = X^2.\n",
    "$$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "\\operatorname{Cov}(X, Y) = \\operatorname{Cov}(X, X^2) = 0,\n",
    "$$\n",
    "\n",
    "but $X$ and $Y$ are clearly **not independent**. Knowing $Y$ restricts the possible values of $X$. So uncorrelatedness is a weaker condition than independence.\n",
    "\n",
    "### Two Random Vectors, $\\bf{x}$ and $\\bf{y}$:\n",
    "\n",
    "$$\\mathbb{E}[\\bf{x} + \\bf{y}] = \\mathbb{E}[\\bf{x}] + \\mathbb{E}[\\bf{y}]$$\n",
    "\n",
    "$$\\mathbb{E}[\\bf{x} - \\bf{y}] = \\mathbb{E}[\\bf{x}] - \\mathbb{E}[\\bf{y}]$$\n",
    "\n",
    "$$\\mathbb{V}[\\bf{x} + \\bf{y}] = \\mathbb{V}[\\bf{x}] + \\mathbb{V}[\\bf{y}] + Cov[\\bf{x}, \\bf{y}] + Cov[\\bf{y}, \\bf{x}]$$\n",
    "\n",
    "$$\\mathbb{V}[\\bf{x} - \\bf{y}] = \\mathbb{V}[\\bf{x}] + \\mathbb{V}[\\bf{y}] - Cov[\\bf{x}, \\bf{y}] - Cov[\\bf{y}, \\bf{x}]$$\n",
    "### Affine transformation of a random vector: \n",
    "\n",
    "The affine transfomation of random vector $\\bf{x}\\in\\mathbb{R}^n$ is $\\bf{y} = A\\bf{x} + \\bf{b}$ and has the expectation\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\mathbb{E}[\\bf{y}] &= \\mathbb{E}[A\\bf{x} + \\bf{b}] \\\\ \n",
    "&= A\\mathbb{E}[\\bf{x}] +  \\mathbb{E}[\\bf{b}]\\\\\n",
    "&= A\\bf{\\mu} + \\bf{b}.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The covariance is \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "Cov[\\bf{y}] &= \\mathbb{E}[(\\bf{y} - \\mathbb{E}[\\bf{y}])(\\bf{y} - \\mathbb{E}[\\bf{y}])^T]\\\\\n",
    "&= \\mathbb{E}[(A\\bf{x} - A\\mu)(A\\bf{x} - A\\mu)^T]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "and using **linearity of expectation**\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "&= \\mathbb{E}[A(\\bf{x} - \\mu)(\\bf{x} - \\mu)^T A^T] \\\\\n",
    "&= A\\mathbb{E}[(\\bf{x} - \\mu)(\\bf{x} - \\mu)^T]A^T \\\\\n",
    "&= A\\Sigma_x A^T\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec067ea6",
   "metadata": {},
   "source": [
    "### Expected Value (continuos):\n",
    "\n",
    "$$\\mathbb{E}[x]  = \\int_{-\\infty}^{\\infty} xf(x)= \\mu.$$\n",
    "\n",
    "### Marginal distribution (continuous): \n",
    "\n",
    "$$f(x) = \\int_{\\mathbb{R}}f(x, y)dy$$\n",
    "\n",
    "where f(x, y) is the joint distribution and f(x) is the marginal distribution.\n",
    "\n",
    "### TODO: more on cont. dist.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd9c4f4",
   "metadata": {},
   "source": [
    "### Ordinary Least Squares Derivation:\n",
    "\n",
    "Show for \n",
    "\n",
    "$$y_i = \\beta_0 + \\beta_1 x_i$$\n",
    "\n",
    "the $\\hat{\\beta}_0$, $\\hat{\\beta}_1$ that minimize $RSS$ using least squares is \n",
    "\n",
    "$$\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n(x_i - \\bar{x})}$$  \n",
    "\n",
    "$$\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1\\bar{x}$$\n",
    "\n",
    "where $\\bar{y} = \\frac{1}{n}\\sum_{i=1}^ny_i$, $\\bar{x} = \\frac{1}{n}\\sum_{i=1}^nx_i$.\n",
    "\n",
    "The residual or error for the $ith$ observation or sample between the actual and estimate is: \n",
    "\n",
    "$e_i = y_i - \\hat{y_i}$\n",
    "\n",
    "and for $n$ observations the $RSS$ or residual sum of squares is \n",
    "\n",
    "$$\\begin{align*}\n",
    "RSS &= \\sum_{i=1}^n e_i^2 \\\\\n",
    "    &= e_1^2 + \\dots + e_n^2 \\\\\n",
    "    &= (y_1 - \\hat{y}_1)^2 + \\dots + (y_n - \\hat{y}_n)^2\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "We now must minimize the $RSS$ with respect to $\\hat{\\beta}_0$, $\\hat{\\beta}_1$:\n",
    "\n",
    "$$\\begin{align*}\n",
    "RSS(\\hat{\\beta}_0,\\hat{\\beta}_1) &=(y_1 - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_1)^2 + \\dots + (y_n - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_n)^2\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "such that \n",
    "\n",
    "$$\\begin{align*}\n",
    "\\frac{\\partial RSS(\\hat{\\beta}_0,\\hat{\\beta}_1)}{\\partial \\hat{\\beta}_0} &= 2(y_1 - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_1)\\cdot(-1) + \\dots + 2(y_n - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_n)\\cdot(-1) \\\\\n",
    "&=2\\sum_{i=1}^n (\\hat{\\beta}_0 +\\hat{\\beta}_1 x_i - y_i) = 0  \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\frac{\\partial RSS(\\hat{\\beta}_0,\\hat{\\beta}_1)}{\\partial \\hat{\\beta}_1} &= 2(y_1 - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_1)\\cdot(-x_1) + \\dots + 2(y_n - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_n)\\cdot(-x_n) \\\\\n",
    "&=2\\sum_{i=1}^n x_i (\\hat{\\beta}_0 +\\hat{\\beta}_1 x_i - y_i) = 0  \n",
    "\\end{align*}\n",
    "$$ \n",
    "\n",
    "and we now have the following 2 equations: \n",
    "\n",
    "$\\frac{\\partial RSS(\\hat{\\beta}_0,\\hat{\\beta}_1)}{\\partial \\hat{\\beta}_0}=\\sum_{i=1}^n (\\hat{\\beta}_0 +\\hat{\\beta}_1 x_i - y_i)=0$\n",
    "\n",
    "$\\frac{\\partial RSS(\\hat{\\beta}_0,\\hat{\\beta}_1)}{\\partial \\hat{\\beta}_1}=\\sum_{i=1}^nx_i (\\hat{\\beta}_0 +\\hat{\\beta}_1 x_i - y_i) = 0 $.\n",
    "\n",
    "The first equation we can use the fact that $n\\hat{\\beta}_0=\\sum_{i=1}^n\\hat{\\beta}_0$ and $\\bar{x} = \\frac{1}{n}\\sum_{i=1}^nx_i$ then we have and solve for $\\hat{\\beta}_0$: \n",
    "\n",
    "$$\\begin{align*}\n",
    "\\hat{\\beta}_0 &= \\frac{1}{n}\\sum_{i=1}^ny_i - \\frac{\\hat{\\beta}_1}{n}\\sum_{i=1}^nx_i \\\\\n",
    "             &= \\bar{y} - \\hat{\\beta}_1\\bar{x}\n",
    "\\end{align*}\n",
    "$$.\n",
    "\n",
    "For the second equation we can identify the following two factorizations: \n",
    "\n",
    "$$\\begin{align*}\n",
    "\\sum_{i=1}^n(x_i - \\bar{x})(x_i - \\bar{x}) &= \\sum x_i^2 - \\bar{x}\\sum x_i - \\bar{x}\\sum x_i + \\sum \\bar{x}^2 \\\\\n",
    "&= \\sum x_i^2 - n\\bar{x}^2 - n\\bar{x}^2 + n\\bar{x}^2 \\\\ \n",
    "&= \\sum x_i^2 - n\\bar{x}^2\n",
    "\\end{align*}\n",
    "$$.\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\sum_{i=1}^n(x_i - \\bar{x})(y_i - \\bar{y}) &= \\sum x_iy_i - \\bar{x}\\sum y_i - \\bar{y}\\sum x_i + \\sum \\bar{x}\\bar{y} \\\\\n",
    "&= \\sum x_iy_i  - n\\bar{x}\\bar{y} - n\\bar{y}\\bar{x} + n\\bar{x}\\bar{y}\\\\ \n",
    "&= \\sum x_iy_i - n\\bar{x}\\bar{y}\n",
    "\\end{align*}\n",
    "$$.\n",
    "\n",
    "Then from the second equation we have: \n",
    "\n",
    "$$\\begin{align*}\n",
    "\\sum_{i=1}^n x_i y_i &= \\hat{\\beta_0}\\sum_{i=1}^nx_i + \\hat{\\beta_1}\\sum_{i=1}^n x_i^2 \\\\\n",
    " &= (\\bar{y} - \\hat{\\beta}_1 \\bar{x})\\sum_{i=1}^nx_i + \\hat{\\beta_1}\\sum_{i=1}^n x_i^2 \\\\\n",
    " &=(\\bar{y} - \\hat{\\beta}_1 \\bar{x})(n\\bar{x}) + \\hat{\\beta_1}\\sum_{i=1}^n x_i^2 \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Solving for $\\hat{\\beta_1}$:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\hat{\\beta_1}(\\sum_{i=1}^n x_i^2 - n\\bar{x}^2) = \\sum_{i=1}^n x_i y_i - n\\bar{x}\\bar{y} \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "and using the previous two factorizations we have: \n",
    "\n",
    "$$\\hat{\\beta_1}\\sum_{i=1}^n (x_i - \\bar{x})^2 = \\sum_{i=1}^n (x_i - \\bar{x})(y_i -\\bar{y})$$ \n",
    "\n",
    "and solving for $\\hat{\\beta_1}$:\n",
    "\n",
    "$$\\hat{\\beta_1} = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i -\\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360d68fe",
   "metadata": {},
   "source": [
    "### Ordinary Least Squares Derivation (matrix):\n",
    "\n",
    "Let's use the matrix gradient the derivation of OLS using the squared norm (same as RSS). First, let's elaborate the following facts for linear quadratic functions of $\\beta$: \n",
    "\n",
    "For any $\\mathbf{c} \\in \\mathbb{R}^n$, we have $\\nabla_{\\beta}(\\mathbf{c}^T\\mathbf{\\beta})=\\mathbf{c}$.\n",
    "\n",
    "For any $X \\in \\mathbb{R}^{n\\times n}$, we have $\\nabla_{\\beta}(\\mathbf{\\beta}^TX\\mathbf{\\beta})=(X + X^T)\\beta$.\n",
    "\n",
    "We are trying to minimize the squared norm with respect to $\\beta$: \n",
    "$$\\begin{align*}\n",
    "0 &= \\nabla||\\bold{y} - X\\bold{\\beta}||^2 \\\\\n",
    "&=\\nabla (\\bold{y} - X\\beta)^T(\\bold{y} - X\\beta)\\\\\n",
    "&= \\nabla (\\bold{y}^T - \\beta^TX^T)(\\bold{y} - X\\beta)\\\\\n",
    "&= \\nabla(\\bold{y}^T\\bold{y} - \\beta^TX^T\\bold{y} - \\bold{y}^TX\\beta +\\beta^TX^TX\\beta)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Since\n",
    "$\\beta \\in \\mathbb{R}^p$, $\\mathbf{y} \\in \\mathbb{R}^p$, and $X \\in \\mathbb{R}^{n\\times p}$, then $\\beta^TX^T\\bold{y}=\\bold{y}^TX\\beta$. We can readily take the transpose since they both result in the same scalar quantity. Continuing on we get \n",
    "\n",
    "$$\\begin{align*}\n",
    "0 &= \\nabla(\\bold{y}^T\\bold{y} - \\beta^TX^T\\bold{y} - \\bold{y}^TX\\beta +\\beta^TX^TX\\beta)\\\\\n",
    "&= \\nabla(\\bold{y}^T\\bold{y} - 2\\beta^TX^T\\bold{y} + \\beta^TX^TX\\beta) \\\\\n",
    "&= - 2X^T\\bold{y} + (X^TX + X^TX)\\beta \\\\\n",
    "&= - 2X^T\\bold{y}  + 2X^TX\\beta\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Solving for $\\beta$ we arrive at \n",
    "\n",
    "$$\\beta = (X^TX)^{-1}X^T\\bold{y}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab57f120",
   "metadata": {},
   "source": [
    "### $\\underline{\\text{Probability notation:}}$\n",
    "\n",
    "$X$ is a **random variable** a quantity whose value is known until we observe or \"sample\" it. \n",
    "\n",
    "### Individual sample values \n",
    "\n",
    "Are when we actually draw a sample of size $n$ we observe numeric realizations \n",
    "\n",
    "$$x_1, x_2, \\dots, x_n$$\n",
    "\n",
    "of the random variables \n",
    "\n",
    "$$X_1, X_2, \\dots, X_n$$ \n",
    "\n",
    "In the probabilistic model we assume there is a single underlying random variable $X$ that captures the population distribution—call its density $f_X$ or pmf.  A *sample* of size $n$ is modelled as $n$ independent replicates of that same variable:\n",
    "\n",
    "$$\n",
    "X_1,\\;X_2,\\;\\ldots,\\;X_n \\;\\stackrel{\\text{iid}}{\\sim}\\; X .\n",
    "$$\n",
    "\n",
    "* **Independent** means the outcome of one draw carries no information about the others.\n",
    "* **Identically distributed** means every draw has the same mean $\\mu$, variance $\\sigma^{2}$, and full distribution $f_X$.\n",
    "\n",
    "So each $X_i$ is a conceptually separate random variable, but all share the same population law.  They are placeholders for the numbers you *might* observe.  Once you actually collect the data, you plug in the realised values and switch to lowercase:\n",
    "\n",
    "$$\n",
    "(x_1,\\dots,x_n)=\\text{the specific numbers your experiment produced}.\n",
    "$$\n",
    "\n",
    "That separation—random $X_i$ in theory, observed $x_i$ in practice—is what lets us derive properties (like expectation and variance) before seeing any data, and then apply them to the concrete sample you end up with.\n",
    "\n",
    "\n",
    "### Sample-mean random variable\n",
    "\n",
    "Is when each $X_i$ is random, their average is \n",
    "\n",
    "$$\\bar{X} = \\frac{1}{n}\\sum_{i=1}^nX_i$$\n",
    "\n",
    "is also a random variable. It has an expectation \n",
    "\n",
    "$$\\mathbb{E}[\\bar{X}] = \\mu$$\n",
    "\n",
    "which says the sample mean is an unbiased estimator of the population mean. \n",
    "\n",
    "### Observed (numeric) sample mean\n",
    "\n",
    "After we collect the data we compute \n",
    "\n",
    "$$\\bar{x} = \\frac{1}{n}\\sum_{i=1}^nx_i$$\n",
    "\n",
    "which is a number. Lowercase is used because $\\bar{x}$ is no longer rrandom but is the variable $\\bar{X}$ takes for this particular sample. \n",
    "\n",
    "### Variances\n",
    "\n",
    "$$\\sigma^2 = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2$$\n",
    "\n",
    "is the *population* variance of a single draw $X$ from the distribution. We can think of each $X_i$ from the sample $$X_1,\\;X_2,\\;\\ldots,\\;X_n \\;\\stackrel{\\text{iid}}{\\sim}\\; X$$ as a separate \"dart throw\" at the same target, where each throw has the same spread around the bullseye\n",
    "\n",
    "$$Var(X_i) = \\sigma^2, \\hspace{0.1cm} i=1, \\dots, n.$$ \n",
    "\n",
    "Because the throws are *independent*, the law of variances for a sum is \n",
    "\n",
    "$$Var(\\sum_{i=1}^nX_i)=\\sum_{i=1}^nVar(X_i).$$\n",
    "\n",
    "Every term of the right-hard side equals the same constant, $\\sigma^2$, so adding the constant to itself $n$ times gives \n",
    "\n",
    "$$\\sum_{i=1}^n Var(X_i) = \\sigma^2 + \\sigma^2 + \\dots + \\sigma^2 = n\\sigma^2.$$\n",
    "\n",
    "As we have discussed, for a random variable $X$, the sample-mean is\n",
    "\n",
    "$$\\bar{X} = \\frac{1}{n}\\sum_{i=1}^n X_i$$\n",
    "\n",
    "has the expectation $\\mathbb{E}[\\bar{X}]=\\mu$ and by independence we have\n",
    "\n",
    "$$Var(\\bar{X}) = Var\\left(\\frac{1}{n}\\sum_{i=1}^n X_i\\right)=\\frac{1}{n^2}\\sum_{i=1}^n Var(X_i) = \\frac{n\\sigma^2}{n^2}=\\frac{\\sigma^2}{n}$$\n",
    "\n",
    "and this is called the *variance of the sample-mean estimator*. As $n$ grows, $\\bar{X}$ concentrates around $\\mu$ at a rate of $\\frac{1}{\\sqrt{n}}$. \n",
    "\n",
    "In other words, the standard error(or standard deviation of your estimator), \n",
    "\n",
    "$$SE(\\bar{X})=\\sqrt{Var(\\bar{X})}=\\sigma/\\sqrt{n}$$ \n",
    "\n",
    "captures two things \n",
    "- the numerator $\\sigma$ captures the variabililty in a single draw from the population\n",
    "- the denomintor $\\sqrt{n}$ captures the mathmatical effect of averaging independent draws. \n",
    "\n",
    "Therefore, if we quadrupled the sample size the *spread* (standard deviation, root-mean-square error) of the sampling distribution - hences your estimator error - falls by one-half. Therefore, any accuracy measure derived from the $SE$ like the half-width confidence interval scales like $1/\\sqrt{n}$. \n",
    "\n",
    "In practice, $\\sigma$ is unknown, and we can plug in the sample standard deviation $S$ to get the full data-based estimate\n",
    "\n",
    "$$\\hat{SE}(\\bar{X}) = \\frac{S}{\\sqrt{n}}.$$\n",
    "\n",
    "The resulting standard error is what appears in the t-statistic, t-based confidence interval, and subsequent inference step, we're carrying forward the same $\\sqrt{n}$ shrinkage. \n",
    "\n",
    "### Summary of probability notation:\n",
    "\n",
    "$X$ is one draw from the population (random)\n",
    "\n",
    "$X_i$ is the $i$-th draw in a sample (random)\n",
    "\n",
    "$\\bar{X}$ is the *sample-mean random variable* (random)\n",
    "\n",
    "$\\mathbb{E}[X]$ or $\\mathbb{E}[\\bar{X}]$ is the population mean $\\mu$ (deterministic constant)\n",
    "\n",
    "$x_i$ is the observed value of $X_i$ (not random)\n",
    "\n",
    "$\\bar{x}$ is the numeric mean of the observed sample (not random)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44a08d6",
   "metadata": {},
   "source": [
    "---\n",
    "### $\\underline{\\text{More Simple Regression Analysis}}$:\n",
    "Since $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ are estimators for $\\beta_0$ and $\\beta_1$ we want to determine how good they are. To construct confidence intervals for the true regression coefficients we need to find the distribution functions for $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$. We can assume that the noise, $\\varepsilon_i$, are independent and normally distributed random variables with zero mean and variance $\\sigma^2$. For now, we assume that $\\sigma^2$ is known. \n",
    "\n",
    "### Expected value of coefficient estimators\n",
    "\n",
    "The true relationship between $Y$ and $X$ is given by \n",
    "\n",
    "$$Y = f(X) + \\varepsilon$$\n",
    "\n",
    "where $f(X) = \\beta_0 + \\beta_1X$.\n",
    "\n",
    "How accurate is the sample mean, $\\hat{\\mu}$, to the population mean, $\\mu$? We can estimate $Y$ for a given $X$ by find the best estimate of the coefficients $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ using least squares\n",
    "\n",
    "$$\\hat{\\beta_0} = \\bar{y} - \\hat{\\beta_1}\\bar{x}$$\n",
    "\n",
    "$$\\hat{\\beta_1} = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i -\\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2}.$$\n",
    "\n",
    "Since \n",
    "\n",
    "$$\\begin{align*}\n",
    "\\sum_{i=1}^n(x_i - \\bar{x})(y_i -\\bar{y}) &= \\sum_{i=1}^nx_iy_i - \\bar{y}\\sum_{i=1}^nx_i - \\bar{x}\\sum_{i=1}^ny_i + \\sum_{i=1}^n\\bar{x}\\bar{y} \\\\\n",
    "&= \\sum_{i=1}^nx_iy_i - n\\bar{x}\\bar{y} - n\\bar{x}\\bar{y} + n\\bar{x}\\bar{y} \\\\\n",
    "&= \\sum_{i=1}^nx_iy_i - n\\bar{x}\\bar{y} \\\\\n",
    "&= \\sum_{i=1}^nx_iy_i - \\bar{x}\\sum_{i=1}^n{y_i} \\\\\n",
    "&= \\sum_{i=1}^n(x_i - \\bar{x})y_i\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\\begin{align*}\n",
    "\\sum_{i=1}^n (x_i - \\bar{x})^2 &= \\sum_{i=1}^n x_i^2 - 2\\bar{x}\\sum_{i=1}^nx_i + \\sum_{i=1}^n\\bar{x}^2 \\\\\n",
    "&=\\sum_{i=1}^n x_i^2 - 2n\\bar{x}^2 + n\\bar{x}^2 \\\\\n",
    "&=\\sum_{i=1}^n x_i^2 - 2n\\bar{x}^2 + n\\bar{x}^2 \\\\\n",
    "&=\\sum_{i=1}^n x_i^2 - n\\bar{x}^2.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "then we can re-write $\\hat{\\beta_1}$ as \n",
    "\n",
    "$$\\hat{\\beta_1} = \\frac{\\sum_{i=1}^n(x_i - \\bar{x})y_i}{\\sum_{i=1}^n x_i^2 - n\\bar{x}^2}.$$\n",
    "\n",
    "We have now factored out $y_i$, which is our true response. This is a random variable since \n",
    "\n",
    "$$y_i = \\beta_0 + \\beta_1x_i + \\varepsilon.$$\n",
    "\n",
    "where $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$ and the other terms like $x_i$ are observations (numeric realizations that are not random) of random variable $X_i$. Let's subsitute $y_i$ into $\\hat{\\beta_1}$\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\hat{\\beta_1} &= \\frac{\\sum_{i=1}^n(x_i - \\bar{x})(\\beta_0 + \\beta_1x_i + \\varepsilon)}{\\sum_{i=1}^n x_i^2 - n\\bar{x}^2} \\\\\n",
    "&=\\frac{\\sum_{i=1}^n(x_i - \\bar{x})\\beta_0 + \\sum_{i=1}^n(x_i - \\bar{x})\\beta_1x_i + \\sum_{i=1}^n(x_i - \\bar{x})\\varepsilon)}{\\sum_{i=1}^n x_i^2 - n\\bar{x}^2}.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Taking the expectation of $\\hat{\\beta_1}$ we have \n",
    "\n",
    "$$\\begin{align*}\n",
    "\\mathbb{E}[\\hat{\\beta_1}] &=\\frac{\\sum_{i=1}^n(x_i - \\bar{x})\\beta_0 + \\sum_{i=1}^n(x_i - \\bar{x})\\beta_1x_i + \\sum_{i=1}^n(x_i - \\bar{x})\\mathbb{E}[\\varepsilon])}{\\sum_{i=1}^n x_i^2 - n\\bar{x}^2}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "and since $\\mathbb{E}[\\varepsilon]=0$ and $\\sum_{i=1}^n(x_i - \\bar{x}) = n\\bar{x} - n\\bar{x}=0$ we have \n",
    "\n",
    "$$\\begin{align*}\n",
    "\\mathbb{E}[\\hat{\\beta}_1] &=\\frac{\\sum_{i=1}^n(x_i - \\bar{x})x_i\\beta_1}{\\sum_{i=1}^n x_i^2 - n\\bar{x}^2}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "and since $\\sum_{i=1}^n x_i^2 - n\\bar{x}^2 = \\sum_{i=1}^n(x_i - \\bar{x})x_i$ \n",
    "\n",
    "$$\\begin{align*}\n",
    "\\mathbb{E}[\\hat{\\beta}_1] &=\\beta_1\\frac{\\sum_{i=1}^n(x_i - \\bar{x})x_i}{\\sum_{i=1}^n(x_i - \\bar{x})x_i}\\\\\n",
    "&=\\beta_1.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Therefore, $\\hat{\\beta_1}$ is an unbiased estimator of $\\beta_1$. We can do the same for $\\hat{\\beta_0}$\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\hat{\\beta}_0 &= \\bar{y} - \\hat{\\beta}_1\\bar{x}\\\\\n",
    "&=\\frac{1}{n}\\sum_{i=1}^n y_i - \\hat{\\beta}_1\\bar{x}\\\\\n",
    "&=\\frac{1}{n}\\sum_{i=1}^n (\\beta_0 + \\beta_1x_i + \\varepsilon_i) - \\hat{\\beta}_1\\bar{x}\\\\\n",
    "&=\\frac{1}{n}(\\sum_{i=1}^n \\beta_0 + \\sum_{i=1}^n \\beta_1 x_i) + \\frac{1}{n}\\sum_{i=1}^n\\varepsilon_i - \\hat{\\beta}_1\\bar{x}\\\\\n",
    "&=\\frac{1}{n}(n\\beta_0 + n\\beta_1\\bar{x}) + \\bar{\\varepsilon} - \\hat{\\beta}_1\\bar{x}\\\\\n",
    "&=\\beta_0 + (\\beta_1 - \\hat{\\beta}_1)\\bar{x} + \\bar{\\varepsilon} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "and taking the expectation of both sides \n",
    "\n",
    "\n",
    "$$\\mathbb{E}[\\hat{\\beta}_0] =\\mathbb{E}[(\\beta_0 + (\\beta_1 - \\hat{\\beta}_1)\\bar{x} + \\bar{\\varepsilon})]$$ \n",
    "\n",
    "and since $\\mathbb{E}[\\hat{\\beta}_1] = \\beta_1$, $\\mathbb{E}[\\bar{\\varepsilon}]=0$, then\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbb{E}[\\hat{\\beta}_0]&=\\beta_0 + (\\beta_1 - \\beta_1)\\bar{x} \\\\\n",
    "&=\\beta_0.\n",
    "\\end{align*}\n",
    "$$ \n",
    "\n",
    "Therefore, $\\hat{\\beta}_0$ is a unbiased estimator of $\\beta_0$.\n",
    "\n",
    "### Variance of coefficient estimators\n",
    "\n",
    "Since we know that the variance of the true response is \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "Var(Y_i) &= Var(\\beta_0 + \\beta_1x_i + \\varepsilon) \\\\\n",
    "&=Var(\\varepsilon) \\\\\n",
    "&=\\sigma^2.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where the the design points $x_i$ (or design matrix, $X_i$) are fixed. \n",
    "\n",
    "### Note\n",
    "A more mathematically correct notation is $Var(y_i|x_i)$, which explicitly says that the variance is conditional on those fixed design values, $x_i$. This is to distinguish it from the unconditional variance $Var(Y)=\\beta_1^2Var(X)+\\sigma^2$ when $X$ is random. Also, this notational difference is made when you are contrasting regression with other models where the predictors themselves are stochastic. In ISLP, the authors will just use $Var(y_i)=\\sigma^2$ or $Var(Y_i)=\\sigma^2$. This is fine as long as you specify that the variance comes from the noise term and that you’re treating the design points as fixed.\n",
    "\n",
    "### Variance of coefficient estimators cont. \n",
    "Back to the coefficient estimates, we have from before    \n",
    "\n",
    "$$\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n(x_i-\\bar{x})y_i}{\\sum_{i=1}^n(x_i-\\bar{x})^2}$$\n",
    "\n",
    "Using the fact that $Var(y_i)=\\sigma^2$ and $Var(aX) = a^2Var(X)$ we can take the variance of both sides to \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "Var(\\hat{\\beta}_1) &=Var\\left(\\frac{\\sum_{i=1}^n(x_i-\\bar{x})y_i}{\\sum_{i=1}^n(x_i-\\bar{x})^2}\\right) \\\\\n",
    "&=\\frac{\\sum_{i=1}^n(x_i-\\bar{x})^2 Var(y_i)}{\\sum_{i=1}^n(x_i-\\bar{x})^4}\\\\\n",
    "&=\\frac{\\sigma^2}{\\sum_{i=1}^n(x_i-\\bar{x})^2}.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Likewise, using the fact \n",
    "\n",
    "$$Var(X - Y) = Var(X) + Var(Y) - 2Cov(X, Y)$$\n",
    "\n",
    "and taking the variance of both sides \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "Var(\\hat{\\beta}_0) &= Var(\\bar{y} - \\hat{\\beta}_1\\bar{x}) \\\\\n",
    "&= Var(\\bar{y}) + \\bar{x}^2Var(\\hat{\\beta}_1) - 2\\bar{x}Cov(\\bar{y},\\hat{\\beta}_1)\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "and since $\\bar{y}$ and $\\hat{\\beta}_1$ are uncorrelated then $Cov(\\bar{y},\\hat{\\beta}_1)=0$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "Var(\\hat{\\beta}_0) &= Var(\\bar{y}) + \\bar{x}^2Var(\\hat{\\beta}_1) \\\\\n",
    "&= Var\\left(\\frac{1}{n}\\sum_{i=1}^ny_i\\right) + \\bar{x}^2Var(\\hat{\\beta}_1) \\\\\n",
    "&= \\frac{1}{n^2}Var(\\sum_{i=1}^ny_i) + \\bar{x}^2Var(\\hat{\\beta}_1) \\\\\n",
    "&= \\frac{1}{n^2}(n\\sigma^2) + \\bar{x}^2\\left(\\frac{\\sigma^2}{\\sum_{i=1}^n(x_i-\\bar{x})^2}\\right)\\\\\n",
    "&=\\frac{\\sigma}{n} + \\frac{\\sigma^2\\bar{x}^2}{\\sum_{i=1}^n(x_i-\\bar{x})^2}.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "### Proof that covariance is zero\n",
    "We start with the formulas for the estimators:\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x}) Y_i}{\\sum_{i=1}^n (x_i - \\bar{x})^2},\n",
    "\\qquad\n",
    "\\bar{Y} = \\frac{1}{n} \\sum_{i=1}^n Y_i.\n",
    "$$\n",
    "\n",
    "We want to compute the covariance:\n",
    "\n",
    "$$\n",
    "\\operatorname{Cov}(\\bar{Y}, \\hat{\\beta}_1)\n",
    "= \\operatorname{Cov}\\left( \\frac{1}{n} \\sum_{i=1}^n Y_i,\n",
    "\\frac{\\sum_{j=1}^n (x_j - \\bar{x}) Y_j}{\\sum_{j=1}^n (x_j - \\bar{x})^2} \\right).\n",
    "$$\n",
    "\n",
    "Since the denominator in $\\hat{\\beta}_1$ is constant (a function of the fixed design points $x_j$), we can factor it out:\n",
    "\n",
    "$$\n",
    "= \\frac{1}{n \\sum_{j=1}^n (x_j - \\bar{x})^2}\n",
    "\\operatorname{Cov} \\left( \\sum_{i=1}^n Y_i, \\sum_{j=1}^n (x_j - \\bar{x}) Y_j \\right).\n",
    "$$\n",
    "\n",
    "Expanding the covariance:\n",
    "\n",
    "$$\n",
    "= \\frac{1}{n \\sum_{j=1}^n (x_j - \\bar{x})^2}\n",
    "\\sum_{i=1}^n \\sum_{j=1}^n (x_j - \\bar{x}) \\operatorname{Cov}(Y_i, Y_j).\n",
    "$$\n",
    "\n",
    "Since $\\operatorname{Cov}(Y_i, Y_j) = 0$ for $i \\ne j$ and $\\sigma^2$ for $i = j$ (due to independent errors):\n",
    "\n",
    "$$\n",
    "= \\frac{1}{n \\sum_{j=1}^n (x_j - \\bar{x})^2}\n",
    "\\sum_{i=1}^n (x_i - \\bar{x}) \\sigma^2\n",
    "= \\frac{\\sigma^2}{n \\sum_{j=1}^n (x_j - \\bar{x})^2}\n",
    "\\sum_{i=1}^n (x_i - \\bar{x}).\n",
    "$$\n",
    "\n",
    "But:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^n (x_i - \\bar{x}) = 0,\n",
    "$$\n",
    "\n",
    "so we conclude:\n",
    "\n",
    "$$\n",
    "\\operatorname{Cov}(\\bar{Y}, \\hat{\\beta}_1) = 0.\n",
    "$$\n",
    "\n",
    "Thus, $\\bar{Y}$ and $\\hat{\\beta}_1$ are uncorrelated.\n",
    "\n",
    "\n",
    "### Sample variance\n",
    "\n",
    "Once we know the distributions for the two estimators $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ such \n",
    "that $\\sigma^2$ is known then we can construct confidence intervals for the regression \n",
    "coefficients. In the majority of cases $\\sigma^2$ is not known, and so we try to come \n",
    "up with an estimator for $\\sigma^2$, where $\\sigma^2$ is a measure of the variation of \n",
    "data around the regression line.\n",
    "\n",
    "We know the $RSS$ is given by \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "RSS &= \\sum_{i=1}^n(y_i - \\hat{y}_i)^2 \\\\\n",
    "&=\\sum_{i=1}^n(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i)^2. \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Let's define the following quantities for convenience \n",
    "$$S_{xx} = \\sum_{i=1}^n(x_i - \\bar{x})^2$$\n",
    "\n",
    "$$S_{yy} = \\sum_{i=1}^n(y_i - \\bar{y})^2$$\n",
    "\n",
    "$$S_{xy} = \\sum_{i=1}^n(x_i - \\bar{x})(y_i - \\bar{y}).$$\n",
    "\n",
    "Since $\\hat{\\beta}_0 = \\bar{y} -  \\hat{\\beta}_1\\bar{x}$ we have \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "RSS &=\\sum_{i=1}^n[y_i - \\bar{y} -  \\hat{\\beta}_1(x_i - \\bar{x})]^2 \\\\\n",
    "&= \\sum_{i=1}^n(y_i - \\bar{y})^2 - 2\\hat{\\beta}_1\\sum_{i=1}^n(y_i - \\bar{y})(x_i - \\bar{x}) + \\hat{\\beta}_1^2\\sum_{i=1}^n(x_i - \\bar{x})^2 \\\\\n",
    "&= S_{yy} - 2\\hat{\\beta}_1S_{xy} + \\hat{\\beta}_1^2S_{xx}.\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "We know that $\\hat{\\beta_1}$ is \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{\\beta_1} &= \\frac{\\sum_{i=1}^n(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n(x_i - \\bar{x})^2} \\\\\n",
    "&=\\frac{S_{xy}}{S_{xx}}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "and plugging back into $RSS$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "RSS &= S_{yy} - 2\\frac{S_{xy}}{S_{xx}}S_{xy} + \\left(\\frac{S_{xy}}{S_{xx}}\\right)^2 S_{xx}\\\\\n",
    "&=S_{yy} - \\frac{S_{xy}^2}{S_{xx}}\\\\\n",
    "&=S_{yy} - \\hat{\\beta_1}S_{xy}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "and taking the expectation of both sides \n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbb{E}[RSS] &= \\mathbb{E}[S_{yy} - \\hat{\\beta_1}S_{xy}] \\\\\n",
    "&=\\mathbb{E}[\\sum_{i=1}^ny_i^2 - n\\bar{y}^2 - \\hat{\\beta}_1^2S_{xx}] \\\\\n",
    "&=\\sum_{i=1}^n(\\mathbb{E}[y_i^2]) - n\\mathbb{E}[\\bar{y}^2] - S_{xx}\\mathbb{E}[\\hat{\\beta}_1^2] \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "and because $\\mathbb{V}[y_i] = \\mathbb{E}[y_i^2] - (\\mathbb{E}[y_i])^2$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbb{E}[RSS]\n",
    "&=\\sum_{i=1}^n(\\mathbb{V}[y_i] + \\mathbb{E}[y_i]^2) - n(\\mathbb{V}[\\bar{y}] + \\mathbb{E}[\\bar{y}]^2) - S_{xx}(\\mathbb{V}[\\hat{\\beta}_1] + \\mathbb{E}[\\hat{\\beta}_1]^2)\\\\\n",
    "\n",
    "&=\\sum_{i=1}^n\\sigma^2 + \\sum_{i=1}^n(\\beta_0 + \\beta_1x_i)^2 - n\\left(\\frac{\\sigma^2}{n} + (\\beta_0 + \\beta_1\\bar{x})^2\\right) -S_{xx}\\left(\\frac{\\sigma^2}{S_{xx}} + \\beta_1^2\\right) \\\\ \n",
    "\n",
    "&=n\\sigma^2 + \\sum_{i=1}^n\\left(\\beta_0^2 + 2\\beta_0\\beta_1x_i + (\\beta_1x_i)^2\\right) - \\sigma^2 - \\left(n\\beta_0^2 + 2\\beta_0\\beta_1(n\\bar{x}) + n\\beta_1^2\\bar{x}^2\\right) - \\sigma^2 - \\beta_1^2\\sum_{i=1}^n(x_i - \\bar{x})^2 \\\\\n",
    "\n",
    "&=(n-2)\\sigma^2 + \\sum_{i=1}^n\\left(\\beta_0^2 + 2\\beta_0\\beta_1x_i + (\\beta_1x_i)^2\\right) - n\\beta_0^2  - 2\\beta_0\\beta_1(n\\bar{x}) - n\\beta_1^2\\bar{x}^2 - \\beta_1^2\\sum_{i=1}^nx_i^2 + n\\beta_1^2\\bar{x}^2\\\\\n",
    "\n",
    "&=(n-2)\\sigma^2 + \\sum_{i=1}^n\\left(\\beta_0^2 + 2\\beta_0\\beta_1x_i + (\\beta_1x_i)^2\\right) - \\sum_{i=1}^n\\beta_0^2 - 2\\beta_0\\beta_1\\sum_{i=1}^nx_i - \\beta_1^2\\sum_{i=1}^nx_i^2\\\\\n",
    "\n",
    "&=(n-2)\\sigma^2 + \\sum_{i=1}^n\\left(\\beta_0^2 + 2\\beta_0\\beta_1x_i + (\\beta_1x_i)^2\\right) - \\sum_{i=1}^n\\left(\\beta_0^2 + 2\\beta_0\\beta_1x_i + (\\beta_1x_i)^2\\right) \\\\\n",
    "\n",
    "&=(n-2)\\sigma^2.\n",
    "\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Then dividing both sides by $n-2$ \n",
    "\n",
    "$$\\mathbb{E}\\left[\\frac{RSS}{n-2}\\right] = \\sigma^2$$\n",
    "\n",
    "in other words, $\\frac{RSS}{n-2}$ is an unbiased estimator of the true error variance. By \n",
    "definition this quantity is called the squared residual standard error \n",
    "\n",
    "$$RSE^2 = \\frac{RSS}{n-2}$$\n",
    "\n",
    "which can be expressed using previous quantities \n",
    "\n",
    "$$RSE^2= \\frac{S_{yy} - \\hat{\\beta}_1S_{xy}}{n-2}.$$\n",
    "\n",
    "Taking the square root we get the **residual standard error**\n",
    "\n",
    "$$RSE= \\sqrt{\\frac{RSS}{n-2}}.$$\n",
    "\n",
    "So why is this unbiased? Because by construction we have\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[\\mathrm{RSS}] = (n-2)\\,\\sigma^2\n",
    "$$\n",
    "it follows immediately that\n",
    "$$\n",
    "\\mathbb{E}\\!\\Bigl[\\frac{\\mathrm{RSS}}{n-2}\\Bigr]\n",
    "= \\frac{\\mathbb{E}[\\mathrm{RSS}]}{n-2}\n",
    "= \\frac{(n-2)\\,\\sigma^2}{n-2}\n",
    "= \\sigma^2.\n",
    "$$\n",
    "That is exactly the definition of an unbiased estimator of $\\sigma^2$: its expectation equals the true parameter. Since $RSE^2$ is unbiased then the distribution function for $RSE^2$ is \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{RSE^2(n-2)}{\\sigma^2} &= \\frac{S_{yy}}{\\sigma^2} - \\frac{\\hat{\\beta}_1 S_{xx}}{\\sigma^2} \\\\\n",
    "&=\\sum_{i=1}^n\\left(\\frac{y_i - \\bar{y}}{\\sigma^2}\\right) - \\frac{\\hat{\\beta}_1^2 S_{xx}}{\\sigma^2} \\\\\n",
    "&=\\sum_{i=1}^n\\underbrace{\\left(\\frac{y_i - \\bar{y}}{\\sigma^2}\\right)}_{\\chi_{n-1}^2}  - \\underbrace{\\left(\\frac{\\hat{\\beta}_1 - \\beta_1}{\\sigma/\\sqrt{S_{xx}}}\\right)^2}_{\\chi_1^2}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "which suggests that $\\frac{RSE^2(n-2)}{\\sigma^2}$ has a $\\chi^2$ distirbution with $n-2$ degress of freedom. \n",
    "\n",
    "Recall, $\\hat{\\beta}_1$ is normally distributed with parameters $\\beta_1$ and $\\sigma^2/\\sqrt{S_{xx}}$, and therefore $\\frac{\\hat{\\beta}_1 - \\beta_1}{RSE/\\sqrt{S_{xx}}} \\sim \\mathcal{N}(0,1)$. If we replace $\\sigma$ by its estimate $RSE$ we find\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\hat{\\beta}_1 - \\beta_1}{RSE/\\sqrt{S_{xx}}} &= \\frac{\\frac{\\hat{\\beta}_1 - \\beta_1}{\\sigma/\\sqrt{S_{xx}}}}{\\sqrt{\\frac{RSE^2}{\\sigma^2}\\cdot\\frac{n-2}{n-2}}} \\\\\n",
    "\n",
    "&=\\frac{\\frac{\\hat{\\beta}_1 - \\beta_1}{\\sigma/\\sqrt{S_{xx}}}\\sqrt{n-2}}{\\sqrt{\\frac{RSE^2}{\\sigma^2}(n-2)}} \\\\\n",
    "\n",
    "&\\sim \\frac{\\mathcal{N}(0, \\sqrt{n-2})}{\\sqrt{\\chi^2(n-2)}} \\\\\n",
    "\n",
    "&\\sim t(n-2)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "which suggests that $\\frac{\\hat{\\beta}_1 - \\beta_1}{RSE/\\sqrt{S_{xx}}}$ has a t-distribution with $n-2$ degress of freedom. Next we can consider the distribution for the estimate of the intercept, $\\hat{\\beta}_0$. Recall, that $\\hat{\\beta}_0 \\sim \\mathcal{N}(\\beta_0, \\sqrt{\\frac{\\sigma^2}{n} + \\frac{\\sigma^2 \\bar{x}^2}{S_{xx}}})$ and therefore $$\\frac{\\hat{\\beta}_0 - \\beta_0}{\\sqrt{\\frac{\\sigma^2}{n} + \\frac{\\sigma^2 \\bar{x}^2}{S_{xx}}}} \\sim \\mathcal{N}(0, 1)$$ and if we replace $\\sigma$ with $RSE$ then \n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\n",
    "\\frac{\\hat{\\beta}_0 - \\beta_0}{\\sqrt{\\frac{\\sigma^2}{n} + \\frac{\\sigma^2 \\bar{x}^2}{S_{xx}}}} &= \\frac{\\frac{\\hat{\\beta}_0 - \\beta_0}{\\sigma\\sqrt{\\frac{1}{n} + \\frac{\\bar{x}^2}{S_{xx}}}}}{\\sqrt{\\frac{RSE^2}{\\sigma^2}\\cdot \\frac{n-2}{n-2}}}\\\\\n",
    "\n",
    "&=\\frac{\\frac{\\hat{\\beta}_0 - \\beta_0}{\\sigma\\sqrt{\\frac{1}{n} + \\frac{\\bar{x}^2}{S_{xx}}}}\\sqrt{n-2}}\n",
    "     {\\sqrt{\\frac{\\mathrm{RSE}^2}{\\sigma^2}\\,(n-2)}}\\\\\n",
    "\n",
    "&\\sim \\frac{\\mathcal{N}(0, \\sqrt{n-2})}{\\sqrt{\\chi^2(n-2)}} \\\\\n",
    "\n",
    "&\\sim t(n-2)\n",
    "\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "which suggests that $\\frac{\\hat{\\beta}_0 - \\beta_0}{RSE\\sqrt{\\frac{1}{n} + \\frac{\\sigma^2 \\bar{x}^2}{S_{xx}}}}$ has a t-distirbution with $n-2$ degrees of freedom. \n",
    "\n",
    "### Bias - Variance relationship to estimators\n",
    "\n",
    "For any estimator $\\hat{\\theta}$ of a scalar target $\\theta$\n",
    "\n",
    "$$\\operatorname{MSE}(\\hat{\\theta})\n",
    "  \\;=\\;\n",
    "  \\mathbb{E}\\!\\bigl[(\\hat{\\theta}-\\theta)^2\\bigr]\n",
    "  \\;=\\;\n",
    "  \\underbrace{\\bigl(\\operatorname{Bias}(\\hat{\\theta})\\bigr)^2}_{\\text{systematic error}}\n",
    "  \\;+\\;\n",
    "  \\underbrace{\\operatorname{Var}(\\hat{\\theta})}_{\\text{sampling noise}}.\n",
    "$$\n",
    "\n",
    "If an estimator is unbiased, like ordinary least squares gives for $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$, then the first term of the bias-variance decomposition vanishes. The mean-squared error reduces to variance alone\n",
    "\n",
    "$$\n",
    "\\operatorname{MSE}(\\hat{\\beta}_0) \\;=\\; \\operatorname{Var}(\\hat{\\beta}_0),\n",
    "\\qquad\n",
    "\\operatorname{MSE}(\\hat{\\beta}_1) \\;=\\; \\operatorname{Var}(\\hat{\\beta}_1).\n",
    "$$\n",
    "\n",
    "Therefore, OLS is the zero-bias corner of the bias-variance plane, and it's total risk is entirely variance driven. Unbiasedness is nice, but it does not guarantee low MSE. If the design points $x_i$ are tightly clustered or the noise level $\\sigma^2$ is high then the variance for $\\hat{\\beta}_0$, $\\hat{\\beta}_1$ can be large, giving wider confidence intervals and unstable predictions. Shrinkage methods like ridge, lasso, elastic net, Bayesian priors deliberately introduce bias to cut variance, often yielding a smaller MSE overall. This \"trade-off\" is deciding where on the curve you want to land. OLS chooses the far left edge and ridge and lasso slide rightward, accepting some bias to reduce variance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d845db7",
   "metadata": {},
   "source": [
    "---\n",
    "### $\\underline{\\text{More Multiple Regression Analysis}}$:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183a0f14",
   "metadata": {},
   "source": [
    "Let's re-write the OLS equations in a different form, that will be amenable to deriving sample variance, f-statistics, and other stats used for multiple regression. We know that the **residual vector** is  \n",
    "\n",
    "$$\\bf{e} = Y - \\hat{Y}$$\n",
    "\n",
    "and the OLS solution is \n",
    "\n",
    "$$\\hat{\\beta} = (X^TX)^{-1}X^TY.$$\n",
    "\n",
    "The fitted values are \n",
    "\n",
    "$$\\hat{Y} = X\\hat{\\beta} = X(X^TX)^{-1}X^TY = PY$$\n",
    "\n",
    "where\n",
    "\n",
    "$$P = X(X^TX)^{-1}X^T.$$\n",
    "\n",
    "Using the residual definition we know\n",
    "\n",
    "$$\\bf{e}=Y - \\hat{Y} = Y - PY = (I - P)Y.$$\n",
    "\n",
    "Plugging in the true model $Y=X\\beta + \\varepsilon$ \n",
    "\n",
    "$$\\bf{e}= (I - P)(X\\beta + \\varepsilon) = (I - P)X\\beta + (I-P)\\varepsilon.$$\n",
    "\n",
    "Since $PX = X(X^TX)^{-1}X^TX = XI = X$ then \n",
    "\n",
    "$$(I-P)X\\beta=X\\beta - PX\\beta = X\\beta - X\\beta = 0$$\n",
    "\n",
    "and all that remains is \n",
    "\n",
    "$$\\bf{e} = (I - P)\\varepsilon$$\n",
    "\n",
    "which can also be written as \n",
    "\n",
    "$$\\bf{e} = (I - X(X^TX)^{-1}X^T)\\varepsilon.$$\n",
    "\n",
    "We will also use the following identities\n",
    "\n",
    "$$\\mathbb{E}[Tr(X)] = Tr(E[X])$$\n",
    "\n",
    "\n",
    "$$Tr(AB) = Tr(BA)$$\n",
    "\n",
    "$$Tr(ABC) = Tr(BCA) = Tr(CAB)$$ \n",
    "\n",
    "which is the circular property. The inner product of of the residual $\\bf{e}\\in\\mathbb{R}^n$ can be expressed as \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\sum_{i=1}^n e_i^2 &= \\bf{e}^T\\bf{e} \\\\\n",
    "&= (\\bf{Y}-\\hat{\\bf{Y}})^T (\\bf{Y}-\\hat{\\bf{Y}})\\\\\n",
    "&=\\it{Tr}\\left((\\bf{Y}-\\hat{\\bf{Y}})(\\bf{Y}-\\hat{\\bf{Y}})^T\\right).\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Also, we have $\\mathbf{\\varepsilon} = (\\varepsilon_1, \\dots, \\varepsilon_n)^T$\n",
    "\n",
    "$$\n",
    "\\mathbf{\\varepsilon\\,\\varepsilon}^T\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "  \\varepsilon_1\\varepsilon_1 & \\cdots & \\varepsilon_1\\varepsilon_n \\\\\n",
    "  \\vdots & \\ddots & \\vdots \\\\\n",
    "  \\varepsilon_n\\varepsilon_1 & \\cdots & \\varepsilon_n\\varepsilon_n\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "and therefore \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "Tr(\\varepsilon\\,\\varepsilon^T) &=\\varepsilon_1^2 + \\cdots + \\varepsilon_n^2 \\\\\n",
    "&=\\sum_{i=1}^n \\varepsilon_i^2 \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "and if we take the expectation of both sides we get \n",
    "\n",
    "$$\\mathbb{E}[Tr(\\varepsilon\\,\\varepsilon^T)] = \\sum_{i=1}^n \\mathbb{E}[\\varepsilon_i^2]$$\n",
    "\n",
    "\n",
    "where \n",
    "$$\n",
    "\\begin{align*}\n",
    "\\sum_{i=1}^n \\mathbb{E}[\\varepsilon_i^2] &= \\sum_{i=1}^n[Var(\\varepsilon_i) + (\\mathbb{E}[\\varepsilon_i])^2]\\\\\n",
    "&= \\sum_{i=1}^nVar(\\varepsilon_i) \\\\\n",
    "&= n\\sigma^2. \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "### F-statistic\n",
    "\n",
    "Recall, in multiple regression we have $p$ predictors, and to determine whether there is a relationship between the response and the predictors, we need to determine whether the regression coefficients are zero. The null hypothesis is \n",
    "\n",
    "$$H_0:\\beta_1 = \\beta_2 =\\cdots=\\beta_p=0$$\n",
    "\n",
    "versus the alternative \n",
    "\n",
    "$$H_a: \\text{atleast one} \\hspace{0.1cm} \\beta_j\\hspace{0.1cm} \\text{is non-zero.}$$\n",
    "\n",
    "The hypothesis test is perfomed by computing the F-statistic\n",
    "\n",
    "$$F=\\frac{(TSS - RSS)/p}{RSS/(n-p-1)}$$\n",
    "\n",
    "where $TSS=\\sum_{i=1}^n(y_i - \\bar{y})^2$ and $RSS=\\sum_{i=1}^n(y_i - \\hat{y})^2$. If the \n",
    "linear model assumptions are correct then \n",
    "\n",
    "$$\\mathbb{E}[RSS/(n-p-1)] = \\sigma^2$$\n",
    "\n",
    "and if $H_0$ is true then \n",
    "\n",
    "$$\\mathbb{E}[(TSS - RSS)/p] = \\sigma^2.$$\n",
    "\n",
    "The $RSS$ is \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "RSS &= Tr\\left( (Y-\\hat{Y})^T(Y - \\hat{Y})\\right)\\\\ \n",
    "&= Tr\\left( (Y-\\hat{Y})(Y - \\hat{Y})^T\\right)\\\\\n",
    "&= Tr\\left((I-P)YY^T(I-P)^T\\right)\\\\\n",
    "&= Tr\\left((I-P)(X\\beta + \\varepsilon)(X\\beta + \\varepsilon)^T(I-P)^T\\right)\\\\\n",
    "&= Tr\\left((X\\beta + \\varepsilon - PX\\beta -P\\varepsilon)(\\beta^TX^T + \\varepsilon^T-\\beta^TX^TP^T - \\varepsilon^T P^T)\\right) \\\\\n",
    "& = Tr\\left(\\varepsilon - P\\varepsilon)(\\varepsilon^T - \\varepsilon^T P^T\\right)\\\\\n",
    "& = Tr\\left(\\varepsilon\\varepsilon^T -P\\varepsilon\\varepsilon^T - \\varepsilon\\varepsilon^TP^T + P\\varepsilon\\varepsilon^TP^T\\right).\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Let's take the expectation of both sides and $P = P^T$ so we have \n",
    "\n",
    "\n",
    "$$\\mathbb{E}[RSS] = \\mathbb{E}[Tr(\\varepsilon\\varepsilon^T) - Tr(P\\varepsilon\\varepsilon^T) - Tr(\\varepsilon\\varepsilon^TP) + Tr(P\\varepsilon\\varepsilon^TP)]$$\n",
    "\n",
    "and since $Tr(\\varepsilon\\varepsilon^TP)=Tr(P\\varepsilon\\varepsilon^T)$  \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbb{E}[RSS] &=\\mathbb{E}[Tr(\\varepsilon\\varepsilon^T)] - 2\\mathbb{E}[Tr(P\\varepsilon\\varepsilon^T)] + \\mathbb{E}[Tr(P\\varepsilon\\varepsilon^TP)]\\\\ \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "and using the linearity of expectation \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbb{E}[RSS]&=Tr(\\mathbb{E}[\\varepsilon\\varepsilon^T]) - 2Tr(P\\mathbb{E}[\\varepsilon\\varepsilon^T]) + Tr(P\\mathbb{E}[\\varepsilon\\varepsilon^T]P)\\\\ \n",
    "&=Tr(\\sigma^2I) - 2Tr(P\\sigma^2I) + Tr(P\\sigma^2IP) \\\\\n",
    "&=\\sigma^2Tr(I) - 2\\sigma^2Tr(P) + \\sigma^2Tr(P^2) \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "and $P$ is idempotent since $P^2 =X(X^TX)^{-1}X^T \\left(X(X^TX)^{-1}X^T\\right)=X(X^TX)^{-1}X^T=P$ so\n",
    "\n",
    "$$\\mathbb{E}[RSS]=\\sigma^2Tr(I_{n\\times n}) - \\sigma^2Tr(P)$$\n",
    "\n",
    "and since $I \\in \\mathbb{R}^{n\\times n}$ and $X$ has $p$ predictors and an intercept then\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbb{E}[RSS]&=\\sigma^2n - \\sigma^2(p + 1) \\\\\n",
    "&=(n - p - 1)\\sigma^2\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "and finally\n",
    "\n",
    "$$\\frac{\\mathbb{E}[RSS]}{(n - p - 1)} = \\sigma^2.$$\n",
    "\n",
    "$$\\boxed{\\mathbb{E}[RSS] = (n - p - 1)\\sigma^2}$$\n",
    "A shorter derivation is \n",
    "\n",
    "$$RSS=\\bf{e}^T\\bf{e}=((I−P)Y)^T(I−P)Y=Y^T(I−P)^T(I−P)Y$$ \n",
    "\n",
    "and $P$ is symmetric such that $(I−P)^T=I−P$ and idempotent such that $(I-P)^2 =I-P$ therefore \n",
    "\n",
    "$$RSS = Y^T(I−P)Y.$$\n",
    "\n",
    "Since $Y=X\\beta + \\varepsilon$ and $(I-P)X\\beta=(X\\beta - X\\beta)=0$ you get \n",
    "\n",
    "$$RSS = \\varepsilon^T(I−P)\\varepsilon$$\n",
    "\n",
    "and then proceed as before by taking the expectation of both sides. \n",
    "\n",
    "The total sum of squares around the sample mean, $TSS$, you’re effectively fitting the intercept-only model\n",
    "$$\n",
    "\\quad\n",
    "Y_i = \\beta_0 + \\varepsilon_i,\n",
    "\\quad \\hat\\beta_0 = \\bar Y.\n",
    "$$\n",
    "\n",
    "and $\\hat{\\beta}_0=\\bar{Y}$. In population terms we write $\\beta_0=\\mu$\n",
    "\n",
    "$$\n",
    "\\quad\n",
    "\\beta_0 = \\mu,\n",
    "\\quad Y_i = \\mu + \\varepsilon_i,\n",
    "\\quad \\varepsilon_i \\sim N(0,\\sigma^2).\n",
    "$$\n",
    "\n",
    "and the residuals are \n",
    "$$\n",
    "\\quad\n",
    "e_i = Y_i - \\bar Y,\n",
    "\\quad\n",
    "\\mathrm{TSS} = \\sum_{i=1}^n e_i^2\n",
    "= \\sum_{i=1}^n (Y_i - \\bar Y)^2.\n",
    "$$\n",
    "\n",
    "Hence we start the derivation from $Y_i = \\mu + \\varepsilon_i$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathrm{TSS}\n",
    "&= \\sum_{i=1}^n (Y_i - \\bar Y)^2\n",
    "= \\sum_{i=1}^n \\bigl(Y_i^2 - 2\\,Y_i\\,\\bar Y + \\bar Y^2\\bigr)\\\\[4pt]\n",
    "&= \\sum_{i=1}^n Y_i^2 \\;-\\; 2\\,\\bar Y\\sum_{i=1}^n Y_i \\;+\\; n\\,\\bar Y^2\n",
    "  = \\sum_{i=1}^n Y_i^2 \\;-\\; n\\,\\bar Y^2,\\\\[6pt]\n",
    "\\mathbb{E}[\\mathrm{TSS}]\n",
    "&= \\mathbb{E}\\Bigl[\\sum_{i=1}^n Y_i^2\\Bigr]\n",
    "  - n\\,\\mathbb{E}[\\bar Y^2].\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "and so we get \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbb{E}[Y_i^2]\n",
    "&= Var(Y_i) + (\\mathbb{E}[Y_i])^2\n",
    "= \\sigma^2 + \\mu^2\n",
    "\\;\\Longrightarrow\\;\n",
    "\\mathbb{E}\\Bigl[\\sum_i Y_i^2\\Bigr] = n(\\sigma^2+\\mu^2),\\\\[6pt]\n",
    "\\bar Y\n",
    "&= \\mu + \\tfrac{1}{n}\\sum_i \\varepsilon_i,\n",
    "\\quad Var(\\bar Y)=\\tfrac{\\sigma^2}{n}\n",
    "\\;\\Longrightarrow\\;\n",
    "\\mathbb{E}[\\bar Y^2] = Var(\\bar Y) + \\mathbb{E}[\\bar{Y}]^2=\\tfrac{\\sigma^2}{n} + \\mu^2,\\\\[6pt]\n",
    "\\mathbb{E}[\\mathrm{TSS}]\n",
    "&= n(\\sigma^2+\\mu^2) - n\\Bigl(\\tfrac{\\sigma^2}{n} + \\mu^2\\Bigr)\n",
    "= (n-1)\\,\\sigma^2.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\boxed{\\mathbb{E}[\\mathrm{TSS}] = (n-1)\\,\\sigma^2.}\n",
    "$$\n",
    "\n",
    "Also, we can derive the same result using\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathrm{TSS}\n",
    "&= (Y - \\bar Y\\,\\mathbf{1})^T (Y - \\bar Y\\,\\mathbf{1})\n",
    "  = \\operatorname{tr}\\bigl((I - H)\\,Y\\,Y^T\\bigr), \n",
    "  \\quad H = \\tfrac{1}{n}\\,\\mathbf{1}\\,\\mathbf{1}^T, \\\\[6pt]\n",
    "Y\\,Y^T\n",
    "&= (\\mu\\,\\mathbf{1} + \\varepsilon)\\,(\\mu\\,\\mathbf{1} + \\varepsilon)^T\n",
    "  = \\mu^2\\,\\mathbf{1}\\,\\mathbf{1}^T\n",
    "    + \\mu\\,\\mathbf{1}\\,\\varepsilon^T\n",
    "    + \\mu\\,\\varepsilon\\,\\mathbf{1}^T\n",
    "    + \\varepsilon\\,\\varepsilon^T, \\\\[6pt]\n",
    "(I - H)\\,\\mathbf{1} &= 0\n",
    "  \\quad\\Longrightarrow\\quad\n",
    "  (I - H)\\,Y\\,Y^T = (I - H)(\\mu^2\\,\\mathbf{1}\\,\\mathbf{1}^T\n",
    "    + \\mu\\,\\mathbf{1}\\,\\varepsilon^T\n",
    "    + \\mu\\,\\varepsilon\\,\\mathbf{1}^T\n",
    "    + \\varepsilon\\,\\varepsilon^T) = (I - H)\\,\\varepsilon\\,\\varepsilon^T, \\\\[6pt]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "because \n",
    "\n",
    "$$\n",
    "H = \\frac{1}{n}\\,\\mathbf{1}\\,\\mathbf{1}^T\n",
    "\\quad\\Longrightarrow\\quad\n",
    "(I - H)\\,\\mathbf{1}\n",
    "= \\mathbf{1} - \\frac{1}{n}\\,\\mathbf{1}(\\mathbf{1}^T\\mathbf{1})\n",
    "= \\mathbf{1} - \\mathbf{1}\n",
    "= 0\n",
    "$$\n",
    "\n",
    "and we are left wth \n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathrm{TSS}\n",
    "&= \\operatorname{tr}\\bigl((I - H)\\,\\varepsilon\\,\\varepsilon^T\\bigr), \\\\[6pt]\n",
    "\\mathbb{E}[\\mathrm{TSS}]\n",
    "&= \\operatorname{tr}\\!\\bigl((I - H)\\,\\mathbb{E}[\\varepsilon\\,\\varepsilon^T]\\bigr)\n",
    "  = \\operatorname{tr}\\!\\bigl((I - H)\\,\\sigma^2 I\\bigr)\n",
    "  = \\sigma^2\\,\\operatorname{tr}(I - H) \\\\[4pt]\n",
    "&= \\sigma^2\\bigl(\\operatorname{tr}(I)-\\operatorname{tr}(H)\\bigr)\n",
    "  = \\sigma^2\\,(n - 1).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "If the linear assumption is true then\n",
    "\n",
    "$$\\mathbb{E}[RSS/(n-p-1)]=\\sigma^2.$$\n",
    "\n",
    "If $H_0$ is true then \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbb{E}[(TSS - RSS)/p] &= \\frac{1}{p}(\\mathbb{E}[TSS] - \\mathbb{E}[RSS]) \\\\\n",
    "&=\\frac{1}{p}(\\sigma^2(n - 1) - \\sigma^2(n - 1 - p))=\\frac{1}{p}p\\sigma^2=\\sigma^2.\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfcd854",
   "metadata": {},
   "source": [
    "### $\\underline{\\text{References}}$:\n",
    "\n",
    "1. James, G., Witten, D., Hastie, T., Tibshirani, R., & Taylor, J.  \n",
    "   *An Introduction to Statistical Learning: With Applications in Python*.  \n",
    "   Springer, 2023. ISBN: 978-3031387463.\n",
    "\n",
    "2. Murphy, K. P.  \n",
    "   *Probabilistic Machine Learning: An Introduction*.  \n",
    "   MIT Press, 2022. ISBN: 978-0262046824.\n",
    "\n",
    "3. Deisenroth, M. P., Faisal, A. A., & Ong, C. S.  \n",
    "   *Mathematics for Machine Learning*.  \n",
    "   Cambridge University Press, 2020. ISBN: 978-1108455145.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
