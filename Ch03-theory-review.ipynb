{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39306530",
   "metadata": {},
   "source": [
    "### Simple Linear Regression Summary ###\n",
    "\n",
    "Approximate **linear relationship** between $Y$ and $X$ (regress $Y$ on $X$) is:\n",
    "\n",
    "$$ Y \\approx \\beta_0 + \\beta_1 X$$ \n",
    "\n",
    "where $\\beta_0$ is the intercept and $\\beta_1$ is the slope. \n",
    "\n",
    "Use the **training data** to estimate $\\beta_0$ and $\\beta_1$:\n",
    "\n",
    "$$\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1.$$\n",
    "\n",
    "Estimate coefficients using observations pairs consisting of $(x_1, y_1), \\cdots, (x_n, y_n)$\n",
    "\n",
    "Goal is to obtain **coefficient estimates** \n",
    "$\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ such that the linear model fits the data: \n",
    "\n",
    "$\\sum_{i=1}^n \\hat{y}_i \\approx \\sum_{i=1}^n( \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)$\n",
    "\n",
    "\n",
    "**Residual** is difference between the observed response value and the $i$ th response value predicted by the linear model:\n",
    "$$e_i = y_i - \\hat{y}_i.$$ \n",
    "\n",
    "**Residual Sum of Squares ($RSS$)** is minimized using least squares to find the best  $\\hat{\\beta}_0$, $\\hat{\\beta}_1$ in the model. \n",
    "\n",
    "**Best Coefficient Estimate, $\\hat{\\beta_0}$, $\\hat{\\beta_1}$**:\n",
    "\n",
    "$$\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1\\bar{x}$$\n",
    "\n",
    "$$\\hat{\\beta_1} = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i -\\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2}.$$\n",
    "\n",
    "**Sample means**:\n",
    "\n",
    "$\\bar{y} = \\frac{1}{n}\\sum_{i=1}^n y_i$, $\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i$.\n",
    "\n",
    "Note: $\\sum_{i=1}^n y_i = n \\bar{y}$, $\\sum_{i=1}^n x_i = n \\bar{x}$,\n",
    "\n",
    "\n",
    "**True relationship**:\n",
    "\n",
    "$$Y = \\beta_0 + \\beta_1 x + \\epsilon$$\n",
    "\n",
    "where $\\epsilon$ is a non-zero random error or noise. \n",
    "\n",
    "\n",
    "**Population regression line**: is the best linear approx. to the true relationship between $X$ and $Y$, where coefficients $\\beta_0$, $\\beta_1$ define the populaion regression line. \n",
    "\n",
    "**Least squares line**: least squares coefficient estimates characterize this line. The more spread out the $x_i$'s the more precise the slope due the denominotor of $\\hat{\\beta_1}$, $\\sum_{i=1}^n(x_i - \\bar{x})^2$. If points are concentrated we can easily \"turn\" the slope, but if they are spread, we can more easily pin down a slope. For experimental design we prefer $x_i$'s more spread out. \n",
    "\n",
    "**Population mean**: on random variable $Y$ is $\\mu$.\n",
    "\n",
    "**Sample mean**: have access to $n$ observations from $Y$ then a reasonable estimate for $\\mu$ is \n",
    "\n",
    "$$\\hat{\\mu}=\\bar{y}=\\frac{1}{n}\\sum_{i=1}^ny_i$$\n",
    "\n",
    "where $\\hat{\\mu}$ and $\\mu$ are different but $\\hat{\\mu}$ is a good estimate of $\\mu$. The sample mean is a umbiased estimate of the population mean . If we could average a large number of estimates of $\\mu$ from a massive number of observations than averaging $\\hat{\\mu}$ is exactly $\\mu$.\n",
    "\n",
    "\n",
    "**Standard error**: is the average amount the estimate $\\hat{\\mu}$ difers from $\\mu$. The estimate shrinks with $n$\n",
    "\n",
    "$$Var(\\hat\\mu)=SE(\\hat{\\mu})^2=\\frac{\\sigma^2}{n}$$\n",
    "\n",
    "where $\\sigma^2=Var(\\epsilon)$ is the standard deviation of each realization $y_i$ of $Y$. \n",
    "\n",
    "\n",
    "$$SE(\\hat{\\beta}_0)^2 = \\sigma^2 \\left[\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum_{i=1}^n(x_i - \\bar{x})^2} \\right]$$\n",
    "\n",
    "$$SE(\\hat{\\beta}_1)^2 = \\frac{\\sigma^2}{\\sum_{i=1}^n(x_i - \\bar{x})^2}.$$\n",
    "\n",
    "For $SE(\\hat{\\beta}_0)^2$, $SE(\\hat{\\beta}_1)^2$ to be valid the error $\\epsilon_i$ for each observation have common variance $\\sigma^2$ and uncorrelated. \n",
    "\n",
    "**Leverage**: $SE(\\hat{\\beta}_1)$ is smaller if $x_i$ is more spread out, then there is more leverage to estimate the slope. \n",
    "\n",
    "**Residual standard error ($RSE$)**: %\\sigma^2$ is usually not known but can be estimated from data \n",
    "\n",
    "$$RSE = \\sqrt{\\frac{RSS}{n-2}}$$\n",
    "\n",
    "where the $RSS = \\sum_{i=1}^n(y_i - \\hat{y_i}_^2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e60ffd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d362810",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81dd6f06",
   "metadata": {},
   "source": [
    "## Cubic vs. Linear Model ##\n",
    "---\n",
    "\n",
    "For the training model we use least squares to minimizes the sum of squares residuals over a larger parameter space (parameter vector $\\hat{\\beta}$ is 4 by 1 instead of 2 by 1). Minimizing over a larger set of possible solutions can't give a higher minimum RSS, and will at worst reproduce a linear fits by setting the extra coefficients to 0 and can often find a way to reduce the training RSS even further. \n",
    "\n",
    "## Linear Model Fit\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_{\\text{lin}}\n",
    "= \n",
    "\\begin{pmatrix}\n",
    "\\beta_0\\\\\n",
    "\\beta_1\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "minimize\n",
    "\n",
    "$$\n",
    "\\mathrm{RSS}_{\\text{lin}}\n",
    "= \\sum_{i=1}^n \n",
    "\\Bigl[\n",
    "    y_i \\;-\\; (\\beta_0 + \\beta_1\\,x_i)\n",
    "\\Bigr]^2.\n",
    "$$\n",
    "\n",
    "In matrix form, let\n",
    "\n",
    "$$\n",
    "X_{\\text{lin}}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "1 & x_1\\\\\n",
    "1 & x_2\\\\\n",
    "\\vdots & \\vdots\\\\\n",
    "1 & x_n\n",
    "\\end{pmatrix},\n",
    "\\quad\n",
    "y\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "y_1\\\\\n",
    "y_2\\\\\n",
    "\\vdots\\\\\n",
    "y_n\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "Then\n",
    "\n",
    "$$\n",
    "\\mathrm{RSS}_{\\text{lin}}\n",
    "= \n",
    "\\min_{\\beta_0,\\;\\beta_1}\n",
    "\\;\\bigl\\|\\,y \\;-\\; X_{\\text{lin}}\\,\\beta\\bigr\\|^2.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Cubic Model Fit\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_{\\text{cub}}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "\\beta_0\\\\\n",
    "\\beta_1\\\\\n",
    "\\beta_2\\\\\n",
    "\\beta_3\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "minimize\n",
    "\n",
    "$$\n",
    "\\mathrm{RSS}_{\\text{cub}}\n",
    "= \\sum_{i=1}^n \n",
    "\\Bigl[\n",
    "    y_i \n",
    "    \\;-\\; \n",
    "    (\\beta_0 + \\beta_1\\,x_i + \\beta_2\\,x_i^2 + \\beta_3\\,x_i^3)\n",
    "\\Bigr]^2.\n",
    "$$\n",
    "\n",
    "In matrix form, let\n",
    "\n",
    "$$\n",
    "X_{\\text{cub}}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "1 & x_1 & x_1^2 & x_1^3\\\\\n",
    "1 & x_2 & x_2^2 & x_2^3\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots\\\\\n",
    "1 & x_n & x_n^2 & x_n^3\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "Then\n",
    "\n",
    "$$\n",
    "\\mathrm{RSS}_{\\text{cub}}\n",
    "= \n",
    "\\min_{\\beta_0,\\;\\beta_1,\\;\\beta_2,\\;\\beta_3}\n",
    "\\;\\bigl\\|\\,y \\;-\\; X_{\\text{cub}}\\,\\beta\\bigr\\|^2.\n",
    "$$\n",
    "\n",
    "## Minimizing Over a Larger Set\n",
    "\n",
    "In least squares, \n",
    "$$\n",
    "\\hat{\\beta}_{\\text{cub}}\n",
    "\\text{ is chosen to minimize }\n",
    "\\|\\,y - X_{\\text{cub}}\\,\\beta\\|^2\n",
    "\\text{ over }\n",
    "\\mathbb{R}^4.\n",
    "$$\n",
    "\n",
    "Because $\\mathbb{R}^2$ (linear parameters) is embedded in $\\mathbb{R}^4$ (cubic parameters) via\n",
    "$$\n",
    "(\\beta_0,\\,\\beta_1)\n",
    "\\;\\mapsto\\;\n",
    "(\\beta_0,\\,\\beta_1,\\,0,\\,0),\n",
    "$$\n",
    "the minimum over the bigger space can only be smaller or equal to the minimum over the smaller space. Symbolically:\n",
    "\n",
    "$$\n",
    "\\min_{(\\beta_0,\\beta_1,\\beta_2,\\beta_3)\\in \\mathbb{R}^4}\n",
    "\\;\\bigl\\|\\,y - X_{\\text{cub}}\\,\\beta\\bigr\\|^2\n",
    "\\;\\;\\le\\;\\;\n",
    "\\min_{(\\beta_0,\\beta_1)\\in \\mathbb{R}^2}\n",
    "\\;\\bigl\\|\\,y - X_{\\text{cub}}(\\beta_0,\\beta_1,0,0)\\bigr\\|^2.\n",
    "$$\n",
    "\n",
    "But\n",
    "$$\n",
    "X_{\\text{cub}}(\\beta_0,\\,\\beta_1,\\,0,\\,0)\n",
    "= \n",
    "X_{\\text{lin}}(\\beta_0,\\,\\beta_1).\n",
    "$$\n",
    "\n",
    "For any $(\\beta_0,\\beta_1)$, the vector of fitted values in the cubic model collapses to the linear model’s fitted values if $\\beta_2=0,\\;\\beta_3=0$. Hence:\n",
    "$$\n",
    "\\bigl\\|\\,y - X_{\\text{cub}}(\\beta_0,\\beta_1,0,0)\\bigr\\|^2\n",
    "=\n",
    "\\bigl\\|\\,y - X_{\\text{lin}}(\\beta_0,\\beta_1)\\bigr\\|^2.\n",
    "$$\n",
    "\n",
    "Therefore,\n",
    "$$\n",
    "\\min_{\\beta_0,\\beta_1,\\beta_2,\\beta_3}\n",
    "\\bigl\\|\\,y - X_{\\text{cub}}\\,\\beta\\bigr\\|^2\n",
    "\\;\\;\\le\\;\\;\n",
    "\\min_{\\beta_0,\\beta_1}\n",
    "\\bigl\\|\\,y - X_{\\text{lin}}(\\beta_0,\\beta_1)\\bigr\\|^2.\n",
    "$$\n",
    "\n",
    "By definition, these minima are precisely the **training RSS** for each model:\n",
    "\n",
    "$$\n",
    "\\mathrm{RSS}_{\\text{cub}}\n",
    "\\;\\le\\;\n",
    "\\mathrm{RSS}_{\\text{lin}}.\n",
    "$$\n",
    "\n",
    "\n",
    "Let’s use 4 observations:\n",
    "\n",
    "$$\n",
    "x = [\\,1,\\;2,\\;3,\\;4\\,], \n",
    "\\quad\n",
    "y = [\\,2,\\;3,\\;6,\\;9\\,].\n",
    "$$\n",
    "\n",
    "So we have \\(n=4\\) data points:\n",
    "$$\n",
    "(x_1,\\,y_1)=(1,\\,2), \n",
    "\\quad\n",
    "(x_2,\\,y_2)=(2,\\,3), \n",
    "\\quad\n",
    "(x_3,\\,y_3)=(3,\\,6), \n",
    "\\quad\n",
    "(x_4,\\,y_4)=(4,\\,9).\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Design Matrices\n",
    "\n",
    "The linear model is\n",
    "\n",
    "$$\n",
    "Y = \\beta_0 \\;+\\; \\beta_1\\,X \\;+\\; \\varepsilon.\n",
    "$$\n",
    "\n",
    "In matrix form, we write $X_{\\text{lin}}$ as\n",
    "\n",
    "$$\n",
    "X_{\\text{lin}}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "1 & 1\\\\\n",
    "1 & 2\\\\\n",
    "1 & 3\\\\\n",
    "1 & 4\n",
    "\\end{pmatrix}_{4\\times 2}.\n",
    "$$\n",
    "\n",
    "- The first column is all 1’s (for $\\beta_0$).\n",
    "- The second column is $x_i$.\n",
    "\n",
    "\n",
    "The cubic model is\n",
    "\n",
    "$$\n",
    "Y = \\beta_0 \\;+\\; \\beta_1\\,X \\;+\\; \\beta_2\\,X^2 \\;+\\; \\beta_3\\,X^3 \\;+\\; \\varepsilon.\n",
    "$$\n",
    "\n",
    "Hence $X_{\\text{cub}}$ is\n",
    "\n",
    "$$\n",
    "X_{\\text{cub}}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "1 & 1 & 1^2 & 1^3\\\\\n",
    "1 & 2 & 2^2 & 2^3\\\\\n",
    "1 & 3 & 3^2 & 3^3\\\\\n",
    "1 & 4 & 4^2 & 4^3\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "1 & 1 & 1 & 1\\\\\n",
    "1 & 2 & 4 & 8\\\\\n",
    "1 & 3 & 9 & 27\\\\\n",
    "1 & 4 & 16 & 64\n",
    "\\end{pmatrix}_{4\\times 4}.\n",
    "$$\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e477d1d9",
   "metadata": {},
   "source": [
    "When you switch to test $RSS$ there is no gurantee that the cubic model will have equal of lower $RSS$ than the linear model. In fact, the test $RSS$ can get worse if you add parameters due to $\\bold{overfitting}$.\n",
    "\n",
    "### Overfitting:\n",
    "\n",
    "When the cubic model uses extra coefficients $\\beta_2$ and $\\beta_3$ it can chase some random noise in the training set that doesn't generalize. This might reduce the training $RSS$ but might not improve predictions on new data. Sometimes the cubic model might capture the true relationship (especially if it's nonlinear!), giving it a lower $RSS$ than the linear model. But these extra parameters can cause overfitting, causing the test $RSS$ to go up for the cubic model. Therefore, unlike training $RSS$, the test $RSS$ can increase, decrease, or be approximately equal when adding parameters. \n",
    "\n",
    "### Bias-Variance Trade-Off: \n",
    "The expected test MSE for $x_0$ is \n",
    "\n",
    "$$Test\\:MSE(\\hat{f}) =\\mathbb{E}(y_0 - \\hat{f}(x_0))^2=\\underbrace{Bias^2[\\hat{f}(x_0)]}_\\text{systematic deviation} + \\underbrace{Var[\\hat{f}(x_0)]}_\\text{variance} + \\text{irreducible error}$$\n",
    "\n",
    "where $\\text{irreducible error}=Var(\\epsilon).$ Adding parameters like $\\beta_2, \\beta_3$ usually reduces bias but increases variance. Therefore, if the variance is greater than the bias reduction, then your test error can go up. On the training set, the variance does not cause you to do worse, since more parameters allow you to fit atleast as well, so training RSS never rises. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bee161",
   "metadata": {},
   "source": [
    "---\n",
    "Show for \n",
    "\n",
    "$$y_i = \\beta_0 + \\beta_1 x_i$$\n",
    "\n",
    "the $\\hat{\\beta}_0$, $\\hat{\\beta}_1$ that minimize $RSS$ using least squares is \n",
    "\n",
    "$$\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n(x_i - \\bar{x})}$$  \n",
    "\n",
    "$$\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1\\bar{x}$$\n",
    "\n",
    "where $\\bar{y} = \\frac{1}{n}\\sum_{i=1}^ny_i$, $\\bar{x} = \\frac{1}{n}\\sum_{i=1}^nx_i$.\n",
    "\n",
    "The residual or error for the $ith$ observation or sample between the actual and estimate is: \n",
    "\n",
    "$e_i = y_i - \\hat{y_i}$\n",
    "\n",
    "and for $n$ observations the $RSS$ or residual sum of squares is \n",
    "\n",
    "$$\\begin{align*}\n",
    "RSS &= \\sum_{i=1}^n e_i^2 \\\\\n",
    "    &= e_1^2 + \\dots + e_n^2 \\\\\n",
    "    &= (y_1 - \\hat{y}_1)^2 + \\dots + (y_n - \\hat{y}_n)^2\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "We now must minimize the $RSS$ with respect to $\\hat{\\beta}_0$, $\\hat{\\beta}_1$:\n",
    "\n",
    "$$\\begin{align*}\n",
    "RSS(\\hat{\\beta}_0,\\hat{\\beta}_1) &=(y_1 - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_1)^2 + \\dots + (y_n - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_n)^2\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "such that \n",
    "\n",
    "$$\\begin{align*}\n",
    "\\frac{\\partial RSS(\\hat{\\beta}_0,\\hat{\\beta}_1)}{\\partial \\hat{\\beta}_0} &= 2(y_1 - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_1)\\cdot(-1) + \\dots + 2(y_n - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_n)\\cdot(-1) \\\\\n",
    "&=2\\sum_{i=1}^n (\\hat{\\beta}_0 +\\hat{\\beta}_1 x_i - y_i) = 0  \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\frac{\\partial RSS(\\hat{\\beta}_0,\\hat{\\beta}_1)}{\\partial \\hat{\\beta}_1} &= 2(y_1 - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_1)\\cdot(-x_1) + \\dots + 2(y_n - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_n)\\cdot(-x_n) \\\\\n",
    "&=2\\sum_{i=1}^n x_i (\\hat{\\beta}_0 +\\hat{\\beta}_1 x_i - y_i) = 0  \n",
    "\\end{align*}\n",
    "$$ \n",
    "\n",
    "and we now have the following 2 equations: \n",
    "\n",
    "$\\frac{\\partial RSS(\\hat{\\beta}_0,\\hat{\\beta}_1)}{\\partial \\hat{\\beta}_0}=\\sum_{i=1}^n (\\hat{\\beta}_0 +\\hat{\\beta}_1 x_i - y_i)=0$\n",
    "\n",
    "$\\frac{\\partial RSS(\\hat{\\beta}_0,\\hat{\\beta}_1)}{\\partial \\hat{\\beta}_1}=\\sum_{i=1}^nx_i (\\hat{\\beta}_0 +\\hat{\\beta}_1 x_i - y_i) = 0 $.\n",
    "\n",
    "The first equation we can use the fact that $n\\hat{\\beta}_0=\\sum_{i=1}^n\\hat{\\beta}_0$ and $\\bar{x} = \\frac{1}{n}\\sum_{i=1}^nx_i$ then we have and solve for $\\hat{\\beta}_0$: \n",
    "\n",
    "$$\\begin{align*}\n",
    "\\hat{\\beta}_0 &= \\frac{1}{n}\\sum_{i=1}^ny_i - \\frac{\\hat{\\beta}_1}{n}\\sum_{i=1}^nx_i \\\\\n",
    "             &= \\bar{y} - \\hat{\\beta}_1\\bar{x}\n",
    "\\end{align*}\n",
    "$$.\n",
    "\n",
    "For the second equation we can identify the following two factorizations: \n",
    "\n",
    "$$\\begin{align*}\n",
    "\\sum_{i=1}^n(x_i - \\bar{x})(x_i - \\bar{x}) &= \\sum x_i^2 - \\bar{x}\\sum x_i - \\bar{x}\\sum x_i + \\sum \\bar{x}^2 \\\\\n",
    "&= \\sum x_i^2 - n\\bar{x}^2 - n\\bar{x}^2 + n\\bar{x}^2 \\\\ \n",
    "&= \\sum x_i^2 - n\\bar{x}^2\n",
    "\\end{align*}\n",
    "$$.\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\sum_{i=1}^n(x_i - \\bar{x})(y_i - \\bar{y}) &= \\sum x_iy_i - \\bar{x}\\sum y_i - \\bar{y}\\sum x_i + \\sum \\bar{x}\\bar{y} \\\\\n",
    "&= \\sum x_iy_i  - n\\bar{x}\\bar{y} - n\\bar{y}\\bar{x} + n\\bar{x}\\bar{y}\\\\ \n",
    "&= \\sum x_iy_i - n\\bar{x}\\bar{y}\n",
    "\\end{align*}\n",
    "$$.\n",
    "\n",
    "Then from the second equation we have: \n",
    "\n",
    "$$\\begin{align*}\n",
    "\\sum_{i=1}^n x_i y_i &= \\hat{\\beta_0}\\sum_{i=1}^nx_i + \\hat{\\beta_1}\\sum_{i=1}^n x_i^2 \\\\\n",
    " &= (\\bar{y} - \\hat{\\beta}_1 \\bar{x})\\sum_{i=1}^nx_i + \\hat{\\beta_1}\\sum_{i=1}^n x_i^2 \\\\\n",
    " &=(\\bar{y} - \\hat{\\beta}_1 \\bar{x})(n\\bar{x}) + \\hat{\\beta_1}\\sum_{i=1}^n x_i^2 \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Solving for $\\hat{\\beta_1}$:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\hat{\\beta_1}(\\sum_{i=1}^n x_i^2 - n\\bar{x}^2) = \\sum_{i=1}^n x_i y_i - n\\bar{x}\\bar{y} \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "and using the previous two factorizations we have: \n",
    "\n",
    "$$\\hat{\\beta_1}\\sum_{i=1}^n (x_i - \\bar{x})^2 = \\sum_{i=1}^n (x_i - \\bar{x})(y_i -\\bar{y})$$ \n",
    "\n",
    "and solving for $\\hat{\\beta_1}$:\n",
    "\n",
    "$$\\hat{\\beta_1} = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i -\\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360d68fe",
   "metadata": {},
   "source": [
    "---\n",
    "Let's use the matrix gradient the derivation of OLS using the squared norm (same as RSS). First, let's elaborate the following facts for linear quadratic functions of $\\beta$: \n",
    "\n",
    "For any $\\mathbf{c} \\in \\mathbb{R}^n$, we have $\\nabla_{\\beta}(\\mathbf{c}^T\\mathbf{\\beta})=\\mathbf{c}$.\n",
    "\n",
    "For any $X \\in \\mathbb{R}^{n\\times n}$, we have $\\nabla_{\\beta}(\\mathbf{\\beta}^TX\\mathbf{\\beta})=(X + X^T)\\beta$.\n",
    "\n",
    "We are trying to minimize the squared norm with respect to $\\beta$: \n",
    "$$\\begin{align*}\n",
    "0 &= \\nabla||\\bold{y} - X\\bold{\\beta}||^2 \\\\\n",
    "&=\\nabla (\\bold{y} - X\\beta)^T(\\bold{y} - X\\beta)\\\\\n",
    "&= \\nabla (\\bold{y}^T - \\beta^TX^T)(\\bold{y} - X\\beta)\\\\\n",
    "&= \\nabla(\\bold{y}^T\\bold{y} - \\beta^TX^T\\bold{y} - \\bold{y}^TX\\beta +\\beta^TX^TX\\beta)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Since\n",
    "$\\beta \\in \\mathbb{R}^p$, $\\mathbf{y} \\in \\mathbb{R}^p$, and $X \\in \\mathbb{R}^{n\\times p}$, then $\\beta^TX^T\\bold{y}=\\bold{y}^TX\\beta$. We can readily take the transpose since they both result in the same scalar quantity. Continuing on we get \n",
    "\n",
    "$$\\begin{align*}\n",
    "0 &= \\nabla(\\bold{y}^T\\bold{y} - \\beta^TX^T\\bold{y} - \\bold{y}^TX\\beta +\\beta^TX^TX\\beta)\\\\\n",
    "&= \\nabla(\\bold{y}^T\\bold{y} - 2\\beta^TX^T\\bold{y} + \\beta^TX^TX\\beta) \\\\\n",
    "&= - 2X^T\\bold{y} + (X^TX + X^TX)\\beta \\\\\n",
    "&= - 2X^T\\bold{y}  + 2X^TX\\beta\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Solving for $\\beta$ we arrive at \n",
    "\n",
    "$$\\beta = (X^TX)^{-1}X^T\\bold{y}.$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
