{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39306530",
   "metadata": {},
   "source": [
    "### $\\underline{\\text{Simple Linear Regression Summary:}}$\n",
    "\n",
    "Approximate **linear relationship** between $Y$ and $X$ (regress $Y$ on $X$) is:\n",
    "\n",
    "$$ Y \\approx \\beta_0 + \\beta_1 X$$ \n",
    "\n",
    "where $\\beta_0$ is the intercept and $\\beta_1$ is the slope. \n",
    "\n",
    "Use the **training data** to estimate $\\beta_0$ and $\\beta_1$:\n",
    "\n",
    "$$\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1.$$\n",
    "\n",
    "Estimate coefficients using observations pairs consisting of $(x_1, y_1), \\cdots, (x_n, y_n)$\n",
    "\n",
    "Goal is to obtain **coefficient estimates** \n",
    "$\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ such that the linear model fits the data: \n",
    "\n",
    "$$\\sum_{i=1}^n \\hat{y}_i \\approx \\sum_{i=1}^n( \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)$$\n",
    "\n",
    "\n",
    "**Residual** is difference between the observed response value and the $i$ th response value predicted by the linear model:\n",
    "$$e_i = y_i - \\hat{y}_i.$$ \n",
    "\n",
    "**Residual Sum of Squares ($RSS$)** is minimized using least squares to find the best  $\\hat{\\beta}_0$, $\\hat{\\beta}_1$ in the model. \n",
    "\n",
    "**Best Coefficient Estimate, $\\hat{\\beta_0}$, $\\hat{\\beta_1}$**:\n",
    "\n",
    "$$\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1\\bar{x}$$\n",
    "\n",
    "$$\\hat{\\beta_1} = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i -\\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2}.$$\n",
    "\n",
    "**Sample means**:\n",
    "\n",
    "$$\\bar{y} = \\frac{1}{n}\\sum_{i=1}^n y_i$$\n",
    "\n",
    "$$\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i$$\n",
    "\n",
    "Note: $\\sum_{i=1}^n y_i = n \\bar{y}$, $\\sum_{i=1}^n x_i = n \\bar{x}$,\n",
    "\n",
    "\n",
    "**True relationship**:\n",
    "\n",
    "$$Y = \\beta_0 + \\beta_1 x + \\epsilon$$\n",
    "\n",
    "where $\\epsilon$ is a non-zero random error or noise. \n",
    "\n",
    "\n",
    "**Population regression line**: is the best linear approx. to the true relationship between $X$ and $Y$, where coefficients $\\beta_0$, $\\beta_1$ define the populaion regression line. \n",
    "\n",
    "**Least squares line**: least squares coefficient estimates characterize this line. The more spread out the $x_i$'s the more precise the slope due the denominotor of $\\hat{\\beta_1}$, $\\sum_{i=1}^n(x_i - \\bar{x})^2$. If points are concentrated we can easily \"turn\" the slope, but if they are spread, we can more easily pin down a slope. For experimental design we prefer $x_i$'s more spread out. \n",
    "\n",
    "**Population mean**: on random variable $Y$ is $\\mu$.\n",
    "\n",
    "**Sample mean**: have access to $n$ observations from $Y$ then a reasonable estimate for $\\mu$ is \n",
    "\n",
    "$$\\hat{\\mu}=\\bar{y}=\\frac{1}{n}\\sum_{i=1}^ny_i$$\n",
    "\n",
    "where $\\hat{\\mu}$ and $\\mu$ are different but $\\hat{\\mu}$ is a good estimate of $\\mu$. The sample mean is a umbiased estimate of the population mean . If we could average a large number of estimates of $\\mu$ from a massive number of observations than averaging $\\hat{\\mu}$ is exactly $\\mu$.\n",
    "\n",
    "\n",
    "**Standard error**: is the average amount the estimate $\\hat{\\mu}$ difers from $\\mu$. The estimate shrinks with $n$\n",
    "\n",
    "$$Var(\\hat\\mu)=SE(\\hat{\\mu})^2=\\frac{\\sigma^2}{n}$$\n",
    "\n",
    "where $\\sigma^2=Var(\\epsilon)$ is the standard deviation of each realization $y_i$ of $Y$. \n",
    "\n",
    "\n",
    "$$SE(\\hat{\\beta}_0)^2 = \\sigma^2 \\left[\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum_{i=1}^n(x_i - \\bar{x})^2} \\right]$$\n",
    "\n",
    "$$SE(\\hat{\\beta}_1)^2 = \\frac{\\sigma^2}{\\sum_{i=1}^n(x_i - \\bar{x})^2}.$$\n",
    "\n",
    "For $SE(\\hat{\\beta}_0)^2$, $SE(\\hat{\\beta}_1)^2$ to be valid the error $\\epsilon_i$ for each observation have common variance $\\sigma^2$ and uncorrelated. \n",
    "\n",
    "**Leverage**: $SE(\\hat{\\beta}_1)$ is smaller if $x_i$ is more spread out, then there is more leverage to estimate the slope. \n",
    "\n",
    "**Residual standard error ($RSE$)**: $\\sigma^2$ is usually not known but can be estimated from data \n",
    "\n",
    "$$RSE = \\sqrt{\\frac{RSS}{n-p-1}}$$\n",
    "\n",
    "where the $RSS = \\sum_{i=1}^n(y_i - \\hat{y_i})^2$. The $RSE$ esimates the standard deviation of $\\epsilon$ and is the average amount the response deviates from the true regression line.  For example if $RSE=3.26$ then the actual sales in each market deviate from the true regression line by $3,260$ units on average. If the mean sales over all markets is approximately $14,000$ units, then the percentage error is $\\frac{3260}{14000}=23\\%$. The RSE is a measure of lack of fit of the model to the data. When $\\sigma^2$ is estimated then we write $\\hat{SE}(\\hat{\\beta}_1)$ for standard error. \n",
    "\n",
    "**Confidence intervals** are a range of values s.t. with $95\\%$ probability this range contains the true unknown value of the parameter: \n",
    "\n",
    "$$\\hat{\\beta}_1 \\pm 2 \\cdot SE(\\hat{\\beta}_1)$$\n",
    "\n",
    "where each confidence interval is drawn from a new population sample.\n",
    "\n",
    "The $95\\%$ confidence interval property we take repeated samples and construct confidence intervals for each sample, then $95\\%$ of the intervals will contain the true unknown  value of the parameter \n",
    "\n",
    "$$[\\hat{\\beta}_1 - 2 \\cdot SE(\\hat{\\beta}_1), \\hat{\\beta}_1 - 2 \\cdot SE(\\hat{\\beta}_1)].$$\n",
    "\n",
    "**Hypothesis tests**: the *null hypothesis* is $H_0:$ there is no relationship between $X$ and $Y$ or $\\beta_1=0$. The *alternative hypothesis* is $H_a:$ there is a relationship between $X$ and $Y$ or $\\beta_1\\neq0$.\n",
    "\n",
    "**T-statistic**: is used to test the null hypothesis, or is $\\hat{\\beta_1}$ sufficient far from $0$ such that $\\beta_1$ is non-zero where it's the number of standard deviations $\\hat{\\beta}$ is from $0$: \n",
    "\n",
    "$$\\hat{t}=\\frac{\\hat{\\beta}_1 - 0}{SE(\\hat{\\beta}_1)}.$$\n",
    "\n",
    "**P-value**: a small p-value infers there is a relationship between $X$ and $Y$ and so we can reject the null hypothesis. The typical cutoff is $0.05$ or $5\\%$ to reject the null hypothesis. \n",
    "\n",
    "**$R^2$ stat**: it's not always clear what a good $RSE$ is and so instead use $R^2$ \n",
    "\n",
    "$$R^2=\\frac{TSS - RSS}{TSS} = 1 - \\frac{RSS}{TSS}$$\n",
    "\n",
    "where $TSS=\\sum(y_i - \\bar{y})^2$ is called the *total sum of squares* and measures the total variance in response $Y$, is the variability in response before regresssion is performed. \n",
    "\n",
    "$R^2$ measures the proportion of variablility in $Y$ that can be explained using $X$, is a measure of the linear relationship between $X$ and $Y$. \n",
    "\n",
    " - $R^2\\approx 1$ means a large proportion of variability in response explained by regression. \n",
    " - $R^2\\approx 0$ regression doesn't explain variability in the response; because the linear model is wrong; or the error variance $\\sigma^2$ is large; or both. \n",
    " - A $R^2=0.612$ tells us that using the predictor, $p$, we reduced the variance in the response, $y$,  by $61\\%$. Therefore, $p$, is a strong predictor of the response, $y$. \n",
    "\n",
    "**Correlation** measures the relation between $Y$ and $X$. \n",
    "\n",
    "$$Cor(X, Y) = \\frac{\\sum_{i=1}^n(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^n(x_i - \\bar{x})^2}\\sqrt{\\sum_{i=1}^n(y_i - \\bar{y})^2}}$$\n",
    "\n",
    "where $r = Cor(X, Y)$ and for simple linear regression $R^2 = r^2$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a055fc4",
   "metadata": {},
   "source": [
    "---\n",
    "### $\\underline{\\text{Multiple Regression Summary:}}$\n",
    "\n",
    "If we have $p$ distinct predictors such that \n",
    "\n",
    "$$y = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p + \\epsilon$$\n",
    "\n",
    "where $\\beta_j$ is the average effect of $Y$ on one unit increase in $X$, holding other predictors fixed. \n",
    "\n",
    "We use the training data to estimate the regression coefficients\n",
    "\n",
    "$$\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_1 + \\cdots + \\hat{\\beta}_p X_p$$\n",
    "\n",
    "and we can choose $\\hat{\\beta}_0$, $\\hat{\\beta}_1$, $\\cdots$, $\\hat{\\beta}_p$ to minimize the residual sum of squares $RSS$\n",
    "\n",
    "$$\\begin{align*}\n",
    "RSS &= \\sum_{i=1}^n(y_i - \\hat{y}_i)^2 \\\\\n",
    "&=\\sum_{i=1}^n[y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 X_1 + \\cdots + \\hat{\\beta}_p X_p)]^2.\n",
    "\\end{align*}$$\n",
    "\n",
    "In multiple regression, a predictor like newspaper ads is not directly related to sales but higher values for the newspaper ads associated is associated with higher sales and so newspaper ads serve as a **surrogate** for another predictor like radio ads so newspaper ads get credit for their association between radio and sales. \n",
    "\n",
    "**Hypothesis test**: for mulitiple regression the null hypothesis is $H_0: \\beta_1 = \\beta_2 = \\cdots = \\beta_p = 0$. The alternative hypothesis is \n",
    "$H_a: \\beta_j \\neq 0$, or atleast one coeffiecient is non-zero.\n",
    "\n",
    "**F-statistic**: for multiple regression this is the hypothesis test \n",
    "\n",
    "$$F = \\frac{(TSS - RSS)/p}{RSS/(n-p-1)}$$\n",
    "\n",
    "where $TSS=\\sum(y_i - \\bar{y})^2$, $RSS=\\sum(y_i - \\hat{y}_i)^2$. \n",
    "\n",
    "- If $H_0$ is true then $F-stat \\sim 1$.\n",
    "\n",
    "- If $H_a$ is true then $\\mathbb{E}[(TSS-RSS)/p]>\\sigma^2\\rightarrow F-stat>1$\n",
    "\n",
    "- If $n$ is large and the $F-stat$ is a little bigger than 1 this can provide evidence against $H_0$ but if $n$ is small, $H_0$ is true the $\\epsilon_i$ errors have a normal distribution, $F-stat$ follows a $F-dist$.\n",
    "\n",
    "- $F-stat$ is good if $p$ is small; if $p>n$ then can't apply the $F-stat$. \n",
    "\n",
    "**Partial effect of adding a variable**: \n",
    "\n",
    "Testing the null hypothesis on a subset of coefficients \n",
    "\n",
    "$$H_0: \\beta_{p-q+1} = \\beta_{p-q+2}= \\cdots = \\beta_p = 0$$\n",
    "\n",
    "and fit a new model that uses all variables except the subset, then $RSS=RSS_0$\n",
    "\n",
    "$$F = \\frac{(RSS_0 - RSS)/q}{RSS/(n-p-1)}$$\n",
    "\n",
    "where this is similar to the $F-test$ that omits single variable from the model while keeping the others to observe the partial effect of adding a variable.\n",
    "\n",
    "**Variable selection**: determines what predictors associated with response to fit single model using only those predictors. Examples of automated methods to decide on best models are: forward selection, backward selection, and mixed selection (covered in later chapters). \n",
    "\n",
    "**$R^2 for multiple regression model** $R^2 \\sim 1$ means that the model explain a large portion of variance in the response variable. The $R^2$ icnreases as more variables are added because adding a variable decreases the residual sum of squares on training data\n",
    "\n",
    "$$RSE = \\sqrt{\\frac{RSS}{n - p -1}}$$\n",
    "\n",
    "where models with more variable can have a higher $RSE$ if a decrease in $ RSS$ is small relative to an increase in $p$. \n",
    "\n",
    "**Interaction effect(synergy)**: is between predictors where combining predictors results in an overestimated response then suing a single predictors. \n",
    "\n",
    "\n",
    "**Prediction**: there are 3 types of prediction uncertainty\n",
    "\n",
    "- *Reducible error*: inaccuracy in the coefficient estimates are due to this type of error. Use confidence intervals to see how close $\\hat{y}=\\hat{\\beta_0} + \\hat{\\beta_1}x_1 + \\cdots + \\hat{\\beta_p}x_p$ is to $f(X)=\\beta_0 + \\beta_1x_1 + \\cdots + \\beta_px_p$. \n",
    "- *Model bias*: is an additional source of reducible error. \n",
    "- *Irreducible error*: true values of $\\beta_1,\\cdots,\\beta_p$ can't be predicted perfectly because of random error $\\epsilon$. Use prediction intervals to quantify how far $y$ is from $\\hat{y}$.\n",
    "\n",
    "**Confidence intervals vs. prediction intervals**: the $95\\%$ confidence interval quantifies uncertainty around average sales over large number of cities (repeated samples drawn). $95\\%$ of these intervals will contain the true value of $f(X)$. The $95\\%$ prediction interval quantifies uncertainty in sales for a particular city. Both intervals are centered at the same value but the prediction interval is wider because there is more uncertainty about sales for a given city versus average sales over many locations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b99a39",
   "metadata": {},
   "source": [
    "---\n",
    "### $\\underline{\\text{Qualitative Predictors:}}$ \n",
    "\n",
    "Suppose that we wish to investigate differences in credit card balance between those who own a house and those who don’t, ignoring the other variables for the moment. If a qualitative predictor (also known as a *factor*) only has two levels, or possible values, then incorporating it into a regression model is very simple. \n",
    "\n",
    "We simply create an **indicator** or **dummy variable** that takes on two possible numerical values. For example, based on the variable `Own`, we can define a new variable:\n",
    "\n",
    "$$\n",
    "x_i = \n",
    "\\begin{cases}\n",
    "1 & \\text{if the $i$th person owns a house} \\\\\\\\\n",
    "0 & \\text{if the $i$th person does not own a house}\n",
    "\\end{cases}\n",
    "\\tag{1}\n",
    "$$\n",
    "\n",
    "We then use this variable as a predictor in the regression model:\n",
    "\n",
    "$$\n",
    "y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i =\n",
    "\\begin{cases}\n",
    "\\beta_0 + \\beta_1 + \\varepsilon_i & \\text{if the $i$th person owns a house} \\\\\\\\\n",
    "\\beta_0 + \\varepsilon_i & \\text{if the $i$th person does not}\n",
    "\\end{cases}\n",
    "\\tag{2}\n",
    "$$\n",
    "\n",
    "Here, $\\beta_0$ represents the **average credit card balance among non-owners**, $\\beta_0 + \\beta_1$ is the **average balance among owners**, and $\\beta_1$ is the **average difference in balance between owners and non-owners**.\n",
    "\n",
    "When a qualitative predictor has more than two levels, a single dummy\n",
    "variable cannot represent all possible values. In this situation, we can create\n",
    "additional dummy variables. For example, for the **region** variable we create\n",
    "two dummy variables. The first could be\n",
    "\n",
    "$$\n",
    "x_{i1} = \n",
    "\\begin{cases}\n",
    "1 & \\text{if the $i$th person is from the South} \\\\\\\\\n",
    "0 & \\text{if the $i$th person is not from the South,}\n",
    "\\end{cases}\n",
    "\\tag{3}\n",
    "$$\n",
    "\n",
    "and the second could be \n",
    "\n",
    "$$\n",
    "x_{i2} = \n",
    "\\begin{cases}\n",
    "1 & \\text{if the $i$th person is from the West} \\\\\\\\\n",
    "0 & \\text{if the $i$th person is not from the West,}\n",
    "\\end{cases}\n",
    "\\tag{4}\n",
    "$$\n",
    "\n",
    "Then both of these variables can be used in the regression equation to obtain the model:\n",
    "\n",
    "$$\n",
    "y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\varepsilon_i =\n",
    "\\begin{cases}\n",
    "\\beta_0 + \\beta_1 + \\varepsilon_i & \\text{if the $i$th person is from the South} \\\\\\\\\n",
    "\\beta_0 + \\beta_2 + \\varepsilon_i & \\text{if the $i$th person is from the West} \\\\\\\\\n",
    "\\beta_0 + \\varepsilon_i & \\text{if the $i$th person is from the East}\n",
    "\\end{cases}\n",
    "\\tag{5}\n",
    "$$\n",
    "\n",
    "Here:\n",
    "\n",
    "- $\\beta_0$ represents the **average credit card balance** for individuals from the **East**.\n",
    "- $\\beta_1$ is the **difference** in average balance between people from the **South** and those from the East.\n",
    "- $\\beta_2$ is the **difference** between those from the **West** and the East.\n",
    "\n",
    "There will always be **one fewer dummy variable than the number of levels**. The level with no dummy variable—**East** in this example—is called the **baseline**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf3ffe2",
   "metadata": {},
   "source": [
    "---\n",
    "### $\\underline{\\text{Extensions of the Linear Model}}$\n",
    "\n",
    "The linear model assumes the *additive assumption*, which roughly the association between predictor $X_j$ and response $Y$ does not depend on other predictors. It also depends on the *linearity assumption* such that the change in $Y$ associated with a one-unit change in $Y_j$ is constant regardless of the value of $X_j$. \n",
    "\n",
    "**Interaction effect**: \n",
    "\n",
    "One way of extending this model is to include a **third predictor**, called an **interaction term**, which is constructed by multiplying $X_1$ and $X_2$. This gives the model:\n",
    "\n",
    "$$\n",
    "Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_1 X_2 + \\varepsilon\n",
    "\\tag{3.31}\n",
    "$$\n",
    "\n",
    "How does the inclusion of this interaction term **relax the additive assumption**?\n",
    "\n",
    "Note that equation (3.31) can be rewritten as:\n",
    "\n",
    "$$\n",
    "Y = \\beta_0 + (\\beta_1 + \\beta_3 X_2) X_1 + \\beta_2 X_2 + \\varepsilon\n",
    "\\tag{3.32}\n",
    "$$\n",
    "\n",
    "Letting:\n",
    "\n",
    "$$\n",
    "\\tilde{\\beta}_1 = \\beta_1 + \\beta_3 X_2,\n",
    "$$\n",
    "\n",
    "we can see that $\\tilde{\\beta}_1$ is now a **function of $X_2$**. This means that the **effect of $X_1$ on $Y$** depends on the value of $X_2$ — the association between $X_1$ and $Y$ is no longer constant.\n",
    "\n",
    "Similarly, the effect of $X_2$ on $Y$ also depends on the value of $X_1$. Therefore, the model allows for **interaction between the predictors**, relaxing the assumption that their effects are strictly additive.\n",
    "\n",
    "We now return to the **Advertising** example. A linear model that uses `radio`, `TV`, and an **interaction** between the two to predict `sales` takes the form:\n",
    "\n",
    "$$\n",
    "\\text{sales} = \\beta_0 + \\beta_1 \\cdot \\text{TV} + \\beta_2 \\cdot \\text{radio} + \\beta_3 \\cdot (\\text{radio} \\times \\text{TV}) + \\varepsilon\n",
    "$$\n",
    "\n",
    "This can be rewritten as:\n",
    "\n",
    "$$\n",
    "\\text{sales} = \\beta_0 + (\\beta_1 + \\beta_3 \\cdot \\text{radio}) \\cdot \\text{TV} + \\beta_2 \\cdot \\text{radio} + \\varepsilon\n",
    "\\tag{3.33}\n",
    "$$\n",
    "\n",
    "From this form, we can interpret $\\beta_3$ as the **change in the effectiveness of TV advertising** associated with a **one-unit increase in radio advertising** — or vice versa.\n",
    "\n",
    "That is, the **marginal effect** of TV on sales depends on the level of radio advertising, and vice versa, due to the interaction term. This captures a **non-additive** relationship between the two predictors.\n",
    "\n",
    "**Nonlinear relationships**: \n",
    "\n",
    "Polynomial regression extends the linear model to non-linear relatioships. By plotting the data we can observe a non-linear relationship between the mpg and horsepower (response and predictor), the quadratic shape suggests the model is \n",
    "\n",
    "$$mpg = \\beta_0 + \\beta_1 \\times horsepower + \\beta_2 \\times horsepower^2 + \\epsilon.$$\n",
    "\n",
    "This involves prediction $mpg$ using nonlinear function of $horsepower$ but this is still a linear model since $X_1 = horsepower$, $X_2=horsepower^2$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec5e764",
   "metadata": {},
   "source": [
    "### $\\underline{\\text{Problems with Linear Regression:}}$\n",
    "\n",
    "1. **Non-linear data**: linear regression assumes a straight-line relationship between predictors and the response variable\n",
    "    - use *residual plots*, $\\hat{y}$ vs. $y_i - \\hat{y}_i$ to identify non-linearity, this plot shows no non-linearity ideally. \n",
    "    - if the residual plot shows a non-linear pattern a simple solution is to use a non-linear transformation $\\log x, \\sqrt{x}, x^2$ in the model. \n",
    "\n",
    "2. **Correlation of error terms**: the standard errors for the estimates are based on assumption that noise $\\epsilon_1, \\epsilon_2, \\cdots, \\epsilon_n$ are uncorrelated. \n",
    "    - if the $\\epsilon_i$'s are correlated then the estimated standard error will underestimate the true standard error, confidence interval will be narrower, p-values smaller, and result in the erroneous conclusion that the parameter is statistically significant. \n",
    "\n",
    "3. **Non-constant variance of error terms**: usually the error terms have constant variance $Var(\\epsilon_i)=\\sigma^2$. \n",
    "    - this non-constant variance is seen as a funnel shape in the residual plot. \n",
    "    - one solution is to transform $Y$ using a concave function like $logY$ or $\\sqrt{Y}$ causing a larger shrinkage of the larger responses. \n",
    "    - another solution is the $i$th response could be the average of $n_i$ raw obseravtions if each observation is uncorrelated with variance $\\sigma^2$ then the average variance is $\\sigma_i^2=\\sigma^2/n_i and fit using a weighted least squares where the weights are proportional to the increase variances or $w_i = n_i$. \n",
    "\n",
    "4. **Outliers**: point for which $y_i$ is far from the value predicted by the model. \n",
    "    - can use the residuals plot to identify the outliers\n",
    "    - can use plot the studentized residuals, which divides each residual $e_i$ by estimated standard error and the observations where studentized residuals > $|3|$ are possible outliers\n",
    "    - outliers can be dut to error in data collection and can be remedied by removing, can also mean missing predictor\n",
    "\n",
    "5. **High leverage points**: when an observation has an unusually high value for $x_i$ such that the predictor value for an observation is large relative to other observations.\n",
    "    - removing high leverage points can impact fit, and identify these points using leverage statistic \n",
    "    \n",
    "    $$h_i = \\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{\\sum_{i'=1}^n(x_{i'} - \\bar{x})^2}$$ \n",
    "\n",
    "    which can be used for simple regression and as $h_i$ increases with distance of $x_i$ from $\\bar{x}$ and statistic is always between \\frac{1}{n} and 1. \n",
    "    - the average leverage for all observations $(p + 1)/n$ so if the observation has a leverage stat $>> (p+1)/n$ then suspect a high leverage point. \n",
    "    - high leverage and high studentized residual is a bad combination! \n",
    "\n",
    "6. **Collinearity**: when predictors are highly correlated with each other (closely related) can be an issue for regression since it's difficult to separate individual effects on the response. \n",
    "    - collinearity reduces the accuracy of estimats, causing the standard error for $\\beta_j$ to grow. \n",
    "    - collinearity reduces the $t-stat$ so may fail to reject $H_0:\\beta_j=0$. \n",
    "    - can detect paris of highly correlated values with *correlation matrix*. \n",
    "    - *multicollinearity* is collinearity between 3 or more variables and so we have to use **variance inflation factor(VIF)**. \n",
    "\n",
    "    $$VIF(\\hat{\\beta_j}) = \\frac{1}{1 - R_{X_j|X_{_j}}^2}$$\n",
    "\n",
    "    - $R_{X_j|X_{_j}}^2$ is $R^2$ from regressing $X_j$ onto all other predictors, VIF = 1 means no multicollinearity, VIF = 5-10 means collinearity. \n",
    "    - one solution is to drop the problematic variable from regression since it's redundant dut to collinearity. \n",
    "    - another solution is to combine collinear variables into single predictor. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81dd6f06",
   "metadata": {},
   "source": [
    "---\n",
    "### $\\underline{\\text{Cubic vs. Linear Model:}}$\n",
    "\n",
    "\n",
    "For the training model we use least squares to minimizes the sum of squares residuals over a larger parameter space (parameter vector $\\hat{\\beta}$ is 4 by 1 instead of 2 by 1). Minimizing over a larger set of possible solutions can't give a higher minimum RSS, and will at worst reproduce a linear fits by setting the extra coefficients to 0 and can often find a way to reduce the training RSS even further. \n",
    "\n",
    "### Linear Model Fit\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_{\\text{lin}}\n",
    "= \n",
    "\\begin{pmatrix}\n",
    "\\beta_0\\\\\n",
    "\\beta_1\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "minimize\n",
    "\n",
    "$$\n",
    "\\mathrm{RSS}_{\\text{lin}}\n",
    "= \\sum_{i=1}^n \n",
    "\\Bigl[\n",
    "    y_i \\;-\\; (\\beta_0 + \\beta_1\\,x_i)\n",
    "\\Bigr]^2.\n",
    "$$\n",
    "\n",
    "In matrix form, let\n",
    "\n",
    "$$\n",
    "X_{\\text{lin}}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "1 & x_1\\\\\n",
    "1 & x_2\\\\\n",
    "\\vdots & \\vdots\\\\\n",
    "1 & x_n\n",
    "\\end{pmatrix},\n",
    "\\quad\n",
    "y\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "y_1\\\\\n",
    "y_2\\\\\n",
    "\\vdots\\\\\n",
    "y_n\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "Then\n",
    "\n",
    "$$\n",
    "\\mathrm{RSS}_{\\text{lin}}\n",
    "= \n",
    "\\min_{\\beta_0,\\;\\beta_1}\n",
    "\\;\\bigl\\|\\,y \\;-\\; X_{\\text{lin}}\\,\\beta\\bigr\\|^2.\n",
    "$$\n",
    "\n",
    "\n",
    "### Cubic Model Fit\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_{\\text{cub}}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "\\beta_0\\\\\n",
    "\\beta_1\\\\\n",
    "\\beta_2\\\\\n",
    "\\beta_3\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "minimize\n",
    "\n",
    "$$\n",
    "\\mathrm{RSS}_{\\text{cub}}\n",
    "= \\sum_{i=1}^n \n",
    "\\Bigl[\n",
    "    y_i \n",
    "    \\;-\\; \n",
    "    (\\beta_0 + \\beta_1\\,x_i + \\beta_2\\,x_i^2 + \\beta_3\\,x_i^3)\n",
    "\\Bigr]^2.\n",
    "$$\n",
    "\n",
    "In matrix form, let\n",
    "\n",
    "$$\n",
    "X_{\\text{cub}}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "1 & x_1 & x_1^2 & x_1^3\\\\\n",
    "1 & x_2 & x_2^2 & x_2^3\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots\\\\\n",
    "1 & x_n & x_n^2 & x_n^3\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "Then\n",
    "\n",
    "$$\n",
    "\\mathrm{RSS}_{\\text{cub}}\n",
    "= \n",
    "\\min_{\\beta_0,\\;\\beta_1,\\;\\beta_2,\\;\\beta_3}\n",
    "\\;\\bigl\\|\\,y \\;-\\; X_{\\text{cub}}\\,\\beta\\bigr\\|^2.\n",
    "$$\n",
    "\n",
    "### Minimizing Over a Larger Set\n",
    "\n",
    "In least squares, \n",
    "$$\n",
    "\\hat{\\beta}_{\\text{cub}}\n",
    "\\text{ is chosen to minimize }\n",
    "\\|\\,y - X_{\\text{cub}}\\,\\beta\\|^2\n",
    "\\text{ over }\n",
    "\\mathbb{R}^4.\n",
    "$$\n",
    "\n",
    "Because $\\mathbb{R}^2$ (linear parameters) is embedded in $\\mathbb{R}^4$ (cubic parameters) via\n",
    "$$\n",
    "(\\beta_0,\\,\\beta_1)\n",
    "\\;\\mapsto\\;\n",
    "(\\beta_0,\\,\\beta_1,\\,0,\\,0),\n",
    "$$\n",
    "the minimum over the bigger space can only be smaller or equal to the minimum over the smaller space. Symbolically:\n",
    "\n",
    "$$\n",
    "\\min_{(\\beta_0,\\beta_1,\\beta_2,\\beta_3)\\in \\mathbb{R}^4}\n",
    "\\;\\bigl\\|\\,y - X_{\\text{cub}}\\,\\beta\\bigr\\|^2\n",
    "\\;\\;\\le\\;\\;\n",
    "\\min_{(\\beta_0,\\beta_1)\\in \\mathbb{R}^2}\n",
    "\\;\\bigl\\|\\,y - X_{\\text{cub}}(\\beta_0,\\beta_1,0,0)\\bigr\\|^2.\n",
    "$$\n",
    "\n",
    "But\n",
    "$$\n",
    "X_{\\text{cub}}(\\beta_0,\\,\\beta_1,\\,0,\\,0)\n",
    "= \n",
    "X_{\\text{lin}}(\\beta_0,\\,\\beta_1).\n",
    "$$\n",
    "\n",
    "For any $(\\beta_0,\\beta_1)$, the vector of fitted values in the cubic model collapses to the linear model’s fitted values if $\\beta_2=0,\\;\\beta_3=0$. Hence:\n",
    "$$\n",
    "\\bigl\\|\\,y - X_{\\text{cub}}(\\beta_0,\\beta_1,0,0)\\bigr\\|^2\n",
    "=\n",
    "\\bigl\\|\\,y - X_{\\text{lin}}(\\beta_0,\\beta_1)\\bigr\\|^2.\n",
    "$$\n",
    "\n",
    "Therefore,\n",
    "$$\n",
    "\\min_{\\beta_0,\\beta_1,\\beta_2,\\beta_3}\n",
    "\\bigl\\|\\,y - X_{\\text{cub}}\\,\\beta\\bigr\\|^2\n",
    "\\;\\;\\le\\;\\;\n",
    "\\min_{\\beta_0,\\beta_1}\n",
    "\\bigl\\|\\,y - X_{\\text{lin}}(\\beta_0,\\beta_1)\\bigr\\|^2.\n",
    "$$\n",
    "\n",
    "By definition, these minima are precisely the **training RSS** for each model:\n",
    "\n",
    "$$\n",
    "\\mathrm{RSS}_{\\text{cub}}\n",
    "\\;\\le\\;\n",
    "\\mathrm{RSS}_{\\text{lin}}.\n",
    "$$\n",
    "\n",
    "\n",
    "Let’s use 4 observations:\n",
    "\n",
    "$$\n",
    "x = [\\,1,\\;2,\\;3,\\;4\\,], \n",
    "\\quad\n",
    "y = [\\,2,\\;3,\\;6,\\;9\\,].\n",
    "$$\n",
    "\n",
    "So we have \\(n=4\\) data points:\n",
    "$$\n",
    "(x_1,\\,y_1)=(1,\\,2), \n",
    "\\quad\n",
    "(x_2,\\,y_2)=(2,\\,3), \n",
    "\\quad\n",
    "(x_3,\\,y_3)=(3,\\,6), \n",
    "\\quad\n",
    "(x_4,\\,y_4)=(4,\\,9).\n",
    "$$\n",
    "\n",
    "### Design Matrices\n",
    "\n",
    "The linear model is\n",
    "\n",
    "$$\n",
    "Y = \\beta_0 \\;+\\; \\beta_1\\,X \\;+\\; \\varepsilon.\n",
    "$$\n",
    "\n",
    "In matrix form, we write $X_{\\text{lin}}$ as\n",
    "\n",
    "$$\n",
    "X_{\\text{lin}}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "1 & 1\\\\\n",
    "1 & 2\\\\\n",
    "1 & 3\\\\\n",
    "1 & 4\n",
    "\\end{pmatrix}_{4\\times 2}.\n",
    "$$\n",
    "\n",
    "- The first column is all 1’s (for $\\beta_0$).\n",
    "- The second column is $x_i$.\n",
    "\n",
    "\n",
    "The cubic model is\n",
    "\n",
    "$$\n",
    "Y = \\beta_0 \\;+\\; \\beta_1\\,X \\;+\\; \\beta_2\\,X^2 \\;+\\; \\beta_3\\,X^3 \\;+\\; \\varepsilon.\n",
    "$$\n",
    "\n",
    "Hence $X_{\\text{cub}}$ is\n",
    "\n",
    "$$\n",
    "X_{\\text{cub}}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "1 & 1 & 1^2 & 1^3\\\\\n",
    "1 & 2 & 2^2 & 2^3\\\\\n",
    "1 & 3 & 3^2 & 3^3\\\\\n",
    "1 & 4 & 4^2 & 4^3\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "1 & 1 & 1 & 1\\\\\n",
    "1 & 2 & 4 & 8\\\\\n",
    "1 & 3 & 9 & 27\\\\\n",
    "1 & 4 & 16 & 64\n",
    "\\end{pmatrix}_{4\\times 4}.\n",
    "$$\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e477d1d9",
   "metadata": {},
   "source": [
    "---\n",
    "### $\\underline{\\text{Overfitting:}}$\n",
    "\n",
    "When you switch to test $RSS$ there is no gurantee that the cubic model will have equal of lower $RSS$ than the linear model. In fact, the test $RSS$ can get worse if you add parameters due to $\\bold{overfitting}$.\n",
    "\n",
    "\n",
    "When the cubic model uses extra coefficients $\\beta_2$ and $\\beta_3$ it can chase some random noise in the training set that doesn't generalize. This might reduce the training $RSS$ but might not improve predictions on new data. Sometimes the cubic model might capture the true relationship (especially if it's nonlinear!), giving it a lower $RSS$ than the linear model. But these extra parameters can cause overfitting, causing the test $RSS$ to go up for the cubic model. Therefore, unlike training $RSS$, the test $RSS$ can increase, decrease, or be approximately equal when adding parameters. \n",
    "\n",
    "---\n",
    "### $\\underline{\\text{Bias-Variance Trade-Off:}}$ \n",
    "The expected test MSE for $x_0$ is \n",
    "\n",
    "$$Test\\:MSE(\\hat{f}) =\\mathbb{E}(y_0 - \\hat{f}(x_0))^2=\\underbrace{Bias^2[\\hat{f}(x_0)]}_\\text{systematic deviation} + \\underbrace{Var[\\hat{f}(x_0)]}_\\text{variance} + \\text{irreducible error}$$\n",
    "\n",
    "where $\\text{irreducible error}=Var(\\epsilon).$ Adding parameters like $\\beta_2, \\beta_3$ usually reduces bias but increases variance. Therefore, if the variance is greater than the bias reduction, then your test error can go up. On the training set, the variance does not cause you to do worse, since more parameters allow you to fit atleast as well, so training RSS never rises. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c7885f",
   "metadata": {},
   "source": [
    "---\n",
    "### $\\underline{\\text{Lecture Questions:}}$\n",
    "\n",
    "### Question 1. \n",
    "Run linear regression with a slope estimate of 0.5 and with an estimate standard error of 0.2. What's the largest value of $\\beta$ for which we would NOT reject the null hypothesis (assume a normal approximation to the distribution using $5\\%$ significance level for a 2-sided test; need the critical value for standard normal approximation assumming a large sample assumption)\n",
    "\n",
    "### Answer 1. \n",
    "\n",
    "This is a hypothesis test where we are interested in whethere the parameter is significantly higher or lower than a hypothesized value. We split the $5\\%$ significance level evenly across both tails of the distribution - $2.5\\%$ in the lower tail and $2.5\\%$ in the upper tail.Using a standard normal approximation (Z-test) and assuming a large sample (so the Central Limit Theorem applies), we want to find the Z-values that correspond to cutoffs at $2.5\\%$ in each tail which are quantiles at 0.025 and 0.975 where the values are \n",
    "\n",
    "$z_{0.025}=-1.96$ (lower critical value)\n",
    "\n",
    "$z_{0.975}=+1.96$ (upper critical value)\n",
    "\n",
    "and so the critical region is \n",
    "\n",
    "$$|z| > 1.96.$$\n",
    "\n",
    "We need to use the *z-score*, which tells us how many standard deviations a data point (or estimate) is from the mean (hypothesized value). The equation for the z-score is \n",
    "\n",
    "$$z = \\frac{\\hat{\\beta} - \\beta_0}{SE}$$ \n",
    "\n",
    "which is the same as the t-stat, except the z-score assumes the sample size is large and you use the standard normal distribution. Also, the z-score is used if the population standard deviation is known (rare in practice). The t-stat assumes a small sample size, estimating the standard deviation from data, errors are assumed to follow a normal distribution, and we use a student's t-distribution, which has heavier tails to account for additional uncertainty. Back to our question\n",
    "\n",
    "$$z = \\frac{\\hat{\\beta} - \\beta_0}{SE}\\in[-1.96, 1.96]$$\n",
    "\n",
    "and solve \n",
    "\n",
    "$$-1.96 \\leq \\frac{0.5 - \\beta}{0.2} \\leq 1.96$$\n",
    "\n",
    "and solving for $\\beta$\n",
    "\n",
    "$$0.108 \\leq \\beta \\leq 0.892$$\n",
    "\n",
    "and so the largest value of $\\beta$ for which we would **not** reject the null hypothesis is 0.892. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b767a6d2",
   "metadata": {},
   "source": [
    "### Question 2. \n",
    "T or F? Estimate $\\hat{\\beta_1}$ in linear regression that controls for many variables (many predictors in addition to $x_1$) is a more reliable measure of the casual relationship then $\\hat{\\beta_1}$ from a univariate regression? \n",
    "\n",
    "### Answer 2. \n",
    "\n",
    "Consider two regression models for estimating the effect of $x_1$ on $Y$:\n",
    "\n",
    "1. **Simple regression**:\n",
    "\n",
    "   $$\n",
    "   Y = \\beta_0 + \\beta_1\\,x_1 + \\varepsilon,\\qquad \\hat\\beta_1^{(S)}\n",
    "   $$\n",
    "\n",
    "   This slope $\\hat\\beta_1^{(S)}$ ignores other predictors and thus can suffer from **omitted-variable bias** whenever a true predictor—say $x_2$—is correlated both with $x_1$ and with $Y$.  In that case, $\\hat\\beta_1^{(S)}$ “soaks up” part of $x_2$’s effect, so its expectation no longer equals the true $\\beta_1$. Therefore, $\\mathbb{E}[\\hat{\\beta}_1] \\neq \\beta_1$.\n",
    "\n",
    "2. **Multiple regression**:\n",
    "\n",
    "   $$\n",
    "   Y = \\beta_0 + \\beta_1\\,x_1 + \\beta_2\\,x_2 + \\cdots + \\beta_p\\,x_p + \\varepsilon,\\qquad \\hat\\beta_1^{(M)}\n",
    "   $$\n",
    "\n",
    "   Here $\\hat\\beta_1^{(M)}$ is the **partial slope** of $x_1$ “holding all other $x_j$ fixed.”  If each added predictor truly belongs—i.e., is associated with both $x_1$ and $Y$—then including it removes omitted-variable bias, so\n",
    "\n",
    "   $$\n",
    "   E\\bigl[\\hat\\beta_1^{(M)}\\bigr] \\;=\\; \\beta_1.\n",
    "   $$\n",
    "\n",
    "However, each extra predictor also increases the **variance** of $\\hat\\beta_1^{(M)}$.  In Chapter 3 terms, adding a variable that is highly correlated with $x_1$ inflates $\\text{Var}(\\hat\\beta_1^{(M)})$, which in turn raises its **standard error**.  A larger standard error makes hypothesis tests and confidence intervals for $\\beta_1$ less precise.\n",
    "\n",
    "**Bias–Variance Trade‐Off**\n",
    "\n",
    "* If those additional predictors genuinely explain part of $Y$ that would otherwise be absorbed into $x_1$’s slope, then $\\hat\\beta_1^{(M)}$ corrects **omitted‐variable bias** and is more reliable despite the larger SE.   \n",
    "\n",
    "* If the extra variables are only weakly related to $Y$ or nearly collinear with $x_1$, the **variance inflation** can outweigh any bias reduction, making $\\hat\\beta_1^{(M)}$ less precise than $\\hat\\beta_1^{(S)}$.\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "Whether $\\hat\\beta_1^{(M)}$ is “more reliable” than $\\hat\\beta_1^{(S)}$ depends on the bias–variance trade‐off.  Include only those predictors that meaningfully reduce omitted‐variable bias; otherwise, a simpler model may yield a more precise (though biased) estimate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e12713a",
   "metadata": {},
   "source": [
    "### Question 3. \n",
    "For our model for sales vs. TV interacted with radio, then what's the effect of an additional $1 of radio ad if TV = $50? \n",
    "\n",
    "\n",
    "### Answer 3. \n",
    "\n",
    "The model with the interaction term is\n",
    "\n",
    "$$sales = \\beta_0 + \\beta_1 \\times TV + \\beta_2 \\times radio + \\beta_3 \\times (radio \\times TV) + \\epsilon.$$\n",
    "\n",
    "To find the effect that radio have on sales we take the partial \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial sales}{\\partial radio} &= \\beta_2 + \\beta_3 \\times TV\\\\\n",
    "&= 0.0289 + 0.0011 \\times 50 \\\\\n",
    "&= 0.0839\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The $R^2$ for including the interaction term is $96.8\\%$ compared to $89.7\\%$ without. Therefore, $\\frac{(96.8 - 89.8)}{(100 - 89.7)} = 69\\%$ of the variability in sales that remains after fitting the model has been explained by the interaction term. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bee161",
   "metadata": {},
   "source": [
    "---\n",
    "### $\\underline{\\text{Facts, Derivations, etc:}}$\n",
    "\n",
    "### Expected Value (mean, average, or most likely value):\n",
    "\n",
    "For discrete random values \n",
    "\n",
    "$$\\mathbb{E}[x]  = \\sum_{x\\in\\mathcal{X}} xp(x)= \\mu$$\n",
    "\n",
    "\n",
    "### Sum Rule for Marginals: \n",
    "\n",
    "For discrete random variables \n",
    "\n",
    "$$p(x) = \\sum_{y}p(x, y)$$\n",
    "\n",
    "where $p(x)$ is the marginal distribution of random variable $x$ and $p(x, y)$ is the joint distirbution.\n",
    "\n",
    "### Variance: \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "Var(x) &= \\mathbb{V}[x]\\\\\n",
    "&=\\mathbb{E}[(x - \\mu)^2]\\\\\n",
    "&= \\mathbb{E}[(x^2 - 2x\\mu + \\mu^2)] \\\\\n",
    "&= \\mathbb{E}[x^2] - 2\\mu\\mathbb{E}[x] + \\mathbb{E}[\\mu^2] \\\\\n",
    "&= \\mathbb{E}[x^2] - 2\\mu^2 + \\mu^2 \\\\\n",
    "&= \\mathbb{E}[x^2] - \\mu^2 \\\\\n",
    "&= \\sigma^2\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "note: sometimes $\\mu^2$ is written as $\\mathbb{E}[x]^2.$ \n",
    "\n",
    "### Linear Transformation of a Random Variable: \n",
    "\n",
    "$x$ is a random variable and $y$ is a linear transformation of $x$ where \n",
    "\n",
    "$$y = ax + b.$$\n",
    "\n",
    "The expectation is\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbb{E}[y] &= \\mathbb{E}[ax + b]\\\\\n",
    "&= \\mathbb{E}[ax] + \\mathbb{E}[b] \\\\\n",
    "&= a\\mathbb{E}[x] + b \\\\\n",
    "&= a\\mu + b. \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "And the variance is \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbb{V}[y] &= \\mathbb{E}[y^2] - \\mathbb{E}[y]^2\\\\\n",
    "&= \\mathbb{E}[(ax + b)^2] - \\mathbb{E}[ax + b]\\cdot\\mathbb{E}[ax + b] \\\\\n",
    "&=  \\mathbb{E}[(ax)^2 + 2abx + b^2] - (a\\mu + b)^2 \\\\\n",
    "&= a^2\\mathbb{E}[x^2] + 2ab\\mu + b^2 -[(a\\mu)^2 + 2ab\\mu + b^2] \\\\\n",
    "&= a^2\\mathbb{E}[x^2] - a^2\\mu^2 \\\\\n",
    "&= a^2(\\mathbb{E}[x^2] - \\mu^2) \\\\\n",
    "&= a^2\\mathbb{V}[x]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "### Covariance between $x \\in \\mathbb{R}$ and $y \\in \\mathbb{R}$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "Cov[x, y] &= \\mathbb{E}[(x - \\mathbb{E}[x])(y - \\mathbb{E}[y])]\\\\\n",
    "&= \\mathbb{E}[xy - y\\mathbb{E}[x] -x\\mathbb{E}[y] + \\mathbb{E}[x]\\mathbb{E}[y]] \\\\\n",
    "&= \\mathbb{E}[xy - y\\mu_x - x\\mu_y + \\mu_x\\mu_y] \\\\\n",
    "&= \\mathbb{E}[xy] - 2\\mu_x\\mu_y + \\mu_x\\mu_y \\\\\n",
    "&= \\mathbb{E}[xy] - \\mu_x\\mu_y \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "### Expected value of a random vector, $\\bf{x}\\in\\mathbb{R}^n$:\n",
    "\n",
    "We can calculate this expected value elementwise representing the averages of each component\n",
    "\n",
    "$$\\mathbb{E}[\\bf{x}] = \\begin{bmatrix}\n",
    "\\mathbb{E}[x_1] \\\\\n",
    "\\vdots \\\\ \n",
    "\\mathbb{E}[x_n] \\\\\n",
    "\\end{bmatrix} = \\mathbf{\\mu} \\in \\mathbb{R}^n.\n",
    "$$\n",
    "\n",
    "### Covariance of two random vectors, $\\bf{x}\\in\\mathbb{R}^n$ and $\\bf{y}\\in\\mathbb{R}^m$:\n",
    "$$\n",
    "\\begin{align*}\n",
    "Cov[\\bf{x}, \\bf{y}] &= \\mathbb{E}[(\\bf{x} - \\mathbb{E}[\\bf{x}])(\\bf{y} - \\mathbb{E}[\\bf{y}])^T]\\\\\n",
    "&= \\mathbb{E}[\\bf{x}\\bf{y}^T - \\mathbb{E}[\\bf{x}]\\bf{y}^T - \\bf{x}\\mathbb{E}[\\bf{y}]^T + \\mathbb{E}[\\bf{x}]\\mathbb{E}[\\bf{y}]^T] \\\\\n",
    "&= \\mathbb{E}[\\bf{x}\\bf{y}^T] - \\mu_x\\mu_y^T - \\mu_x\\mu_y^T + \\mu_x\\mu_y^T\\\\\n",
    "&= \\mathbb{E}[\\bf{x}\\bf{y}^T] - \\mu_x\\mu_y^T \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "### Two Random Vectors, $\\bf{x}$ and $\\bf{y}$:\n",
    "\n",
    "$$\\mathbb{E}[\\bf{x} + \\bf{y}] = \\mathbb{E}[\\bf{x}] + \\mathbb{E}[\\bf{y}]$$\n",
    "\n",
    "$$\\mathbb{E}[\\bf{x} - \\bf{y}] = \\mathbb{E}[\\bf{x}] - \\mathbb{E}[\\bf{y}]$$\n",
    "\n",
    "$$Cov[\\bf{x} + \\bf{y}] = Cov[\\bf{x}] + Cov[\\bf{y}] + Cov[\\bf{x}, \\bf{y}] + Cov[\\bf{y}, \\bf{x}]$$\n",
    "\n",
    "### Affine transformation of a random vector: \n",
    "\n",
    "The affine transfomation of random vector $\\bf{x}\\in\\mathbb{R}^n$ is $\\bf{y} = A\\bf{x} + \\bf{b}$ and has the expectation\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\mathbb{E}[\\bf{y}] &= \\mathbb{E}[A\\bf{x} + \\bf{b}] \\\\ \n",
    "&= A\\mathbb{E}[\\bf{x}] +  \\mathbb{E}[\\bf{b}]\\\\\n",
    "&= A\\bf{\\mu} + \\bf{b}.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The covariance is \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "Cov[\\bf{y}] &= \\mathbb{E}[(\\bf{y} - \\mathbb{E}[\\bf{y}])(\\bf{y} - \\mathbb{E}[\\bf{y}])^T]\\\\\n",
    "&= \\mathbb{E}[(A\\bf{x} - A\\mu)(A\\bf{x} - A\\mu)^T]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "and using **linearity of expectation**\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "&= \\mathbb{E}[A(\\bf{x} - \\mu)(\\bf{x} - \\mu)^T A^T] \\\\\n",
    "&= A\\mathbb{E}[(\\bf{x} - \\mu)(\\bf{x} - \\mu)^T]A^T \\\\\n",
    "&= A\\Sigma_x A^T\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec067ea6",
   "metadata": {},
   "source": [
    "### Expected Value (continuos):\n",
    "\n",
    "$$\\mathbb{E}[x]  = \\int_{-\\infty}^{\\infty} xf(x)= \\mu.$$\n",
    "\n",
    "### Marginal distribution (continuous): \n",
    "\n",
    "$$f(x) = \\int_{\\mathbb{R}}f(x, y)dy$$\n",
    "\n",
    "where f(x, y) is the joint distribution and f(x) is the marginal distribution.\n",
    "\n",
    "### TODO: more on cont. dist.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd9c4f4",
   "metadata": {},
   "source": [
    "### Ordinary Least Squares Derivation:\n",
    "\n",
    "Show for \n",
    "\n",
    "$$y_i = \\beta_0 + \\beta_1 x_i$$\n",
    "\n",
    "the $\\hat{\\beta}_0$, $\\hat{\\beta}_1$ that minimize $RSS$ using least squares is \n",
    "\n",
    "$$\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n(x_i - \\bar{x})}$$  \n",
    "\n",
    "$$\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1\\bar{x}$$\n",
    "\n",
    "where $\\bar{y} = \\frac{1}{n}\\sum_{i=1}^ny_i$, $\\bar{x} = \\frac{1}{n}\\sum_{i=1}^nx_i$.\n",
    "\n",
    "The residual or error for the $ith$ observation or sample between the actual and estimate is: \n",
    "\n",
    "$e_i = y_i - \\hat{y_i}$\n",
    "\n",
    "and for $n$ observations the $RSS$ or residual sum of squares is \n",
    "\n",
    "$$\\begin{align*}\n",
    "RSS &= \\sum_{i=1}^n e_i^2 \\\\\n",
    "    &= e_1^2 + \\dots + e_n^2 \\\\\n",
    "    &= (y_1 - \\hat{y}_1)^2 + \\dots + (y_n - \\hat{y}_n)^2\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "We now must minimize the $RSS$ with respect to $\\hat{\\beta}_0$, $\\hat{\\beta}_1$:\n",
    "\n",
    "$$\\begin{align*}\n",
    "RSS(\\hat{\\beta}_0,\\hat{\\beta}_1) &=(y_1 - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_1)^2 + \\dots + (y_n - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_n)^2\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "such that \n",
    "\n",
    "$$\\begin{align*}\n",
    "\\frac{\\partial RSS(\\hat{\\beta}_0,\\hat{\\beta}_1)}{\\partial \\hat{\\beta}_0} &= 2(y_1 - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_1)\\cdot(-1) + \\dots + 2(y_n - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_n)\\cdot(-1) \\\\\n",
    "&=2\\sum_{i=1}^n (\\hat{\\beta}_0 +\\hat{\\beta}_1 x_i - y_i) = 0  \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\frac{\\partial RSS(\\hat{\\beta}_0,\\hat{\\beta}_1)}{\\partial \\hat{\\beta}_1} &= 2(y_1 - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_1)\\cdot(-x_1) + \\dots + 2(y_n - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_n)\\cdot(-x_n) \\\\\n",
    "&=2\\sum_{i=1}^n x_i (\\hat{\\beta}_0 +\\hat{\\beta}_1 x_i - y_i) = 0  \n",
    "\\end{align*}\n",
    "$$ \n",
    "\n",
    "and we now have the following 2 equations: \n",
    "\n",
    "$\\frac{\\partial RSS(\\hat{\\beta}_0,\\hat{\\beta}_1)}{\\partial \\hat{\\beta}_0}=\\sum_{i=1}^n (\\hat{\\beta}_0 +\\hat{\\beta}_1 x_i - y_i)=0$\n",
    "\n",
    "$\\frac{\\partial RSS(\\hat{\\beta}_0,\\hat{\\beta}_1)}{\\partial \\hat{\\beta}_1}=\\sum_{i=1}^nx_i (\\hat{\\beta}_0 +\\hat{\\beta}_1 x_i - y_i) = 0 $.\n",
    "\n",
    "The first equation we can use the fact that $n\\hat{\\beta}_0=\\sum_{i=1}^n\\hat{\\beta}_0$ and $\\bar{x} = \\frac{1}{n}\\sum_{i=1}^nx_i$ then we have and solve for $\\hat{\\beta}_0$: \n",
    "\n",
    "$$\\begin{align*}\n",
    "\\hat{\\beta}_0 &= \\frac{1}{n}\\sum_{i=1}^ny_i - \\frac{\\hat{\\beta}_1}{n}\\sum_{i=1}^nx_i \\\\\n",
    "             &= \\bar{y} - \\hat{\\beta}_1\\bar{x}\n",
    "\\end{align*}\n",
    "$$.\n",
    "\n",
    "For the second equation we can identify the following two factorizations: \n",
    "\n",
    "$$\\begin{align*}\n",
    "\\sum_{i=1}^n(x_i - \\bar{x})(x_i - \\bar{x}) &= \\sum x_i^2 - \\bar{x}\\sum x_i - \\bar{x}\\sum x_i + \\sum \\bar{x}^2 \\\\\n",
    "&= \\sum x_i^2 - n\\bar{x}^2 - n\\bar{x}^2 + n\\bar{x}^2 \\\\ \n",
    "&= \\sum x_i^2 - n\\bar{x}^2\n",
    "\\end{align*}\n",
    "$$.\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\sum_{i=1}^n(x_i - \\bar{x})(y_i - \\bar{y}) &= \\sum x_iy_i - \\bar{x}\\sum y_i - \\bar{y}\\sum x_i + \\sum \\bar{x}\\bar{y} \\\\\n",
    "&= \\sum x_iy_i  - n\\bar{x}\\bar{y} - n\\bar{y}\\bar{x} + n\\bar{x}\\bar{y}\\\\ \n",
    "&= \\sum x_iy_i - n\\bar{x}\\bar{y}\n",
    "\\end{align*}\n",
    "$$.\n",
    "\n",
    "Then from the second equation we have: \n",
    "\n",
    "$$\\begin{align*}\n",
    "\\sum_{i=1}^n x_i y_i &= \\hat{\\beta_0}\\sum_{i=1}^nx_i + \\hat{\\beta_1}\\sum_{i=1}^n x_i^2 \\\\\n",
    " &= (\\bar{y} - \\hat{\\beta}_1 \\bar{x})\\sum_{i=1}^nx_i + \\hat{\\beta_1}\\sum_{i=1}^n x_i^2 \\\\\n",
    " &=(\\bar{y} - \\hat{\\beta}_1 \\bar{x})(n\\bar{x}) + \\hat{\\beta_1}\\sum_{i=1}^n x_i^2 \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Solving for $\\hat{\\beta_1}$:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\hat{\\beta_1}(\\sum_{i=1}^n x_i^2 - n\\bar{x}^2) = \\sum_{i=1}^n x_i y_i - n\\bar{x}\\bar{y} \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "and using the previous two factorizations we have: \n",
    "\n",
    "$$\\hat{\\beta_1}\\sum_{i=1}^n (x_i - \\bar{x})^2 = \\sum_{i=1}^n (x_i - \\bar{x})(y_i -\\bar{y})$$ \n",
    "\n",
    "and solving for $\\hat{\\beta_1}$:\n",
    "\n",
    "$$\\hat{\\beta_1} = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i -\\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360d68fe",
   "metadata": {},
   "source": [
    "### Ordinary Least Squares Derivation (matrix):\n",
    "\n",
    "Let's use the matrix gradient the derivation of OLS using the squared norm (same as RSS). First, let's elaborate the following facts for linear quadratic functions of $\\beta$: \n",
    "\n",
    "For any $\\mathbf{c} \\in \\mathbb{R}^n$, we have $\\nabla_{\\beta}(\\mathbf{c}^T\\mathbf{\\beta})=\\mathbf{c}$.\n",
    "\n",
    "For any $X \\in \\mathbb{R}^{n\\times n}$, we have $\\nabla_{\\beta}(\\mathbf{\\beta}^TX\\mathbf{\\beta})=(X + X^T)\\beta$.\n",
    "\n",
    "We are trying to minimize the squared norm with respect to $\\beta$: \n",
    "$$\\begin{align*}\n",
    "0 &= \\nabla||\\bold{y} - X\\bold{\\beta}||^2 \\\\\n",
    "&=\\nabla (\\bold{y} - X\\beta)^T(\\bold{y} - X\\beta)\\\\\n",
    "&= \\nabla (\\bold{y}^T - \\beta^TX^T)(\\bold{y} - X\\beta)\\\\\n",
    "&= \\nabla(\\bold{y}^T\\bold{y} - \\beta^TX^T\\bold{y} - \\bold{y}^TX\\beta +\\beta^TX^TX\\beta)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Since\n",
    "$\\beta \\in \\mathbb{R}^p$, $\\mathbf{y} \\in \\mathbb{R}^p$, and $X \\in \\mathbb{R}^{n\\times p}$, then $\\beta^TX^T\\bold{y}=\\bold{y}^TX\\beta$. We can readily take the transpose since they both result in the same scalar quantity. Continuing on we get \n",
    "\n",
    "$$\\begin{align*}\n",
    "0 &= \\nabla(\\bold{y}^T\\bold{y} - \\beta^TX^T\\bold{y} - \\bold{y}^TX\\beta +\\beta^TX^TX\\beta)\\\\\n",
    "&= \\nabla(\\bold{y}^T\\bold{y} - 2\\beta^TX^T\\bold{y} + \\beta^TX^TX\\beta) \\\\\n",
    "&= - 2X^T\\bold{y} + (X^TX + X^TX)\\beta \\\\\n",
    "&= - 2X^T\\bold{y}  + 2X^TX\\beta\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Solving for $\\beta$ we arrive at \n",
    "\n",
    "$$\\beta = (X^TX)^{-1}X^T\\bold{y}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab57f120",
   "metadata": {},
   "source": [
    "### Probability notation:\n",
    "\n",
    "$X$ is a **random variable** a quantity whose value is known until we observe or \"sample\" it. \n",
    "\n",
    "**Individual sample values** \n",
    "\n",
    "Are when we actually draw a sample of size $n$ we observe numeric realizations \n",
    "\n",
    "$$x_1, x_2, \\dots, x_n$$\n",
    "\n",
    "of the random variables \n",
    "\n",
    "$$X_1, X_2, \\dots, X_n$$ \n",
    "\n",
    "where $$X_i \\stackrel{\\text{iid}}{\\sim} X.$$\n",
    "\n",
    "**Sample-mean random variable** \n",
    "\n",
    "Is when each $X_i$ is random, their average is \n",
    "\n",
    "$$\\bar{X} = \\frac{1}{n}\\sum_{i=1}^nX_i$$\n",
    "\n",
    "is also a random variable. It has an expectation \n",
    "\n",
    "$$\\mathbb{E}[\\bar{X}] = \\mu$$\n",
    "\n",
    "which says the sample mean is an unbiased estimator of the population mean. \n",
    "\n",
    "**Observed (numeric) sample mean**\n",
    "\n",
    "After we collect the data we compute \n",
    "\n",
    "$$\\bar{x} = \\frac{1}{n}\\sum_{i=1}^nx_i$$\n",
    "\n",
    "which is a number. Lowercase is used because $\\bar{x}$ is no longer rrandom but is the variable $\\bar{X}$ takes for this particular sample. \n",
    "\n",
    "\n",
    "In the probabilistic model we assume there is a single underlying random variable $X$ that captures the population distribution—call its density or pmf $f_X$.  A *sample* of size $n$ is modelled as $n$ independent replicates of that same variable:\n",
    "\n",
    "$$\n",
    "X_1,\\;X_2,\\;\\ldots,\\;X_n \\;\\stackrel{\\text{iid}}{\\sim}\\; X .\n",
    "$$\n",
    "\n",
    "* **Independent** means the outcome of one draw carries no information about the others.\n",
    "* **Identically distributed** means every draw has the same mean $\\mu$, variance $\\sigma^{2}$, and full distribution $f_X$.\n",
    "\n",
    "So each $X_i$ is a conceptually separate random variable, but all share the same population law.  They are placeholders for the numbers you *might* observe.  Once you actually collect the data, you plug in the realised values and switch to lowercase:\n",
    "\n",
    "$$\n",
    "(x_1,\\dots,x_n)=\\text{the specific numbers your experiment produced}.\n",
    "$$\n",
    "\n",
    "That separation—random $X_i$ in theory, observed $x_i$s in practice—is what lets us derive properties (like $E[\\bar X]=\\mu$ or $\\operatorname{Var}(\\bar X)=\\sigma^{2}/n$) before seeing any data, and then apply them to the concrete sample you end up with.\n",
    "\n",
    "**Summary of X notation**\n",
    "\n",
    "$X$ is one draw from the population (random)\n",
    "\n",
    "$X_i$ is the $i$th draw in a sample (random)\n",
    "\n",
    "$\\bar{X}$ is the *sample-mean random variable* (random)\n",
    "\n",
    "$\\mathbb{E}[X] or \\mathbb{E}[\\bar{X}]$ is the population mean $\\mu$ (deterministic constant), the expected value of the sample mean is the same as the population mean. \n",
    "\n",
    "$x_i$ is the observed value of $X_i$ (not random)\n",
    "\n",
    "$\\bar{x}$ is the numeric mean of the observed sample (not random)\n",
    "\n",
    "**Variances**\n",
    "\n",
    "$$\\sigma^2 = \\mathbb{E}[X^2] - (\\mathbb{E})^2$$\n",
    "\n",
    "is the *population* variance of a single draw $X$ from the distribution. When you collect a sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fa211e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d61226",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
